[
     {
          "algorithm": null,
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Evaluate solutions and select base solution\n    non_dominated = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] <= obj[0] and other_obj[1] <= obj[1] and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            non_dominated.append(i)\n\n    if not non_dominated:\n        combined_costs = [obj[0] + obj[1] for _, obj in archive]\n        selected_idx = np.argmin(combined_costs)\n    else:\n        qualities = [1.0 / (1.0 + archive[i][1][0] + archive[i][1][1]) for i in non_dominated]\n        selected_idx = random.choices(non_dominated, weights=qualities, k=1)[0]\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Hybrid local search with dynamic node swapping\n    n = len(base_solution)\n    if n < 3:\n        return new_solution\n\n    def calculate_total_cost(sol):\n        cost1 = 0.0\n        cost2 = 0.0\n        for i in range(n):\n            cost1 += distance_matrix_1[sol[i], sol[(i+1)%n]]\n            cost2 += distance_matrix_2[sol[i], sol[(i+1)%n]]\n        return cost1, cost2\n\n    original_cost1, original_cost2 = calculate_total_cost(new_solution)\n\n    # Select two nodes to swap based on their relative positions in both spaces\n    i, j = random.sample(range(n), 2)\n    node_i = new_solution[i]\n    node_j = new_solution[j]\n\n    # Calculate the contribution of each node to both objectives\n    def calculate_node_contribution(sol, idx):\n        node = sol[idx]\n        prev_node = sol[idx-1]\n        next_node = sol[(idx+1)%n]\n        cost1 = distance_matrix_1[prev_node, node] + distance_matrix_1[node, next_node]\n        cost2 = distance_matrix_2[prev_node, node] + distance_matrix_2[node, next_node]\n        return cost1, cost2\n\n    contrib_i1, contrib_i2 = calculate_node_contribution(new_solution, i)\n    contrib_j1, contrib_j2 = calculate_node_contribution(new_solution, j)\n\n    # Perform the swap\n    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Calculate new costs\n    new_cost1, new_cost2 = calculate_total_cost(new_solution)\n\n    # Acceptance criterion\n    if not (new_cost1 <= original_cost1 and new_cost2 <= original_cost2) and random.random() >= 0.2:\n        # If not accepted, try another swap\n        for _ in range(2):\n            i_new, j_new = random.sample(range(n), 2)\n            new_solution[i_new], new_solution[j_new] = new_solution[j_new], new_solution[i_new]\n            temp_cost1, temp_cost2 = calculate_total_cost(new_solution)\n            if (temp_cost1 <= original_cost1 and temp_cost2 <= original_cost2) or random.random() < 0.15:\n                break\n            new_solution[i_new], new_solution[j_new] = new_solution[j_new], new_solution[i_new]\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        new_solution = base_solution.copy()\n\n    return new_solution\n\n",
          "score": [
               -0.9729095779833985,
               0.7925425171852112
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid Pareto-geometric selection and path reconstruction strategy that combines a probabilistic dominance-based solution selection with a novel objective-space alignment operator. First, it will use non-dominated sorting to identify promising solutions in the archive, then probabilistically select a base solution based on its Pareto rank. Next, it will analyze the geometric alignment of nodes in both objective spaces using a custom alignment metric, and reconstruct the tour by sequentially connecting nodes that show the highest alignment potential in both spaces, with a dynamic segment reordering mechanism that alternates between objective spaces to balance improvement in both dimensions. The method will ensure feasibility through a permutation validation step and a geometric repair mechanism if needed, while prioritizing edges that show the most promising alignment improvement in both spaces.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Pareto-based probabilistic selection\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and all(objectives[j] <= objectives[i]) and any(objectives[j] < objectives[i]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if pareto_front:\n        selected_idx = np.random.choice(pareto_front)\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space alignment analysis\n    alignment_scores = []\n    for i in range(n):\n        u = new_solution[i]\n        v = new_solution[(i + 1) % n]\n\n        # Calculate alignment metric: cosine similarity of direction vectors\n        vec1 = instance[v, :2] - instance[u, :2]\n        vec2 = instance[v, 2:] - instance[u, 2:]\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n\n        if norm1 > 0 and norm2 > 0:\n            cos_sim = np.dot(vec1, vec2) / (norm1 * norm2)\n            alignment_scores.append((i, cos_sim))\n        else:\n            alignment_scores.append((i, 0.0))\n\n    # Step 3: Dynamic segment reordering\n    if n > 3:\n        # Sort segments by alignment score (highest first)\n        alignment_scores.sort(key=lambda x: -x[1])\n        selected_segments = [x[0] for x in alignment_scores[:max(2, n//5)]]\n\n        # Reorder segments in alternating objective spaces\n        toggle = True\n        new_order = []\n        used_nodes = set()\n\n        for seg in selected_segments:\n            if toggle:\n                # Connect in first objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([u, v])\n            else:\n                # Connect in second objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([v, u])\n            toggle = not toggle\n\n        # Add remaining nodes\n        remaining_nodes = [node for node in base_solution if node not in new_order]\n        new_order.extend(remaining_nodes)\n\n        new_solution = np.array(new_order)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -1.027370962532713,
               2.509422779083252
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hierarchical multi-objective path decomposition strategy that first hierarchically partitions the solution tour into geometrically distinct layers based on the relative dominance relationships between nodes in both objective spaces, then probabilistically selects and reorders these layers using a novel geometric entropy metric that balances the spatial distribution and objective contributions of each layer, followed by a constrained layer fusion process that dynamically merges adjacent layers based on their geometric compatibility and potential for Pareto improvement, with feasibility maintained through a hierarchical boundary validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a multi-objective centroid-based ordering approach if invalidity is detected.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hierarchical layer partitioning\n    def hierarchical_partition(solution, depth=3):\n        if depth == 0 or len(solution) <= 3:\n            return [solution]\n\n        # Calculate geometric properties\n        coords1 = instance[solution, :2]\n        coords2 = instance[solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate geometric entropy\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        entropy = np.sum(dist1 * dist2)\n\n        # Split based on entropy\n        sorted_indices = np.argsort(entropy)\n        split_point = len(solution) // 2\n        left = solution[sorted_indices[:split_point]]\n        right = solution[sorted_indices[split_point:]]\n\n        return hierarchical_partition(left, depth-1) + hierarchical_partition(right, depth-1)\n\n    # Step 2: Select base solution\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    layers = hierarchical_partition(base_solution)\n\n    # Step 3: Layer reordering\n    if len(layers) > 1:\n        # Calculate layer properties\n        layer_properties = []\n        for layer in layers:\n            layer_coords1 = instance[layer, :2]\n            layer_coords2 = instance[layer, 2:]\n            layer_centroid1 = np.mean(layer_coords1, axis=0)\n            layer_centroid2 = np.mean(layer_coords2, axis=0)\n            layer_properties.append({\n                'centroid1': layer_centroid1,\n                'centroid2': layer_centroid2,\n                'size': len(layer),\n                'coords1': layer_coords1,\n                'coords2': layer_coords2\n            })\n\n        # Calculate layer compatibility\n        num_layers = len(layers)\n        compatibility_matrix = np.zeros((num_layers, num_layers))\n        for i in range(num_layers):\n            for j in range(num_layers):\n                if i != j:\n                    dist1 = np.linalg.norm(layer_properties[i]['centroid1'] - layer_properties[j]['centroid1'])\n                    dist2 = np.linalg.norm(layer_properties[i]['centroid2'] - layer_properties[j]['centroid2'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist1 + dist2)\n\n        # Reorder layers based on compatibility\n        current_order = list(range(num_layers))\n        for _ in range(2):\n            i = random.randint(0, num_layers-1)\n            j = np.argmax(compatibility_matrix[i])\n            if i != j:\n                current_order[i], current_order[j] = current_order[j], current_order[i]\n\n        new_solution = np.concatenate([layers[i] for i in current_order])\n    else:\n        new_solution = base_solution.copy()\n\n    # Step 4: Layer fusion\n    if len(layers) > 1 and random.random() < 0.4:\n        # Find most compatible adjacent layers\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n\n        for i in range(len(layers)-1):\n            dist1 = np.linalg.norm(layer_properties[i]['centroid1'] - layer_properties[i+1]['centroid1'])\n            dist2 = np.linalg.norm(layer_properties[i]['centroid2'] - layer_properties[i+1]['centroid2'])\n            compatibility = 1.0 / (1.0 + dist1 + dist2)\n            if compatibility > best_compatibility:\n                best_compatibility = compatibility\n                best_i, best_j = i, i+1\n\n        if best_i != -1 and best_j != -1:\n            # Merge compatible layers\n            merged_layer = np.concatenate([layers[best_i], layers[best_j]])\n            new_layers = layers[:best_i] + [merged_layer] + layers[best_j+1:]\n            new_solution = np.concatenate(new_layers)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != len(base_solution):\n        # Multi-objective centroid-based repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate combined geometric properties\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        combined_metric = dist1 * dist2\n\n        # Sort nodes by combined metric\n        sorted_indices = np.argsort(combined_metric)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n",
          "score": [
               -0.9642584663819409,
               0.2063705325126648
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid multi-objective selection and geometric transformation strategy that first identifies the most balanced solutions in the archive by combining Pareto dominance with a novel geometric diversity metric, then probabilistically selects a base solution based on both objective performance and geometric alignment of its path segments. It will generate a neighbor solution by applying a geometric transformation operator that warps the tour in one objective space while preserving geometric consistency in the other space, using a novel \"objective-space morphing\" technique that dynamically adjusts the transformation based on the relative improvement potential in each space, with a feasibility-preserving mechanism that ensures the transformed path remains a valid tour through a combination of geometric validation and path reconstruction.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Multi-objective balanced selection\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-8)\n\n    # Calculate geometric diversity\n    geometric_diversity = []\n    for sol, _ in archive:\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        centroid1 = np.mean(path1, axis=0)\n        centroid2 = np.mean(path2, axis=0)\n        diversity = np.linalg.norm(centroid1 - centroid2)\n        geometric_diversity.append(diversity)\n\n    # Combine metrics for selection\n    combined_metric = normalized_obj[:, 0] + normalized_obj[:, 1] + np.array(geometric_diversity)\n    selected_idx = np.argmax(combined_metric)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space morphing transformation\n    if n > 3:\n        # Select a segment to transform\n        segment_length = max(3, n // 4)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate transformation parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n        scale_factor = 0.8 + 0.4 * random.random()\n\n        # Apply geometric transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Transform in first objective space\n            vec1 = instance[node, :2] - centroid1\n            instance[node, :2] = centroid1 + scale_factor * vec1\n            # Transform in second objective space\n            vec2 = instance[node, 2:] - centroid2\n            instance[node, 2:] = centroid2 + (1.0 / scale_factor) * vec2\n\n        # Rebuild solution to maintain feasibility\n        temp_solution = []\n        remaining_nodes = set(range(n)) - set(segment)\n        temp_solution.extend(segment)\n        temp_solution.extend(list(remaining_nodes))\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair by randomizing the order of remaining nodes\n        remaining = list(set(range(n)) - set(new_solution[:n]))\n        random.shuffle(remaining)\n        new_solution[n-len(remaining):] = remaining\n\n    return new_solution\n\n",
          "score": [
               -1.0180486271477518,
               3.075184404850006
          ]
     },
     {
          "algorithm": "{The proposed heuristic function 'select_neighbor' employs a multi-objective adaptive local search strategy that intelligently selects a promising solution from the archive based on a combined objective score (normalized sum of both objectives), then applies a novel hybrid local search operator that combines edge insertion with a biased random walk to explore high-potential regions of the search space. The method first identifies the Pareto-frontier solutions in the archive, then selects a base solution probabilistically weighted by its dominance and diversity, followed by a two-stage perturbation: 1) a constrained segment inversion that preserves node connectivity, and 2) a multi-objective-aware edge swap that prioritizes improvements in both objectives while ensuring feasibility through a validation check. The generated neighbor solution is guaranteed to be a valid TSP tour through the use of a permutation repair mechanism that corrects any invalid sequences.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a base solution (weighted random selection)\n    objectives = np.array([obj for _, obj in archive])\n    norm_obj = objectives / np.max(objectives, axis=0)\n    combined_scores = np.sum(norm_obj, axis=1)\n    probabilities = combined_scores / np.sum(combined_scores)\n    selected_idx = np.random.choice(len(archive), p=probabilities)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Hybrid local search operator\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Stage 1: Constrained segment inversion\n    if n > 3:\n        segment_length = np.random.randint(2, min(5, n//2))\n        start = np.random.randint(0, n - segment_length)\n        end = start + segment_length\n        segment = new_solution[start:end]\n        np.random.shuffle(segment)\n        new_solution[start:end] = segment\n\n    # Stage 2: Multi-objective-aware edge swap\n    for _ in range(2):\n        i, j = np.random.choice(n, 2, replace=False)\n        if i > j:\n            i, j = j, i\n\n        # Calculate cost before swap\n        prev_cost1 = (distance_matrix_1[new_solution[i-1], new_solution[i]] +\n                      distance_matrix_1[new_solution[j], new_solution[(j+1)%n]])\n        prev_cost2 = (distance_matrix_2[new_solution[i-1], new_solution[i]] +\n                      distance_matrix_2[new_solution[j], new_solution[(j+1)%n]])\n\n        # Calculate cost after swap\n        new_cost1 = (distance_matrix_1[new_solution[i-1], new_solution[j]] +\n                     distance_matrix_1[new_solution[i], new_solution[(j+1)%n]])\n        new_cost2 = (distance_matrix_2[new_solution[i-1], new_solution[j]] +\n                     distance_matrix_2[new_solution[i], new_solution[(j+1)%n]])\n\n        # Accept if both objectives improve\n        if (new_cost1 < prev_cost1) and (new_cost2 < prev_cost2):\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism if invalid\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -0.9434502893378602,
               0.6291956901550293
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a multi-phase geometric fragmentation and reassembly strategy that first decomposes the solution tour into geometrically distinct segments based on the relative positions of nodes in both objective spaces, then probabilistically selects and reorders these segments using a novel geometric harmony metric that balances the spatial distribution and objective contributions of each segment, followed by a constrained segment fusion process that dynamically merges adjacent segments based on their geometric compatibility and potential for Pareto improvement, with feasibility maintained through a segment boundary validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a geometric centroid-based ordering approach if invalidity is detected.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution based on combined objective quality\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Geometric fragmentation\n    if n > 4:\n        # Calculate geometric properties for each node in both spaces\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n\n        # Calculate centroids and angles\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate angles relative to centroids\n        angles1 = np.arctan2(coords1[:,1] - centroid1[1], coords1[:,0] - centroid1[0])\n        angles2 = np.arctan2(coords2[:,1] - centroid2[1], coords2[:,0] - centroid2[0])\n\n        # Calculate geometric harmony metric\n        angle_diff = np.abs(angles1 - angles2)\n        harmony = 1.0 / (1.0 + angle_diff)\n\n        # Sort nodes by harmony\n        sorted_indices = np.argsort(harmony)[::-1]\n\n        # Split into segments based on harmony\n        num_segments = max(2, min(4, n // 3))\n        segment_size = n // num_segments\n        segments = [new_solution[sorted_indices[i*segment_size:(i+1)*segment_size]] for i in range(num_segments)]\n\n        # Step 3: Segment reordering\n        # Calculate segment properties\n        segment_properties = []\n        for seg in segments:\n            seg_coords1 = instance[seg, :2]\n            seg_coords2 = instance[seg, 2:]\n            seg_centroid1 = np.mean(seg_coords1, axis=0)\n            seg_centroid2 = np.mean(seg_coords2, axis=0)\n            seg_properties = {\n                'centroid1': seg_centroid1,\n                'centroid2': seg_centroid2,\n                'size': len(seg),\n                'coords1': seg_coords1,\n                'coords2': seg_coords2\n            }\n            segment_properties.append(seg_properties)\n\n        # Calculate segment compatibility\n        compatibility_matrix = np.zeros((num_segments, num_segments))\n        for i in range(num_segments):\n            for j in range(num_segments):\n                if i != j:\n                    dist1 = np.linalg.norm(segment_properties[i]['centroid1'] - segment_properties[j]['centroid1'])\n                    dist2 = np.linalg.norm(segment_properties[i]['centroid2'] - segment_properties[j]['centroid2'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist1 + dist2)\n\n        # Reorder segments based on compatibility\n        current_order = list(range(num_segments))\n        for _ in range(2):\n            i = random.randint(0, num_segments-1)\n            j = np.argmax(compatibility_matrix[i])\n            if i != j:\n                current_order[i], current_order[j] = current_order[j], current_order[i]\n\n        # Reconstruct solution from reordered segments\n        new_solution = np.concatenate([segments[i] for i in current_order])\n\n    # Step 4: Segment fusion\n    if n > 5 and random.random() < 0.3:\n        # Find the most compatible adjacent segments\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n\n        for i in range(len(segments)-1):\n            dist1 = np.linalg.norm(segment_properties[i]['centroid1'] - segment_properties[i+1]['centroid1'])\n            dist2 = np.linalg.norm(segment_properties[i]['centroid2'] - segment_properties[i+1]['centroid2'])\n            compatibility = 1.0 / (1.0 + dist1 + dist2)\n            if compatibility > best_compatibility:\n                best_compatibility = compatibility\n                best_i, best_j = i, i+1\n\n        if best_i != -1 and best_j != -1:\n            # Merge the compatible segments\n            merged_segment = np.concatenate([segments[best_i], segments[best_j]])\n            new_segments = segments[:best_i] + [merged_segment] + segments[best_j+1:]\n            new_solution = np.concatenate(new_segments)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != n:\n        # Geometric centroid-based repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate angles relative to centroids\n        angles1 = np.arctan2(coords1[:,1] - centroid1[1], coords1[:,0] - centroid1[0])\n        angles2 = np.arctan2(coords2[:,1] - centroid2[1], coords2[:,0] - centroid2[0])\n\n        # Sort nodes by combined angle\n        combined_angles = angles1 + angles2\n        sorted_indices = np.argsort(combined_angles)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n",
          "score": [
               -0.934290307549595,
               0.31503432989120483
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid approach combining a multi-objective aware edge insertion heuristic with a constrained geometric perturbation, where it first identifies critical edges in the Pareto frontier solutions using a geometric analysis of the instance coordinates, then probabilistically selects a base solution based on its geometric diversity, followed by a two-phase perturbation process: 1) a directed edge insertion that prioritizes edges connecting geometrically distant nodes while maintaining Pareto optimality, and 2) a constrained geometric rotation that dynamically selects and rotates segments based on the relative positions of nodes in both objective spaces, with feasibility maintained through a geometric validation check and a permutation repair mechanism that ensures the solution remains a valid tour by reconstructing the tour using a nearest-neighbor approach from the closest valid node if invalidity is detected.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify critical edges in Pareto frontier solutions\n    pareto_front = []\n    for i, (sol_i, obj_i) in enumerate(archive):\n        dominated = False\n        for j, (sol_j, obj_j) in enumerate(archive):\n            if i != j and obj_j[0] <= obj_i[0] and obj_j[1] <= obj_i[1] and (obj_j[0] < obj_i[0] or obj_j[1] < obj_i[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(sol_i)\n\n    if not pareto_front:\n        pareto_front = [sol for sol, _ in archive]\n\n    # Step 2: Select base solution based on geometric diversity\n    def geometric_diversity(sol):\n        coords1 = instance[sol, :2]\n        coords2 = instance[sol, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n        diversity = np.sum(np.linalg.norm(coords1 - centroid1, axis=1)) + np.sum(np.linalg.norm(coords2 - centroid2, axis=1))\n        return diversity\n\n    base_solution = max(pareto_front, key=geometric_diversity).copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 3: Directed edge insertion\n    if n > 3:\n        # Find geometrically distant nodes\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        distances1 = np.linalg.norm(coords1 - np.roll(coords1, 1, axis=0), axis=1)\n        distances2 = np.linalg.norm(coords2 - np.roll(coords2, 1, axis=0), axis=1)\n        total_distances = distances1 + distances2\n        critical_edges = np.argsort(total_distances)[-min(3, n//2):]\n\n        for edge in critical_edges:\n            # Try inserting a new edge\n            u, v = new_solution[edge], new_solution[(edge+1)%n]\n            # Find the best insertion point\n            best_insert = -1\n            best_gain = -np.inf\n            for i in range(n):\n                if i != edge and i != (edge+1)%n:\n                    # Calculate potential gain\n                    old_cost = distance_matrix_1[new_solution[i], new_solution[(i+1)%n]] + distance_matrix_2[new_solution[i], new_solution[(i+1)%n]]\n                    new_cost = distance_matrix_1[u, new_solution[(i+1)%n]] + distance_matrix_2[u, new_solution[(i+1)%n]] + \\\n                               distance_matrix_1[new_solution[i], v] + distance_matrix_2[new_solution[i], v]\n                    gain = old_cost - new_cost\n                    if gain > best_gain:\n                        best_gain = gain\n                        best_insert = i\n\n            if best_insert != -1 and best_gain > 0:\n                # Perform insertion\n                new_solution = np.concatenate([new_solution[:best_insert+1], [v], new_solution[best_insert+1:]])\n                new_solution = np.concatenate([new_solution[:edge+1], [u], new_solution[edge+1:]])\n\n    # Step 4: Constrained geometric rotation\n    if n > 4:\n        segment_length = np.random.randint(2, min(5, n//2))\n        start = np.random.randint(0, n - segment_length)\n        end = start + segment_length\n\n        # Calculate geometric properties\n        segment_coords1 = instance[new_solution[start:end], :2]\n        segment_coords2 = instance[new_solution[start:end], 2:]\n        centroid1 = np.mean(segment_coords1, axis=0)\n        centroid2 = np.mean(segment_coords2, axis=0)\n\n        # Rotate segment based on geometric properties\n        angle1 = np.arctan2(segment_coords1[:,1] - centroid1[1], segment_coords1[:,0] - centroid1[0])\n        angle2 = np.arctan2(segment_coords2[:,1] - centroid2[1], segment_coords2[:,0] - centroid2[0])\n        avg_angle = np.mean(angle1 + angle2)\n\n        # Sort nodes by angle\n        angles = angle1 + angle2\n        sorted_indices = np.argsort(angles)\n        new_solution[start:end] = new_solution[start:end][sorted_indices]\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != n:\n        # Nearest-neighbor repair\n        valid_tour = []\n        remaining_nodes = set(new_solution)\n        current_node = new_solution[0]\n        valid_tour.append(current_node)\n        remaining_nodes.remove(current_node)\n\n        while remaining_nodes:\n            min_dist = np.inf\n            next_node = None\n            for node in remaining_nodes:\n                dist = distance_matrix_1[current_node, node] + distance_matrix_2[current_node, node]\n                if dist < min_dist:\n                    min_dist = dist\n                    next_node = node\n            valid_tour.append(next_node)\n            remaining_nodes.remove(next_node)\n            current_node = next_node\n\n        new_solution = np.array(valid_tour)\n\n    return new_solution\n\n",
          "score": [
               -0.9502865232406194,
               0.8973692655563354
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid Pareto-geometric selection and path reconstruction strategy that combines a probabilistic dominance-based solution selection with a novel objective-space alignment operator. First, it will use non-dominated sorting to identify promising solutions in the archive, then probabilistically select a base solution based on its Pareto rank. Next, it will analyze the geometric alignment of nodes in both objective spaces using a custom alignment metric, and reconstruct the tour by sequentially connecting nodes that show the highest alignment potential in both spaces, with a dynamic segment reordering mechanism that alternates between objective spaces to balance improvement in both dimensions. The method will ensure feasibility through a permutation validation step and a geometric repair mechanism if needed, while prioritizing edges that show the most promising alignment improvement in both spaces.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Pareto-based probabilistic selection\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and all(objectives[j] <= objectives[i]) and any(objectives[j] < objectives[i]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if pareto_front:\n        selected_idx = np.random.choice(pareto_front)\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space alignment analysis\n    alignment_scores = []\n    for i in range(n):\n        u = new_solution[i]\n        v = new_solution[(i + 1) % n]\n\n        # Calculate alignment metric: cosine similarity of direction vectors\n        vec1 = instance[v, :2] - instance[u, :2]\n        vec2 = instance[v, 2:] - instance[u, 2:]\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n\n        if norm1 > 0 and norm2 > 0:\n            cos_sim = np.dot(vec1, vec2) / (norm1 * norm2)\n            alignment_scores.append((i, cos_sim))\n        else:\n            alignment_scores.append((i, 0.0))\n\n    # Step 3: Dynamic segment reordering\n    if n > 3:\n        # Sort segments by alignment score (highest first)\n        alignment_scores.sort(key=lambda x: -x[1])\n        selected_segments = [x[0] for x in alignment_scores[:max(2, n//5)]]\n\n        # Reorder segments in alternating objective spaces\n        toggle = True\n        new_order = []\n        used_nodes = set()\n\n        for seg in selected_segments:\n            if toggle:\n                # Connect in first objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([u, v])\n            else:\n                # Connect in second objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([v, u])\n            toggle = not toggle\n\n        # Add remaining nodes\n        remaining_nodes = [node for node in base_solution if node not in new_order]\n        new_order.extend(remaining_nodes)\n\n        new_solution = np.array(new_order)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -0.9596246872564067,
               1.9203274846076965
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid objective-space partitioning and adaptive tour reconstruction strategy that first identifies promising regions of the objective space through a novel multi-objective grid partitioning approach, then selects a base solution from the most under-explored partition to balance exploration and exploitation. It dynamically decomposes the tour into segments based on alternating objective-space improvement patterns, applies a geometric path smoothing operator to optimize local segments while preserving global structure, and reconstructs the tour with an adaptive segment insertion mechanism that prioritizes edges showing complementary improvements in both objectives. The method ensures feasibility through a geometric validation step and a partition-aware repair mechanism, while incorporating a dynamic objective weighting scheme that adapts to the current archive's Pareto front characteristics to guide the local search toward more balanced solutions in the objective space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Objective-space partitioning\n    objectives = np.array([obj for _, obj in archive])\n    grid_size = max(3, int(np.sqrt(len(archive)) / 2))\n    grid_x = np.linspace(objectives[:, 0].min(), objectives[:, 0].max(), grid_size + 1)\n    grid_y = np.linspace(objectives[:, 1].min(), objectives[:, 1].max(), grid_size + 1)\n\n    # Assign partitions\n    partitions = {}\n    for i, obj in enumerate(objectives):\n        x_idx = np.digitize(obj[0], grid_x) - 1\n        y_idx = np.digitize(obj[1], grid_y) - 1\n        partitions.setdefault((x_idx, y_idx), []).append(i)\n\n    # Select from least explored partition\n    partition_counts = {k: len(v) for k, v in partitions.items()}\n    selected_partition = min(partition_counts, key=partition_counts.get)\n    partition_indices = partitions[selected_partition]\n\n    if not partition_indices:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = np.random.choice(partition_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Dynamic tour decomposition\n    decomposition_points = []\n    for i in range(n-1):\n        u, v = new_solution[i], new_solution[i+1]\n        obj_u = archive[selected_idx][1]\n        obj_v = (distance_matrix_1[u,v], distance_matrix_2[u,v])\n        if (obj_v[0] < obj_u[0] and obj_v[1] <= obj_u[1]) or (obj_v[0] <= obj_u[0] and obj_v[1] < obj_u[1]):\n            decomposition_points.append(i+1)\n\n    # Step 3: Geometric path smoothing\n    if decomposition_points:\n        segments = []\n        start = 0\n        for point in sorted(decomposition_points):\n            segments.append(new_solution[start:point+1])\n            start = point+1\n        if start < n:\n            segments.append(new_solution[start:])\n\n        # Apply smoothing to each segment\n        for i in range(len(segments)):\n            segment = segments[i]\n            if len(segment) > 3:\n                # Sort by alternating objective coordinates\n                if i % 2 == 0:\n                    segment = sorted(segment, key=lambda x: instance[x, 0] + instance[x, 2])\n                else:\n                    segment = sorted(segment, key=lambda x: instance[x, 1] + instance[x, 3])\n                segments[i] = segment\n\n        new_solution = np.concatenate(segments)\n\n    # Step 4: Adaptive segment insertion\n    if len(decomposition_points) > 1 and np.random.random() < 0.3:\n        insert_pos = np.random.choice(decomposition_points)\n        new_segment = np.random.permutation(new_solution[insert_pos-2:insert_pos+2])\n        new_solution = np.concatenate([new_solution[:insert_pos], new_segment, new_solution[insert_pos+2:]])\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -0.9555656211302912,
               1.22073096036911
          ]
     },
     {
          "algorithm": "{The new algorithm builds upon the common backbone idea of intelligent solution selection and hybrid local search from the provided algorithms, but introduces a novel approach by combining multi-objective solution evaluation with a dynamic segment insertion mechanism. It first identifies the most promising solutions in the archive by analyzing their dominance relationships and combined objective values, then employs a weighted random selection to choose a base solution with higher probability for better-performing solutions. The hybrid local search operator dynamically selects and inserts a segment from one part of the tour into another position, evaluating the impact on both objectives and accepting the change if it improves at least one objective or with a small probability if it doesn't. The algorithm incorporates a multi-objective-aware acceptance criterion, a fallback mechanism to ensure solution feasibility, and a secondary segment insertion attempt if the initial insertion doesn't yield improvement, while going beyond standard segment-based operations by considering dynamic segment selection and insertion points based on both objective spaces.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Evaluate solutions and select base solution\n    non_dominated = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] <= obj[0] and other_obj[1] <= obj[1] and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            non_dominated.append(i)\n\n    if not non_dominated:\n        combined_costs = [obj[0] + obj[1] for _, obj in archive]\n        selected_idx = np.argmin(combined_costs)\n    else:\n        qualities = [1.0 / (1.0 + archive[i][1][0] + archive[i][1][1]) for i in non_dominated]\n        selected_idx = random.choices(non_dominated, weights=qualities, k=1)[0]\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Hybrid local search with dynamic segment insertion\n    n = len(base_solution)\n    if n < 4:\n        return new_solution\n\n    # Select a random segment to insert elsewhere\n    segment_start = random.randint(0, n - 3)\n    segment_length = random.randint(2, min(4, n - segment_start - 1))\n    segment_end = segment_start + segment_length\n    segment = new_solution[segment_start:segment_end]\n\n    # Calculate segment cost\n    def calculate_segment_cost(sol, start, end):\n        cost1 = 0.0\n        cost2 = 0.0\n        for i in range(start, end):\n            cost1 += distance_matrix_1[sol[i], sol[(i+1)%n]]\n            cost2 += distance_matrix_2[sol[i], sol[(i+1)%n]]\n        return cost1, cost2\n\n    original_cost1, original_cost2 = calculate_segment_cost(new_solution, segment_start, segment_end)\n\n    # Remove the segment from its current position\n    new_solution = np.concatenate([new_solution[:segment_start], new_solution[segment_end:]])\n\n    # Find a new position to insert the segment\n    insert_pos = random.randint(0, n - segment_length - 1)\n    new_solution = np.concatenate([new_solution[:insert_pos], segment, new_solution[insert_pos:]])\n\n    # Calculate new segment cost\n    new_cost1, new_cost2 = calculate_segment_cost(new_solution, insert_pos, insert_pos + segment_length)\n\n    # Acceptance criterion\n    if not (new_cost1 <= original_cost1 and new_cost2 <= original_cost2) and random.random() >= 0.3:\n        # If not accepted, try another insertion position\n        for _ in range(2):\n            new_insert_pos = random.randint(0, n - segment_length - 1)\n            temp_solution = np.concatenate([new_solution[:new_insert_pos], segment, new_solution[new_insert_pos:]])\n            temp_cost1, temp_cost2 = calculate_segment_cost(temp_solution, new_insert_pos, new_insert_pos + segment_length)\n            if (temp_cost1 <= original_cost1 and temp_cost2 <= original_cost2) or random.random() < 0.2:\n                new_solution = temp_solution\n                break\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        new_solution = base_solution.copy()\n\n    return new_solution\n\n",
          "score": [
               -0.9076509273958202,
               0.41547876596450806
          ]
     }
]