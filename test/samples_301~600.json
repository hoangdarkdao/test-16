[
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm dynamically selects promising solutions from the archive using \u03b5-dominance ranking with adaptive \u03b5 scaling, prioritizes item flips based on weighted marginal gains combining both objectives, and applies capacity-aware perturbations to generate feasible neighbors while dynamically adjusting \u03b5 to balance exploration and exploitation. It emphasizes high-marginal-gain items and uses probabilistic perturbations to escape local optima, ensuring feasibility through iterative checks and removals. The solution selection is guided by \u03b5-dominance, while the neighbor generation balances greedy flips with adaptive perturbations to maintain diversity, with capacity_ratio dynamically adjusting perturbation intensity.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic \u03b5-dominance ranking with adaptive \u03b5\n    epsilon = 0.1\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest \u03b5-dominance rank (more dominated)\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] + x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Adaptive weighted marginal gains with dynamic \u03b5\n    capacity_ratio = (capacity - current_weight) / capacity\n    epsilon = max(0.05, 0.2 * capacity_ratio)  # Dynamic \u03b5 scaling\n    marginal_gain = (value1_lst + epsilon * value2_lst) * (1 + capacity_ratio) / (weight_lst + 1e-6)\n\n    # Sort by marginal gain (descending)\n    sorted_indices = np.argsort(-marginal_gain)\n\n    # Perform feasibility-aware flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            # Try removing item\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            # Try adding item\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Adaptive perturbation based on marginal gain and remaining capacity\n    perturbation_prob = min(0.5, 0.2 + capacity_ratio * 0.3)\n    if np.random.rand() < perturbation_prob:\n        # Select items with low marginal gain for potential flip\n        low_gain_indices = np.argsort(marginal_gain)\n        for i in low_gain_indices[:min(3, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: marginal_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 301,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid approach combining crowding distance and adaptive objective weighting, then applies a feasibility-aware local search that prioritizes high-marginal-gain items with dynamic trade-off factors, while maintaining capacity constraints through probabilistic swaps and perturbations. It balances exploration (via probabilistic item flips) and exploitation (via marginal gain optimization), with capacity-aware adjustments to escape local optima. The trade-off factor dynamically shifts between objectives based on current weight utilization, favoring value1 when capacity is scarce and value2 when capacity is abundant.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection using crowding distance and adaptive objective weighting\n    objectives = np.array([obj for _, obj in archive])\n    crowding_dist = np.zeros(len(archive))\n\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding_dist[sorted_idx[0]] = np.inf\n        crowding_dist[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n\n    # Adaptive objective weighting based on solution diversity\n    obj_weights = np.array([0.7, 0.3]) if np.std(crowding_dist) > 0.5 else np.array([0.5, 0.5])\n    combined_scores = objectives @ obj_weights\n\n    # Select solution with highest combined score\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Normalized marginal gains with dynamic trade-off factors\n    norm_value1 = (value1_lst - np.min(value1_lst)) / (np.max(value1_lst) - np.min(value1_lst) + 1e-6)\n    norm_value2 = (value2_lst - np.min(value2_lst)) / (np.max(value2_lst) - np.min(value2_lst) + 1e-6)\n    tradeoff_factor = 0.7 if current_weight < 0.5 * capacity else 0.3\n    marginal_gain = tradeoff_factor * norm_value1 + (1 - tradeoff_factor) * norm_value2\n\n    # Sort by marginal gain (descending)\n    sorted_indices = np.argsort(-marginal_gain)\n\n    # Perform feasibility-aware flips with probabilistic swaps\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            if np.random.rand() < 0.3:  # 30% chance to remove\n                temp_weight = current_weight - weight_lst[i]\n                if temp_weight <= capacity:\n                    new_solution[i] = 0\n                    current_weight = temp_weight\n        else:\n            if np.random.rand() < 0.7:  # 70% chance to add\n                temp_weight = current_weight + weight_lst[i]\n                if temp_weight <= capacity:\n                    new_solution[i] = 1\n                    current_weight = temp_weight\n\n    # Capacity-aware perturbations with probabilistic item swaps\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        candidate_indices = np.where(new_solution == 0)[0]\n        if len(candidate_indices) > 0:\n            swap_prob = min(0.4, remaining_capacity / capacity)\n            if np.random.rand() < swap_prob:\n                swap_idx = np.random.choice(candidate_indices)\n                if weight_lst[swap_idx] <= remaining_capacity:\n                    new_solution[swap_idx] = 1\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        remove_idx = np.random.choice(excess_items)\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.44823830218090466,
            3.6510781347751617
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection using crowding distance and adaptive objective weighting\n    objectives = np.array([obj for _, obj in archive])\n    crowding_dist = np.zeros(len(archive))\n\n    for m in range(2):\n        sorted_idx = np.argsort(objectives[:, m])\n        crowding_dist[sorted_idx[0]] = np.inf\n        crowding_dist[sorted_idx[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_idx[i]] += (objectives[sorted_idx[i+1], m] - objectives[sorted_idx[i-1], m]) / (objectives[sorted_idx[-1], m] - objectives[sorted_idx[0], m] + 1e-6)\n\n    # Adaptive objective weighting based on solution diversity\n    obj_weights = np.array([0.7, 0.3]) if np.std(crowding_dist) > 0.5 else np.array([0.5, 0.5])\n    combined_scores = objectives @ obj_weights\n\n    # Select solution with highest combined score\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Normalized marginal gains with dynamic trade-off factors\n    norm_value1 = (value1_lst - np.min(value1_lst)) / (np.max(value1_lst) - np.min(value1_lst) + 1e-6)\n    norm_value2 = (value2_lst - np.min(value2_lst)) / (np.max(value2_lst) - np.min(value2_lst) + 1e-6)\n    tradeoff_factor = 0.7 if current_weight < 0.5 * capacity else 0.3\n    marginal_gain = tradeoff_factor * norm_value1 + (1 - tradeoff_factor) * norm_value2\n\n    # Sort by marginal gain (descending)\n    sorted_indices = np.argsort(-marginal_gain)\n\n    # Perform feasibility-aware flips with probabilistic swaps\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            if np.random.rand() < 0.3:  # 30% chance to remove\n                temp_weight = current_weight - weight_lst[i]\n                if temp_weight <= capacity:\n                    new_solution[i] = 0\n                    current_weight = temp_weight\n        else:\n            if np.random.rand() < 0.7:  # 70% chance to add\n                temp_weight = current_weight + weight_lst[i]\n                if temp_weight <= capacity:\n                    new_solution[i] = 1\n                    current_weight = temp_weight\n\n    # Capacity-aware perturbations with probabilistic item swaps\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0:\n        candidate_indices = np.where(new_solution == 0)[0]\n        if len(candidate_indices) > 0:\n            swap_prob = min(0.4, remaining_capacity / capacity)\n            if np.random.rand() < swap_prob:\n                swap_idx = np.random.choice(candidate_indices)\n                if weight_lst[swap_idx] <= remaining_capacity:\n                    new_solution[swap_idx] = 1\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        remove_idx = np.random.choice(excess_items)\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a diverse solution from the archive based on its distance from the mean objectives, then generates neighbors through two phases: first flipping items with highest weighted marginal gains, followed by probabilistic perturbations prioritizing underrepresented objectives while respecting capacity constraints. The selection weights favor solutions with imbalanced objective values, and the marginal gain calculation adapts to the current solution's objective weights, ensuring balanced exploration of both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate diversity-based selection weights\n    v1_values = [obj[0] for _, obj in archive]\n    v2_values = [obj[1] for _, obj in archive]\n    v1_mean = np.mean(v1_values)\n    v2_mean = np.mean(v2_values)\n    selection_weights = []\n    for sol, obj in archive:\n        # Weight solutions based on their distance from the mean objectives\n        dist_v1 = abs(obj[0] - v1_mean)\n        dist_v2 = abs(obj[1] - v2_mean)\n        selection_weights.append(dist_v1 + dist_v2)\n\n    # Select solution with highest diversity weight\n    selected_idx = np.argmax(selection_weights)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Calculate adaptive objective weights\n    total_v1 = np.sum(value1_lst * base_solution)\n    total_v2 = np.sum(value2_lst * base_solution)\n    obj_weight1 = 1 / (total_v1 + 1e-6)\n    obj_weight2 = 1 / (total_v2 + 1e-6)\n\n    # Phase 1: Weighted marginal gain flips\n    marginal_gain = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-marginal_gain)\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n\n    # Phase 2: Capacity-aware probabilistic perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_strength = min(0.5, 1.5 * (remaining_capacity / capacity))\n    num_perturbations = max(1, int(5 * perturbation_strength))\n\n    for _ in range(num_perturbations):\n        if np.random.rand() < perturbation_strength:\n            # Select item based on weighted randomness\n            weights = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) * (1 - new_solution)\n            weights = np.where(weights > 0, weights, 1e-6)\n            perturbation_candidate = np.random.choice(len(weight_lst), p=weights/np.sum(weights))\n\n            if new_solution[perturbation_candidate] == 1:\n                temp_weight = current_weight - weight_lst[perturbation_candidate]\n                if temp_weight <= capacity:\n                    new_solution[perturbation_candidate] = 0\n                    current_weight = temp_weight\n            else:\n                temp_weight = current_weight + weight_lst[perturbation_candidate]\n                if temp_weight <= capacity:\n                    new_solution[perturbation_candidate] = 1\n                    current_weight = temp_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects promising solutions from the archive using adaptive \u03b5-dominance ranking, prioritizes item flips based on hypervolume-aware marginal gains, and applies capacity-constrained perturbations to generate neighbors while ensuring feasibility through iterative checks. It prioritizes items with high hypervolume contribution (value1 \u00d7 value2 / weight) and dynamically adjusts perturbations based on remaining capacity, with lower-priority items being flipped last to maintain feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive \u03b5-dominance ranking\n    epsilon = 0.15\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.4, 0.1 + remaining_capacity / capacity * 0.3)\n\n    if np.random.rand() < perturbation_prob:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(3, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThis algorithm combines dynamic \u03b5-dominance selection with hypervolume-aware flips and adaptive capacity-constrained perturbations to generate high-quality neighbors by prioritizing items with maximum hypervolume contribution while dynamically balancing objective exploration through \u03b5-dominated solution ranking and capacity-adaptive perturbation probabilities. It first filters solutions using \u03b5-dominance, selects the one with highest hypervolume contribution, then performs feasibility-aware flips and adaptive perturbations to generate a new solution while ensuring capacity constraints are met. The method intelligently prioritizes high-value items and adaptively adjusts perturbations based on remaining capacity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic \u03b5-dominance selection\n    epsilon = 0.15\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n\n    # Adaptive capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.5, 0.1 + remaining_capacity / capacity * 0.4)\n\n    if np.random.rand() < perturbation_prob:\n        # Perturb low-gain items first\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(5, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design for `select_neighbor`:\n\n- **Keywords**: Dynamic \u03b5-dominance, hypervolume-aware flips, adaptive weight balancing, capacity-aware diversification.\n- **Advice**: Rank solutions by \u03b5-dominance, prioritize flips maximizing hypervolume (`value1 * value2`), and perturb low-contribution items adaptively (probability inversely proportional to remaining capacity).\n- **Avoid**: Static selection, rigid perturbation rates, or feasibility violations. Use hypervolume-driven flips to balance objectives.\n- **Explanation**: Dynamic ranking ensures diverse, high-quality candidates. Hypervolume-aware flips and adaptive perturbations guide exploitation while maintaining feasibility. This balances exploration and exploitation while ensuring feasibility.\n\n**Code Implementation**:\n```python\ndef select_neighbor(archive, items, capacity, epsilon=0.5):\n    # Step 1: Rank solutions by \u03b5-dominance\n    ranked = sorted(archive, key=lambda x: (x[1][0] + epsilon * x[1][1]), reverse=True)\n\n    # Step 2: Select a solution with adaptive probability (bias toward top-ranked)\n    selected_idx = min(int(len(ranked) * random.betavariate(2, 5)), len(ranked)-1)\n    selected = ranked[selected_idx][0]\n\n    # Step 3: Generate neighbor via hypervolume-aware flips\n    neighbor = selected.copy()\n    remaining_cap = capacity - sum(items[i][0] for i in range(len(items)) if neighbor[i])\n\n    # Flip items with highest hypervolume contributions first\n    for i in sorted(range(len(items)), key=lambda x: (items[x][1] * items[x][2]), reverse=True):\n        if neighbor[i] == 0 and items[i][0] <= remaining_cap:\n            neighbor[i] = 1\n            remaining_cap -= items[i][0]\n        elif neighbor[i] == 1 and random.random() < remaining_cap / capacity:\n            neighbor[i] = 0\n            remaining_cap += items[i][0]\n\n    return neighbor\n```\n\nThis heuristic balances exploration (\u03b5-dominance) and exploitation (hypervolume-driven flips) while ensuring feasibility.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 302,
        "algorithm": "The algorithm combines dynamic \u03b5-dominance selection with adaptive objective-weighted flips and capacity-aware perturbations, prioritizing items with balanced hypervolume contributions while dynamically adjusting perturbations based on remaining capacity to generate high-quality, feasible neighbors. It first filters solutions using \u03b5-dominance, selects the one with the highest balanced hypervolume contribution, and then applies two-phase local search: first flipping items based on weighted gains, and second perturbing low-gain items adaptively to explore the solution space while maintaining feasibility. The algorithm ensures feasibility through final excess item removal, with perturbation strength scaling with remaining capacity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic \u03b5-dominance selection\n    epsilon = 0.2\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest balanced hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1] / (x[1][0] + x[1][1] + 1e-6)))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Calculate adaptive objective weights\n    total_v1 = np.sum(value1_lst * base_solution)\n    total_v2 = np.sum(value2_lst * base_solution)\n    obj_weight1 = 1 / (total_v1 + 1e-6)\n    obj_weight2 = 1 / (total_v2 + 1e-6)\n\n    # Phase 1: Balanced hypervolume-aware flips\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    weighted_gain = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) * hypervolume_gain\n    sorted_indices = np.argsort(-weighted_gain)\n\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n\n    # Phase 2: Adaptive capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_strength = min(0.6, 0.3 + remaining_capacity / capacity * 0.3)\n\n    if np.random.rand() < perturbation_strength:\n        # Perturb low-gain items first\n        low_gain_indices = np.argsort(weighted_gain)\n        for i in low_gain_indices[:min(3, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: weighted_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.6900179398811617,
            10.445427298545837
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic \u03b5-dominance selection\n    epsilon = 0.2\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest balanced hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1] / (x[1][0] + x[1][1] + 1e-6)))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Calculate adaptive objective weights\n    total_v1 = np.sum(value1_lst * base_solution)\n    total_v2 = np.sum(value2_lst * base_solution)\n    obj_weight1 = 1 / (total_v1 + 1e-6)\n    obj_weight2 = 1 / (total_v2 + 1e-6)\n\n    # Phase 1: Balanced hypervolume-aware flips\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    weighted_gain = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) * hypervolume_gain\n    sorted_indices = np.argsort(-weighted_gain)\n\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n\n    # Phase 2: Adaptive capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_strength = min(0.6, 0.3 + remaining_capacity / capacity * 0.3)\n\n    if np.random.rand() < perturbation_strength:\n        # Perturb low-gain items first\n        low_gain_indices = np.argsort(weighted_gain)\n        for i in low_gain_indices[:min(3, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: weighted_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects promising solutions from the archive using adaptive \u03b5-dominance ranking, prioritizes item flips based on hypervolume-aware marginal gains, and applies capacity-constrained perturbations to generate neighbors while ensuring feasibility through iterative checks. It prioritizes items with high hypervolume contribution (value1 \u00d7 value2 / weight) and dynamically adjusts perturbations based on remaining capacity, with lower-priority items being flipped last to maintain feasibility.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive \u03b5-dominance ranking\n    epsilon = 0.15\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.4, 0.1 + remaining_capacity / capacity * 0.3)\n\n    if np.random.rand() < perturbation_prob:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(3, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        \n        Your **sole objective** is to reduce the runtime of the algorithm as much as possible.\n        Ignore any performance metrics other than runtime.\n        \n        Please create a new algorithm that is a modified, faster version of the provided one.\n        Focus on optimizations such as reducing algorithmic complexity, improving loops,\n        using efficient data structures, removing redundant computations, and simplifying logic.\n        \n        1. First, identify parts of the code that most likely cause high runtime and suggest optimizations.\n        2. Next, describe your new optimized algorithm and its main steps in one detailed sentence. The description must be inside boxed {}.\n        3. Finally, implement the optimized algorithm in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        \n        Check syntax and code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 303,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Optimized \u03b5-dominance ranking (precompute objectives and use vectorized operations)\n    epsilon = 0.15\n    objectives = np.array([obj for _, obj in archive])\n    dominated = np.zeros(len(archive), dtype=bool)\n    for i in range(len(archive)):\n        if dominated[i]:\n            continue\n        mask = (objectives[:, 0] >= objectives[i, 0] - epsilon * objectives[i, 0]) & \\\n               (objectives[:, 1] >= objectives[i, 1] - epsilon * objectives[i, 1]) & \\\n               ((objectives[:, 0] > objectives[i, 0] - epsilon * objectives[i, 0]) |\n                (objectives[:, 1] > objectives[i, 1] - epsilon * objectives[i, 1]))\n        dominated[mask] = True\n\n    filtered_indices = np.where(~dominated)[0]\n    if len(filtered_indices) == 0:\n        filtered_indices = np.arange(len(archive))\n\n    # Select solution with highest hypervolume contribution\n    hypervolume = objectives[:, 0] * objectives[:, 1]\n    selected_idx = filtered_indices[np.argmax(hypervolume[filtered_indices])]\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Precompute hypervolume gain and sort indices once\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips (only check capacity once)\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations (simplified)\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0 and np.random.rand() < 0.3:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:3]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check (simplified)\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) > 0:\n            remove_idx = np.argmin(hypervolume_gain[excess_items])\n            new_solution[excess_items[remove_idx]] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.8078844068414727,
            1.5135948061943054
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Optimized \u03b5-dominance ranking (precompute objectives and use vectorized operations)\n    epsilon = 0.15\n    objectives = np.array([obj for _, obj in archive])\n    dominated = np.zeros(len(archive), dtype=bool)\n    for i in range(len(archive)):\n        if dominated[i]:\n            continue\n        mask = (objectives[:, 0] >= objectives[i, 0] - epsilon * objectives[i, 0]) & \\\n               (objectives[:, 1] >= objectives[i, 1] - epsilon * objectives[i, 1]) & \\\n               ((objectives[:, 0] > objectives[i, 0] - epsilon * objectives[i, 0]) |\n                (objectives[:, 1] > objectives[i, 1] - epsilon * objectives[i, 1]))\n        dominated[mask] = True\n\n    filtered_indices = np.where(~dominated)[0]\n    if len(filtered_indices) == 0:\n        filtered_indices = np.arange(len(archive))\n\n    # Select solution with highest hypervolume contribution\n    hypervolume = objectives[:, 0] * objectives[:, 1]\n    selected_idx = filtered_indices[np.argmax(hypervolume[filtered_indices])]\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Precompute hypervolume gain and sort indices once\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips (only check capacity once)\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations (simplified)\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0 and np.random.rand() < 0.3:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:3]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check (simplified)\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) > 0:\n            remove_idx = np.argmin(hypervolume_gain[excess_items])\n            new_solution[excess_items[remove_idx]] = 0\n\n    return new_solution\n\n",
        "operation": "r1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects promising solutions from the archive using adaptive \u03b5-dominance ranking, prioritizes item flips based on hypervolume-aware marginal gains, and applies capacity-constrained perturbations to generate neighbors while ensuring feasibility through iterative checks. It prioritizes items with high hypervolume contribution (value1 \u00d7 value2 / weight) and dynamically adjusts perturbations based on remaining capacity, with lower-priority items being flipped last to maintain feasibility.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive \u03b5-dominance ranking\n    epsilon = 0.15\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.4, 0.1 + remaining_capacity / capacity * 0.3)\n\n    if np.random.rand() < perturbation_prob:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(3, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        \n        Your **sole objective** is to reduce the runtime of the algorithm as much as possible.\n        Ignore any performance metrics other than runtime.\n        \n        Please create a new algorithm that is a modified, faster version of the provided one.\n        Focus on optimizations such as reducing algorithmic complexity, improving loops,\n        using efficient data structures, removing redundant computations, and simplifying logic.\n        \n        1. First, identify parts of the code that most likely cause high runtime and suggest optimizations.\n        2. Next, describe your new optimized algorithm and its main steps in one detailed sentence. The description must be inside boxed {}.\n        3. Finally, implement the optimized algorithm in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        \n        Check syntax and code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 304,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest hypervolume contribution (simplified \u03b5-dominance)\n    selected = max(archive, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Precompute hypervolume gains and sort indices\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Single pass flip with early termination\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations (simplified)\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0 and np.random.rand() < 0.3:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(2, len(low_gain_indices))]:\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final minimal feasibility check\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(hypervolume_gain[excess_items])]\n            new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7610630751583055,
            1.0792761445045471
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select solution with highest hypervolume contribution (simplified \u03b5-dominance)\n    selected = max(archive, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Precompute hypervolume gains and sort indices\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Single pass flip with early termination\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations (simplified)\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0 and np.random.rand() < 0.3:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(2, len(low_gain_indices))]:\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final minimal feasibility check\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        excess_items = np.where(new_solution == 1)[0]\n        if len(excess_items) > 0:\n            remove_idx = excess_items[np.argmin(hypervolume_gain[excess_items])]\n            new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "r1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects promising solutions from the archive using adaptive \u03b5-dominance ranking, prioritizes item flips based on hypervolume-aware marginal gains, and applies capacity-constrained perturbations to generate neighbors while ensuring feasibility through iterative checks. It prioritizes items with high hypervolume contribution (value1 \u00d7 value2 / weight) and dynamically adjusts perturbations based on remaining capacity, with lower-priority items being flipped last to maintain feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive \u03b5-dominance ranking\n    epsilon = 0.15\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.4, 0.1 + remaining_capacity / capacity * 0.3)\n\n    if np.random.rand() < perturbation_prob:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(3, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines a multi-phase approach: it first selects the best solution by combined objective value, then uses weighted marginal gains to perform high-impact flips while ensuring feasibility, followed by adaptive perturbations to remove low-value items, and finally enforces strict feasibility checks by removing the least valuable items. The weighted marginal gain prioritizes items with higher combined value-to-weight ratios, while the perturbation phase adaptively removes items based on remaining capacity, ensuring diversity in the search.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select base solution with highest combined objective value\n    selected_idx = np.argmax([obj[0] + obj[1] for _, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Calculate weighted marginal gains\n    weight_factor = 0.7\n    weighted_gain = (value1_lst + value2_lst) / (weight_lst ** weight_factor + 1e-6)\n    sorted_indices = np.argsort(-weighted_gain)\n\n    # Phase 1: High-impact flips with feasibility check\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            if current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n                break\n        else:\n            if current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n                break\n\n    # Phase 2: Capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_factor = min(0.5, 0.1 + 0.3 * (remaining_capacity / capacity))\n\n    if np.random.rand() < perturbation_factor:\n        # Select items with low weighted gain and high weight contribution\n        low_gain_mask = weighted_gain < np.percentile(weighted_gain, 30)\n        candidate_indices = np.where(new_solution)[0]\n\n        if len(candidate_indices) > 0:\n            candidate_indices = sorted(candidate_indices, key=lambda x: weight_lst[x] / (value1_lst[x] + value2_lst[x] + 1e-6))\n            for i in candidate_indices[:min(2, len(candidate_indices))]:\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Phase 3: Strict feasibility validation\n    while current_weight > capacity:\n        excess_items = np.where(new_solution)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove items with lowest weighted gain\n        excess_items_sorted = sorted(excess_items, key=lambda x: weighted_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n        current_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for `select_neighbor`:\n\n- **Keywords**: Dynamic \u03b5-dominance, hypervolume-aware flips, adaptive weight balancing, capacity-aware diversification.\n- **Advice**: Rank solutions by \u03b5-dominance, prioritize flips maximizing hypervolume (`value1 * value2`), and perturb low-contribution items adaptively (probability inversely proportional to remaining capacity).\n- **Avoid**: Static selection, rigid perturbation rates, or feasibility violations. Use hypervolume-driven flips to balance objectives.\n- **Explanation**: Dynamic ranking ensures diverse, high-quality candidates. Hypervolume-aware flips and adaptive perturbations guide exploitation while maintaining feasibility. This balances exploration and exploitation while ensuring feasibility.\n\n**Code Implementation**:\n```python\ndef select_neighbor(archive, items, capacity, epsilon=0.5):\n    # Step 1: Rank solutions by \u03b5-dominance\n    ranked = sorted(archive, key=lambda x: (x[1][0] + epsilon * x[1][1]), reverse=True)\n\n    # Step 2: Select a solution with adaptive probability (bias toward top-ranked)\n    selected_idx = min(int(len(ranked) * random.betavariate(2, 5)), len(ranked)-1)\n    selected = ranked[selected_idx][0]\n\n    # Step 3: Generate neighbor via hypervolume-aware flips\n    neighbor = selected.copy()\n    remaining_cap = capacity - sum(items[i][0] for i in range(len(items)) if neighbor[i])\n\n    # Flip items with highest hypervolume contributions first\n    for i in sorted(range(len(items)), key=lambda x: (items[x][1] * items[x][2]), reverse=True):\n        if neighbor[i] == 0 and items[i][0] <= remaining_cap:\n            neighbor[i] = 1\n            remaining_cap -= items[i][0]\n        elif neighbor[i] == 1 and random.random() < remaining_cap / capacity:\n            neighbor[i] = 0\n            remaining_cap += items[i][0]\n\n    return neighbor\n```\n\nThis heuristic balances exploration (\u03b5-dominance) and exploitation (hypervolume-driven flips) while ensuring feasibility.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 305,
        "algorithm": "The algorithm combines dynamic solution selection with an objective-balancing flip strategy that prioritizes high-gain items (normalized by both objectives and weight) while adaptively perturbing low-priority items based on remaining capacity, ensuring feasibility through iterative removal of least valuable items. It balances exploration (beta-distribution selection) and exploitation (\u03b5-dominance ranking) while maintaining diversity through capacity-aware perturbations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic solution selection using \u03b5-dominance and hypervolume\n    epsilon = 0.2\n    ranked_solutions = []\n    for sol, obj in archive:\n        is_dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                is_dominated = True\n                break\n        if not is_dominated:\n            ranked_solutions.append((sol, obj))\n\n    if not ranked_solutions:\n        ranked_solutions = archive.copy()\n\n    # Select solution with adaptive probability (beta distribution)\n    selection_prob = np.random.beta(2, 3)\n    selected_idx = min(int(selection_prob * len(ranked_solutions)), len(ranked_solutions) - 1)\n    base_solution = ranked_solutions[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Objective-balancing flip strategy\n    # Normalize objectives and calculate weighted gains\n    norm_value1 = value1_lst / (np.max(value1_lst) + 1e-6)\n    norm_value2 = value2_lst / (np.max(value2_lst) + 1e-6)\n    weighted_gain = (norm_value1 + norm_value2) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-weighted_gain)\n\n    # Flip high-priority items first\n    for i in sorted_indices:\n        if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n            new_solution[i] = 1\n            current_weight += weight_lst[i]\n        elif new_solution[i] == 1 and np.random.rand() < 0.3:\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Step 3: Adaptive capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.5, 0.1 + 0.4 * (remaining_capacity / capacity))\n\n    if np.random.rand() < perturbation_prob:\n        # Flip low-priority items (using weight-to-value ratio)\n        low_priority_gain = weight_lst / (value1_lst + value2_lst + 1e-6)\n        low_priority_indices = np.argsort(-low_priority_gain)\n\n        for i in low_priority_indices[:min(2, len(low_priority_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Step 4: Final feasibility check\n    while current_weight > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: weighted_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n        current_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.43719205768810276,
            2.31437885761261
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Dynamic solution selection using \u03b5-dominance and hypervolume\n    epsilon = 0.2\n    ranked_solutions = []\n    for sol, obj in archive:\n        is_dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                is_dominated = True\n                break\n        if not is_dominated:\n            ranked_solutions.append((sol, obj))\n\n    if not ranked_solutions:\n        ranked_solutions = archive.copy()\n\n    # Select solution with adaptive probability (beta distribution)\n    selection_prob = np.random.beta(2, 3)\n    selected_idx = min(int(selection_prob * len(ranked_solutions)), len(ranked_solutions) - 1)\n    base_solution = ranked_solutions[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Step 2: Objective-balancing flip strategy\n    # Normalize objectives and calculate weighted gains\n    norm_value1 = value1_lst / (np.max(value1_lst) + 1e-6)\n    norm_value2 = value2_lst / (np.max(value2_lst) + 1e-6)\n    weighted_gain = (norm_value1 + norm_value2) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-weighted_gain)\n\n    # Flip high-priority items first\n    for i in sorted_indices:\n        if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n            new_solution[i] = 1\n            current_weight += weight_lst[i]\n        elif new_solution[i] == 1 and np.random.rand() < 0.3:\n            new_solution[i] = 0\n            current_weight -= weight_lst[i]\n\n    # Step 3: Adaptive capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.5, 0.1 + 0.4 * (remaining_capacity / capacity))\n\n    if np.random.rand() < perturbation_prob:\n        # Flip low-priority items (using weight-to-value ratio)\n        low_priority_gain = weight_lst / (value1_lst + value2_lst + 1e-6)\n        low_priority_indices = np.argsort(-low_priority_gain)\n\n        for i in low_priority_indices[:min(2, len(low_priority_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Step 4: Final feasibility check\n    while current_weight > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: weighted_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n        current_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines a multi-phase approach: it first selects the best solution by combined objective value, then uses weighted marginal gains to perform high-impact flips while ensuring feasibility, followed by adaptive perturbations to remove low-value items, and finally enforces strict feasibility checks by removing the least valuable items. The weighted marginal gain prioritizes items with higher combined value-to-weight ratios, while the perturbation phase adaptively removes items based on remaining capacity, ensuring diversity in the search.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select base solution with highest combined objective value\n    selected_idx = np.argmax([obj[0] + obj[1] for _, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Calculate weighted marginal gains\n    weight_factor = 0.7\n    weighted_gain = (value1_lst + value2_lst) / (weight_lst ** weight_factor + 1e-6)\n    sorted_indices = np.argsort(-weighted_gain)\n\n    # Phase 1: High-impact flips with feasibility check\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            if current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n                break\n        else:\n            if current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n                break\n\n    # Phase 2: Capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_factor = min(0.5, 0.1 + 0.3 * (remaining_capacity / capacity))\n\n    if np.random.rand() < perturbation_factor:\n        # Select items with low weighted gain and high weight contribution\n        low_gain_mask = weighted_gain < np.percentile(weighted_gain, 30)\n        candidate_indices = np.where(new_solution)[0]\n\n        if len(candidate_indices) > 0:\n            candidate_indices = sorted(candidate_indices, key=lambda x: weight_lst[x] / (value1_lst[x] + value2_lst[x] + 1e-6))\n            for i in candidate_indices[:min(2, len(candidate_indices))]:\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Phase 3: Strict feasibility validation\n    while current_weight > capacity:\n        excess_items = np.where(new_solution)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove items with lowest weighted gain\n        excess_items_sorted = sorted(excess_items, key=lambda x: weighted_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n        current_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 306,
        "algorithm": "The algorithm selects a promising solution from an archive using a hybrid metric combining objective values and diversity, then applies a three-phase local search: first optimizing for marginal gains with adaptive weighting, second performing capacity-aware targeted perturbations, and finally enforcing feasibility by removing low-value items. It prioritizes items with high value-to-weight ratios while dynamically adjusting search intensity based on remaining capacity, ensuring feasibility through value-weighted removal of excess items. The adaptive weighting balances both objectives during marginal gain optimization, while the perturbation phase focuses on high-value-density items for selective removal.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection: combine objective values with diversity\n    objectives = np.array([obj[0] + obj[1] for _, obj in archive])\n    diversity = np.array([np.sum(sol != archive[0][0]) for sol, _ in archive])\n    combined_score = objectives + 0.3 * diversity\n    selected_idx = np.argmax(combined_score)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Adaptive weighted marginal gains\n    value_ratio = value1_lst / (value2_lst + 1e-6)\n    value_weight = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    adaptive_weight = value_weight * (1 + 0.2 * (1 / (1 + np.exp(-value_ratio)) - 0.5))\n    sorted_indices = np.argsort(-adaptive_weight)\n\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            if current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n                break\n        else:\n            if current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n                break\n\n    # Phase 2: Capacity-aware targeted perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_intensity = min(0.6, 0.2 + 0.4 * (remaining_capacity / capacity))\n\n    if np.random.rand() < perturbation_intensity:\n        value_density = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        candidate_indices = np.where(new_solution)[0]\n\n        if len(candidate_indices) > 0:\n            candidate_scores = value_density[candidate_indices] / (1 + weight_lst[candidate_indices])\n            perturb_indices = np.argsort(candidate_scores)[:min(3, len(candidate_indices))]\n\n            for idx in perturb_indices:\n                i = candidate_indices[idx]\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Phase 3: Feasibility enforcement with value-weighted removal\n    while current_weight > capacity:\n        excess_items = np.where(new_solution)[0]\n        if len(excess_items) == 0:\n            break\n\n        removal_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        removal_scores[excess_items] = -removal_scores[excess_items]\n        remove_idx = np.argmax(removal_scores)\n\n        if new_solution[remove_idx] == 1:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8927712186194576,
            2.282717525959015
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection: combine objective values with diversity\n    objectives = np.array([obj[0] + obj[1] for _, obj in archive])\n    diversity = np.array([np.sum(sol != archive[0][0]) for sol, _ in archive])\n    combined_score = objectives + 0.3 * diversity\n    selected_idx = np.argmax(combined_score)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Phase 1: Adaptive weighted marginal gains\n    value_ratio = value1_lst / (value2_lst + 1e-6)\n    value_weight = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    adaptive_weight = value_weight * (1 + 0.2 * (1 / (1 + np.exp(-value_ratio)) - 0.5))\n    sorted_indices = np.argsort(-adaptive_weight)\n\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            if current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n                break\n        else:\n            if current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n                break\n\n    # Phase 2: Capacity-aware targeted perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_intensity = min(0.6, 0.2 + 0.4 * (remaining_capacity / capacity))\n\n    if np.random.rand() < perturbation_intensity:\n        value_density = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        candidate_indices = np.where(new_solution)[0]\n\n        if len(candidate_indices) > 0:\n            candidate_scores = value_density[candidate_indices] / (1 + weight_lst[candidate_indices])\n            perturb_indices = np.argsort(candidate_scores)[:min(3, len(candidate_indices))]\n\n            for idx in perturb_indices:\n                i = candidate_indices[idx]\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Phase 3: Feasibility enforcement with value-weighted removal\n    while current_weight > capacity:\n        excess_items = np.where(new_solution)[0]\n        if len(excess_items) == 0:\n            break\n\n        removal_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        removal_scores[excess_items] = -removal_scores[excess_items]\n        remove_idx = np.argmax(removal_scores)\n\n        if new_solution[remove_idx] == 1:\n            new_solution[remove_idx] = 0\n            current_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines a multi-phase approach: it first selects the best solution by combined objective value, then uses weighted marginal gains to perform high-impact flips while ensuring feasibility, followed by adaptive perturbations to remove low-value items, and finally enforces strict feasibility checks by removing the least valuable items. The weighted marginal gain prioritizes items with higher combined value-to-weight ratios, while the perturbation phase adaptively removes items based on remaining capacity, ensuring diversity in the search.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select base solution with highest combined objective value\n    selected_idx = np.argmax([obj[0] + obj[1] for _, obj in archive])\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Calculate weighted marginal gains\n    weight_factor = 0.7\n    weighted_gain = (value1_lst + value2_lst) / (weight_lst ** weight_factor + 1e-6)\n    sorted_indices = np.argsort(-weighted_gain)\n\n    # Phase 1: High-impact flips with feasibility check\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            if current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n                break\n        else:\n            if current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n                break\n\n    # Phase 2: Capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_factor = min(0.5, 0.1 + 0.3 * (remaining_capacity / capacity))\n\n    if np.random.rand() < perturbation_factor:\n        # Select items with low weighted gain and high weight contribution\n        low_gain_mask = weighted_gain < np.percentile(weighted_gain, 30)\n        candidate_indices = np.where(new_solution)[0]\n\n        if len(candidate_indices) > 0:\n            candidate_indices = sorted(candidate_indices, key=lambda x: weight_lst[x] / (value1_lst[x] + value2_lst[x] + 1e-6))\n            for i in candidate_indices[:min(2, len(candidate_indices))]:\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Phase 3: Strict feasibility validation\n    while current_weight > capacity:\n        excess_items = np.where(new_solution)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove items with lowest weighted gain\n        excess_items_sorted = sorted(excess_items, key=lambda x: weighted_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n        current_weight -= weight_lst[remove_idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 307,
        "algorithm": "This algorithm selects a promising solution from the archive using a weighted combination of objective values and diversity, then applies a multi-phase hybrid local search that prioritizes items with high combined value-to-weight ratios while adaptively perturbing low-quality items and enforcing feasibility through targeted removals. The hybrid gain calculation balances value and weight factors, with dynamic perturbations based on solution quality, and final feasibility checks that prioritize removing items with the least marginal gain.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive solution selection based on both objectives and diversity\n    objectives = np.array([obj for _, obj in archive])\n    norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    diversity_scores = np.linalg.norm(norm_obj[:, None] - norm_obj, axis=2).sum(axis=1)\n    combined_scores = 0.6 * (objectives.sum(axis=1) / objectives.sum()) + 0.4 * (diversity_scores / diversity_scores.max())\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Novel hybrid marginal gain calculation\n    value_factor = 0.6\n    weight_factor = 0.4\n    hybrid_gain = (value_factor * (value1_lst + value2_lst) ** 0.8) / (weight_factor * (weight_lst ** 1.2 + 1e-6))\n    sorted_indices = np.argsort(-hybrid_gain)\n\n    # Phase 1: Capacity-aware flips with adaptive selection\n    flip_candidates = min(3, len(sorted_indices))\n    for i in sorted_indices[:flip_candidates]:\n        if new_solution[i] == 1:\n            if current_weight - weight_lst[i] <= capacity * 1.05:  # Allow slight overcapacity\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n        else:\n            if current_weight + weight_lst[i] <= capacity * 1.05:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Phase 2: Dynamic perturbation based on solution quality\n    solution_quality = np.sum(value1_lst + value2_lst) * 0.5 / (current_weight + 1e-6)\n    perturbation_prob = min(0.7, 0.3 + 0.4 * (1 - solution_quality))\n\n    if np.random.rand() < perturbation_prob:\n        # Remove items with low hybrid gain and high weight contribution\n        low_gain_mask = hybrid_gain < np.percentile(hybrid_gain, 25)\n        candidate_indices = np.where(new_solution & low_gain_mask)[0]\n\n        if len(candidate_indices) > 0:\n            candidate_indices = sorted(candidate_indices, key=lambda x: weight_lst[x] / (value1_lst[x] + value2_lst[x] + 1e-6))\n            for i in candidate_indices[:min(3, len(candidate_indices))]:\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Phase 3: Enhanced feasibility enforcement\n    while current_weight > capacity:\n        excess_items = np.where(new_solution)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove items with lowest hybrid gain-to-weight ratio\n        excess_items_sorted = sorted(excess_items, key=lambda x: hybrid_gain[x] / (weight_lst[x] + 1e-6))\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n        current_weight -= weight_lst[remove_idx]\n\n    # Final capacity check\n    if current_weight > capacity:\n        # If still infeasible, remove the largest item\n        largest_item = np.argmax(weight_lst * new_solution)\n        new_solution[largest_item] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.9227771038811872,
            1.9128405451774597
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive solution selection based on both objectives and diversity\n    objectives = np.array([obj for _, obj in archive])\n    norm_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-6)\n    diversity_scores = np.linalg.norm(norm_obj[:, None] - norm_obj, axis=2).sum(axis=1)\n    combined_scores = 0.6 * (objectives.sum(axis=1) / objectives.sum()) + 0.4 * (diversity_scores / diversity_scores.max())\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * new_solution)\n\n    # Novel hybrid marginal gain calculation\n    value_factor = 0.6\n    weight_factor = 0.4\n    hybrid_gain = (value_factor * (value1_lst + value2_lst) ** 0.8) / (weight_factor * (weight_lst ** 1.2 + 1e-6))\n    sorted_indices = np.argsort(-hybrid_gain)\n\n    # Phase 1: Capacity-aware flips with adaptive selection\n    flip_candidates = min(3, len(sorted_indices))\n    for i in sorted_indices[:flip_candidates]:\n        if new_solution[i] == 1:\n            if current_weight - weight_lst[i] <= capacity * 1.05:  # Allow slight overcapacity\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n        else:\n            if current_weight + weight_lst[i] <= capacity * 1.05:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Phase 2: Dynamic perturbation based on solution quality\n    solution_quality = np.sum(value1_lst + value2_lst) * 0.5 / (current_weight + 1e-6)\n    perturbation_prob = min(0.7, 0.3 + 0.4 * (1 - solution_quality))\n\n    if np.random.rand() < perturbation_prob:\n        # Remove items with low hybrid gain and high weight contribution\n        low_gain_mask = hybrid_gain < np.percentile(hybrid_gain, 25)\n        candidate_indices = np.where(new_solution & low_gain_mask)[0]\n\n        if len(candidate_indices) > 0:\n            candidate_indices = sorted(candidate_indices, key=lambda x: weight_lst[x] / (value1_lst[x] + value2_lst[x] + 1e-6))\n            for i in candidate_indices[:min(3, len(candidate_indices))]:\n                if current_weight - weight_lst[i] <= capacity:\n                    new_solution[i] = 0\n                    current_weight -= weight_lst[i]\n\n    # Phase 3: Enhanced feasibility enforcement\n    while current_weight > capacity:\n        excess_items = np.where(new_solution)[0]\n        if len(excess_items) == 0:\n            break\n        # Remove items with lowest hybrid gain-to-weight ratio\n        excess_items_sorted = sorted(excess_items, key=lambda x: hybrid_gain[x] / (weight_lst[x] + 1e-6))\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n        current_weight -= weight_lst[remove_idx]\n\n    # Final capacity check\n    if current_weight > capacity:\n        # If still infeasible, remove the largest item\n        largest_item = np.argmax(weight_lst * new_solution)\n        new_solution[largest_item] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a diverse solution from the archive based on its distance from the mean objectives, then generates neighbors through two phases: first flipping items with highest weighted marginal gains, followed by probabilistic perturbations prioritizing underrepresented objectives while respecting capacity constraints. The selection weights favor solutions with imbalanced objective values, and the marginal gain calculation adapts to the current solution's objective weights, ensuring balanced exploration of both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Calculate diversity-based selection weights\n    v1_values = [obj[0] for _, obj in archive]\n    v2_values = [obj[1] for _, obj in archive]\n    v1_mean = np.mean(v1_values)\n    v2_mean = np.mean(v2_values)\n    selection_weights = []\n    for sol, obj in archive:\n        # Weight solutions based on their distance from the mean objectives\n        dist_v1 = abs(obj[0] - v1_mean)\n        dist_v2 = abs(obj[1] - v2_mean)\n        selection_weights.append(dist_v1 + dist_v2)\n\n    # Select solution with highest diversity weight\n    selected_idx = np.argmax(selection_weights)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Calculate adaptive objective weights\n    total_v1 = np.sum(value1_lst * base_solution)\n    total_v2 = np.sum(value2_lst * base_solution)\n    obj_weight1 = 1 / (total_v1 + 1e-6)\n    obj_weight2 = 1 / (total_v2 + 1e-6)\n\n    # Phase 1: Weighted marginal gain flips\n    marginal_gain = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-marginal_gain)\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n\n    # Phase 2: Capacity-aware probabilistic perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_strength = min(0.5, 1.5 * (remaining_capacity / capacity))\n    num_perturbations = max(1, int(5 * perturbation_strength))\n\n    for _ in range(num_perturbations):\n        if np.random.rand() < perturbation_strength:\n            # Select item based on weighted randomness\n            weights = (obj_weight1 * value1_lst + obj_weight2 * value2_lst) * (1 - new_solution)\n            weights = np.where(weights > 0, weights, 1e-6)\n            perturbation_candidate = np.random.choice(len(weight_lst), p=weights/np.sum(weights))\n\n            if new_solution[perturbation_candidate] == 1:\n                temp_weight = current_weight - weight_lst[perturbation_candidate]\n                if temp_weight <= capacity:\n                    new_solution[perturbation_candidate] = 0\n                    current_weight = temp_weight\n            else:\n                temp_weight = current_weight + weight_lst[perturbation_candidate]\n                if temp_weight <= capacity:\n                    new_solution[perturbation_candidate] = 1\n                    current_weight = temp_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects promising solutions from the archive using adaptive \u03b5-dominance ranking, prioritizes item flips based on hypervolume-aware marginal gains, and applies capacity-constrained perturbations to generate neighbors while ensuring feasibility through iterative checks. It prioritizes items with high hypervolume contribution (value1 \u00d7 value2 / weight) and dynamically adjusts perturbations based on remaining capacity, with lower-priority items being flipped last to maintain feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive \u03b5-dominance ranking\n    epsilon = 0.15\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.4, 0.1 + remaining_capacity / capacity * 0.3)\n\n    if np.random.rand() < perturbation_prob:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(3, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThis algorithm combines dynamic \u03b5-dominance selection with hypervolume-aware flips and adaptive capacity-constrained perturbations to generate high-quality neighbors by prioritizing items with maximum hypervolume contribution while dynamically balancing objective exploration through \u03b5-dominated solution ranking and capacity-adaptive perturbation probabilities. It first filters solutions using \u03b5-dominance, selects the one with highest hypervolume contribution, then performs feasibility-aware flips and adaptive perturbations to generate a new solution while ensuring capacity constraints are met. The method intelligently prioritizes high-value items and adaptively adjusts perturbations based on remaining capacity.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic \u03b5-dominance selection\n    epsilon = 0.15\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n\n    # Adaptive capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.5, 0.1 + remaining_capacity / capacity * 0.4)\n\n    if np.random.rand() < perturbation_prob:\n        # Perturb low-gain items first\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(5, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: Dynamic \u03b5-dominance, hypervolume-aware flips, adaptive weight balancing, capacity-aware diversification.\n- **Advice**: Rank solutions by \u03b5-dominance, prioritize flips maximizing hypervolume (`value1 * value2`), and perturb low-contribution items adaptively (probability inversely proportional to remaining capacity).\n- **Avoid**: Static selection, rigid perturbation rates, or feasibility violations. Use hypervolume-driven flips to balance objectives.\n- **Explanation**: Dynamic ranking ensures diverse, high-quality candidates. Hypervolume-aware flips and adaptive perturbations guide exploitation while maintaining feasibility. This balances exploration and exploitation while ensuring feasibility.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 308,
        "algorithm": "This algorithm combines dynamic \u03b5-dominance selection, hypervolume-aware flips, and adaptive capacity-constrained perturbations to generate high-quality neighbors. It prioritizes solutions with maximum hypervolume contribution, balances objective exploration through \u03b5-dominated ranking, and dynamically adjusts perturbation probabilities based on remaining capacity. The method intelligently flips item selections while ensuring feasibility, using weighted gains and diversity to guide local search.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic \u03b5-dominance selection with adaptive epsilon\n    epsilon = 0.15 + 0.05 * (1 - len(archive) / max(10, len(archive)))\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume and diversity\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1] * (1 + np.sum(np.abs(x[0] - np.mean([s[0] for s in archive], axis=0))))))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains with objective balancing\n    total_v1 = np.sum(value1_lst * base_solution)\n    total_v2 = np.sum(value2_lst * base_solution)\n    obj_weight1 = 1 / (total_v1 + 1e-6)\n    obj_weight2 = 1 / (total_v2 + 1e-6)\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    weighted_gain = obj_weight1 * value1_lst + obj_weight2 * value2_lst\n    combined_gain = hypervolume_gain * weighted_gain\n    sorted_indices = np.argsort(-combined_gain)\n\n    # Perform feasibility-aware flips with objective balancing\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n\n    # Adaptive capacity-aware perturbations with dynamic probabilities\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.6, 0.2 + remaining_capacity / capacity * 0.4)\n    num_perturbations = max(1, int(3 + remaining_capacity / capacity * 5))\n\n    for _ in range(num_perturbations):\n        if np.random.rand() < perturbation_prob:\n            # Select item based on combined gain and diversity\n            weights = combined_gain * (1 - new_solution) + np.random.rand(len(weight_lst)) * 0.1\n            weights = np.where(weights > 0, weights, 1e-6)\n            perturbation_candidate = np.random.choice(len(weight_lst), p=weights/np.sum(weights))\n\n            if new_solution[perturbation_candidate] == 1:\n                temp_weight = current_weight - weight_lst[perturbation_candidate]\n                if temp_weight <= capacity:\n                    new_solution[perturbation_candidate] = 0\n                    current_weight = temp_weight\n            else:\n                temp_weight = current_weight + weight_lst[perturbation_candidate]\n                if temp_weight <= capacity:\n                    new_solution[perturbation_candidate] = 1\n                    current_weight = temp_weight\n\n    # Final feasibility check with objective-aware removal\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        # Remove item with lowest combined gain\n        excess_items_sorted = sorted(excess_items, key=lambda x: combined_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.5149380707691824,
            5.89046186208725
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic \u03b5-dominance selection with adaptive epsilon\n    epsilon = 0.15 + 0.05 * (1 - len(archive) / max(10, len(archive)))\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume and diversity\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1] * (1 + np.sum(np.abs(x[0] - np.mean([s[0] for s in archive], axis=0))))))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains with objective balancing\n    total_v1 = np.sum(value1_lst * base_solution)\n    total_v2 = np.sum(value2_lst * base_solution)\n    obj_weight1 = 1 / (total_v1 + 1e-6)\n    obj_weight2 = 1 / (total_v2 + 1e-6)\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    weighted_gain = obj_weight1 * value1_lst + obj_weight2 * value2_lst\n    combined_gain = hypervolume_gain * weighted_gain\n    sorted_indices = np.argsort(-combined_gain)\n\n    # Perform feasibility-aware flips with objective balancing\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n\n    # Adaptive capacity-aware perturbations with dynamic probabilities\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.6, 0.2 + remaining_capacity / capacity * 0.4)\n    num_perturbations = max(1, int(3 + remaining_capacity / capacity * 5))\n\n    for _ in range(num_perturbations):\n        if np.random.rand() < perturbation_prob:\n            # Select item based on combined gain and diversity\n            weights = combined_gain * (1 - new_solution) + np.random.rand(len(weight_lst)) * 0.1\n            weights = np.where(weights > 0, weights, 1e-6)\n            perturbation_candidate = np.random.choice(len(weight_lst), p=weights/np.sum(weights))\n\n            if new_solution[perturbation_candidate] == 1:\n                temp_weight = current_weight - weight_lst[perturbation_candidate]\n                if temp_weight <= capacity:\n                    new_solution[perturbation_candidate] = 0\n                    current_weight = temp_weight\n            else:\n                temp_weight = current_weight + weight_lst[perturbation_candidate]\n                if temp_weight <= capacity:\n                    new_solution[perturbation_candidate] = 1\n                    current_weight = temp_weight\n\n    # Final feasibility check with objective-aware removal\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        # Remove item with lowest combined gain\n        excess_items_sorted = sorted(excess_items, key=lambda x: combined_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects promising solutions from the archive using adaptive \u03b5-dominance ranking, prioritizes item flips based on hypervolume-aware marginal gains, and applies capacity-constrained perturbations to generate neighbors while ensuring feasibility through iterative checks. It prioritizes items with high hypervolume contribution (value1 \u00d7 value2 / weight) and dynamically adjusts perturbations based on remaining capacity, with lower-priority items being flipped last to maintain feasibility.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive \u03b5-dominance ranking\n    epsilon = 0.15\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.4, 0.1 + remaining_capacity / capacity * 0.3)\n\n    if np.random.rand() < perturbation_prob:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(3, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        \n        Your **sole objective** is to reduce the runtime of the algorithm as much as possible.\n        Ignore any performance metrics other than runtime.\n        \n        Please create a new algorithm that is a modified, faster version of the provided one.\n        Focus on optimizations such as reducing algorithmic complexity, improving loops,\n        using efficient data structures, removing redundant computations, and simplifying logic.\n        \n        1. First, identify parts of the code that most likely cause high runtime and suggest optimizations.\n        2. Next, describe your new optimized algorithm and its main steps in one detailed sentence. The description must be inside boxed {}.\n        3. Finally, implement the optimized algorithm in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        \n        Check syntax and code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 309,
        "algorithm": "The algorithm selects the highest hypervolume-contributing solution from the archive, performs a single-pass flip of items sorted by their hypervolume gain (value1 \u00d7 value2 / weight) while ensuring feasibility, and applies a minimal perturbation step to explore low-gain items probabilistically. It prioritizes items with higher hypervolume gain for flips and ensures feasibility by removing excess items in a final step. The design avoids redundant computations by precomputing hypervolume gains and sorting indices once, and uses a greedy approach to maintain feasibility throughout the process.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Precompute hypervolume gains and sort indices once\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Select solution with highest hypervolume contribution\n    selected = max(archive, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Single pass through sorted indices for flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Minimal perturbation step\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0 and np.random.rand() < 0.3:\n        # Only flip low-gain items that fit\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:3]:\n            if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n                break\n\n    # Final minimal feasibility check\n    if current_weight > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        for i in excess_items_sorted:\n            if current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n                break\n\n    return new_solution\n\n",
        "score": [
            -0.7741222638584204,
            1.0537969172000885
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Precompute hypervolume gains and sort indices once\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Select solution with highest hypervolume contribution\n    selected = max(archive, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Single pass through sorted indices for flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Minimal perturbation step\n    remaining_capacity = capacity - current_weight\n    if remaining_capacity > 0 and np.random.rand() < 0.3:\n        # Only flip low-gain items that fit\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:3]:\n            if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n                break\n\n    # Final minimal feasibility check\n    if current_weight > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        for i in excess_items_sorted:\n            if current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n                break\n\n    return new_solution\n\n",
        "operation": "r1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects promising solutions from the archive using adaptive \u03b5-dominance ranking, prioritizes item flips based on hypervolume-aware marginal gains, and applies capacity-constrained perturbations to generate neighbors while ensuring feasibility through iterative checks. It prioritizes items with high hypervolume contribution (value1 \u00d7 value2 / weight) and dynamically adjusts perturbations based on remaining capacity, with lower-priority items being flipped last to maintain feasibility.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive \u03b5-dominance ranking\n    epsilon = 0.15\n    filtered_solutions = []\n    for sol, obj in archive:\n        dominated = False\n        for _, other_obj in archive:\n            if (other_obj[0] >= obj[0] - epsilon * obj[0] and\n                other_obj[1] >= obj[1] - epsilon * obj[1] and\n                (other_obj[0] > obj[0] - epsilon * obj[0] or\n                 other_obj[1] > obj[1] - epsilon * obj[1])):\n                dominated = True\n                break\n        if not dominated:\n            filtered_solutions.append((sol, obj))\n\n    if not filtered_solutions:\n        filtered_solutions = archive.copy()\n\n    # Select solution with highest hypervolume contribution\n    selected = max(filtered_solutions, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Hypervolume-aware marginal gains\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Perform feasibility-aware flips\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations\n    remaining_capacity = capacity - current_weight\n    perturbation_prob = min(0.4, 0.1 + remaining_capacity / capacity * 0.3)\n\n    if np.random.rand() < perturbation_prob:\n        low_gain_indices = np.argsort(hypervolume_gain)\n        for i in low_gain_indices[:min(3, len(low_gain_indices))]:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check\n    while np.sum(weight_lst * new_solution) > capacity:\n        excess_items = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n        if not excess_items:\n            break\n        excess_items_sorted = sorted(excess_items, key=lambda x: hypervolume_gain[x])\n        remove_idx = excess_items_sorted[0]\n        new_solution[remove_idx] = 0\n\n    return new_solution\n\n\n        \n        Your **sole objective** is to reduce the runtime of the algorithm as much as possible.\n        Ignore any performance metrics other than runtime.\n        \n        Please create a new algorithm that is a modified, faster version of the provided one.\n        Focus on optimizations such as reducing algorithmic complexity, improving loops,\n        using efficient data structures, removing redundant computations, and simplifying logic.\n        \n        1. First, identify parts of the code that most likely cause high runtime and suggest optimizations.\n        2. Next, describe your new optimized algorithm and its main steps in one detailed sentence. The description must be inside boxed {}.\n        3. Finally, implement the optimized algorithm in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        \n        Check syntax and code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 310,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Precompute hypervolume gains once\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Select top solution by hypervolume\n    selected = max(archive, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Perform single flip of highest gain item\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations (simplified)\n    remaining_capacity = capacity - current_weight\n    if np.random.rand() < min(0.4, 0.1 + remaining_capacity / capacity * 0.3):\n        low_gain_indices = np.argsort(hypervolume_gain)[:3]\n        for i in low_gain_indices:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check (simplified)\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        for i in np.argsort(hypervolume_gain):\n            if new_solution[i] == 1 and excess <= 0:\n                break\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n                excess -= weight_lst[i]\n\n    return new_solution\n\n",
        "score": [
            -0.934219954534906,
            1.0735616087913513
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Precompute hypervolume gains once\n    hypervolume_gain = (value1_lst * value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(-hypervolume_gain)\n\n    # Select top solution by hypervolume\n    selected = max(archive, key=lambda x: (x[1][0] * x[1][1]))\n    base_solution = selected[0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst * base_solution)\n\n    # Perform single flip of highest gain item\n    for i in sorted_indices:\n        if new_solution[i] == 1:\n            temp_weight = current_weight - weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 0\n                current_weight = temp_weight\n                break\n        else:\n            temp_weight = current_weight + weight_lst[i]\n            if temp_weight <= capacity:\n                new_solution[i] = 1\n                current_weight = temp_weight\n                break\n\n    # Capacity-aware perturbations (simplified)\n    remaining_capacity = capacity - current_weight\n    if np.random.rand() < min(0.4, 0.1 + remaining_capacity / capacity * 0.3):\n        low_gain_indices = np.argsort(hypervolume_gain)[:3]\n        for i in low_gain_indices:\n            if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n            elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n\n    # Final feasibility check (simplified)\n    total_weight = np.sum(weight_lst * new_solution)\n    if total_weight > capacity:\n        excess = total_weight - capacity\n        for i in np.argsort(hypervolume_gain):\n            if new_solution[i] == 1 and excess <= 0:\n                break\n            if new_solution[i] == 1:\n                new_solution[i] = 0\n                excess -= weight_lst[i]\n\n    return new_solution\n\n",
        "operation": "r1"
    }
]