[
     {
          "algorithm": null,
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Evaluate solutions and select base solution\n    non_dominated = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] <= obj[0] and other_obj[1] <= obj[1] and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            non_dominated.append(i)\n\n    if not non_dominated:\n        combined_costs = [obj[0] + obj[1] for _, obj in archive]\n        selected_idx = np.argmin(combined_costs)\n    else:\n        qualities = [1.0 / (1.0 + archive[i][1][0] + archive[i][1][1]) for i in non_dominated]\n        selected_idx = random.choices(non_dominated, weights=qualities, k=1)[0]\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Hybrid local search with dynamic node swapping\n    n = len(base_solution)\n    if n < 3:\n        return new_solution\n\n    def calculate_total_cost(sol):\n        cost1 = 0.0\n        cost2 = 0.0\n        for i in range(n):\n            cost1 += distance_matrix_1[sol[i], sol[(i+1)%n]]\n            cost2 += distance_matrix_2[sol[i], sol[(i+1)%n]]\n        return cost1, cost2\n\n    original_cost1, original_cost2 = calculate_total_cost(new_solution)\n\n    # Select two nodes to swap based on their relative positions in both spaces\n    i, j = random.sample(range(n), 2)\n    node_i = new_solution[i]\n    node_j = new_solution[j]\n\n    # Calculate the contribution of each node to both objectives\n    def calculate_node_contribution(sol, idx):\n        node = sol[idx]\n        prev_node = sol[idx-1]\n        next_node = sol[(idx+1)%n]\n        cost1 = distance_matrix_1[prev_node, node] + distance_matrix_1[node, next_node]\n        cost2 = distance_matrix_2[prev_node, node] + distance_matrix_2[node, next_node]\n        return cost1, cost2\n\n    contrib_i1, contrib_i2 = calculate_node_contribution(new_solution, i)\n    contrib_j1, contrib_j2 = calculate_node_contribution(new_solution, j)\n\n    # Perform the swap\n    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Calculate new costs\n    new_cost1, new_cost2 = calculate_total_cost(new_solution)\n\n    # Acceptance criterion\n    if not (new_cost1 <= original_cost1 and new_cost2 <= original_cost2) and random.random() >= 0.2:\n        # If not accepted, try another swap\n        for _ in range(2):\n            i_new, j_new = random.sample(range(n), 2)\n            new_solution[i_new], new_solution[j_new] = new_solution[j_new], new_solution[i_new]\n            temp_cost1, temp_cost2 = calculate_total_cost(new_solution)\n            if (temp_cost1 <= original_cost1 and temp_cost2 <= original_cost2) or random.random() < 0.15:\n                break\n            new_solution[i_new], new_solution[j_new] = new_solution[j_new], new_solution[i_new]\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        new_solution = base_solution.copy()\n\n    return new_solution\n\n",
          "score": [
               -0.9729095779833985,
               0.7925425171852112
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid Pareto-geometric selection and path reconstruction strategy that combines a probabilistic dominance-based solution selection with a novel objective-space alignment operator. First, it will use non-dominated sorting to identify promising solutions in the archive, then probabilistically select a base solution based on its Pareto rank. Next, it will analyze the geometric alignment of nodes in both objective spaces using a custom alignment metric, and reconstruct the tour by sequentially connecting nodes that show the highest alignment potential in both spaces, with a dynamic segment reordering mechanism that alternates between objective spaces to balance improvement in both dimensions. The method will ensure feasibility through a permutation validation step and a geometric repair mechanism if needed, while prioritizing edges that show the most promising alignment improvement in both spaces.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Pareto-based probabilistic selection\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and all(objectives[j] <= objectives[i]) and any(objectives[j] < objectives[i]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if pareto_front:\n        selected_idx = np.random.choice(pareto_front)\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space alignment analysis\n    alignment_scores = []\n    for i in range(n):\n        u = new_solution[i]\n        v = new_solution[(i + 1) % n]\n\n        # Calculate alignment metric: cosine similarity of direction vectors\n        vec1 = instance[v, :2] - instance[u, :2]\n        vec2 = instance[v, 2:] - instance[u, 2:]\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n\n        if norm1 > 0 and norm2 > 0:\n            cos_sim = np.dot(vec1, vec2) / (norm1 * norm2)\n            alignment_scores.append((i, cos_sim))\n        else:\n            alignment_scores.append((i, 0.0))\n\n    # Step 3: Dynamic segment reordering\n    if n > 3:\n        # Sort segments by alignment score (highest first)\n        alignment_scores.sort(key=lambda x: -x[1])\n        selected_segments = [x[0] for x in alignment_scores[:max(2, n//5)]]\n\n        # Reorder segments in alternating objective spaces\n        toggle = True\n        new_order = []\n        used_nodes = set()\n\n        for seg in selected_segments:\n            if toggle:\n                # Connect in first objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([u, v])\n            else:\n                # Connect in second objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([v, u])\n            toggle = not toggle\n\n        # Add remaining nodes\n        remaining_nodes = [node for node in base_solution if node not in new_order]\n        new_order.extend(remaining_nodes)\n\n        new_solution = np.array(new_order)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -1.027370962532713,
               2.509422779083252
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hierarchical multi-objective path decomposition strategy that first hierarchically partitions the solution tour into geometrically distinct layers based on the relative dominance relationships between nodes in both objective spaces, then probabilistically selects and reorders these layers using a novel geometric entropy metric that balances the spatial distribution and objective contributions of each layer, followed by a constrained layer fusion process that dynamically merges adjacent layers based on their geometric compatibility and potential for Pareto improvement, with feasibility maintained through a hierarchical boundary validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a multi-objective centroid-based ordering approach if invalidity is detected.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hierarchical layer partitioning\n    def hierarchical_partition(solution, depth=3):\n        if depth == 0 or len(solution) <= 3:\n            return [solution]\n\n        # Calculate geometric properties\n        coords1 = instance[solution, :2]\n        coords2 = instance[solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate geometric entropy\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        entropy = np.sum(dist1 * dist2)\n\n        # Split based on entropy\n        sorted_indices = np.argsort(entropy)\n        split_point = len(solution) // 2\n        left = solution[sorted_indices[:split_point]]\n        right = solution[sorted_indices[split_point:]]\n\n        return hierarchical_partition(left, depth-1) + hierarchical_partition(right, depth-1)\n\n    # Step 2: Select base solution\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    layers = hierarchical_partition(base_solution)\n\n    # Step 3: Layer reordering\n    if len(layers) > 1:\n        # Calculate layer properties\n        layer_properties = []\n        for layer in layers:\n            layer_coords1 = instance[layer, :2]\n            layer_coords2 = instance[layer, 2:]\n            layer_centroid1 = np.mean(layer_coords1, axis=0)\n            layer_centroid2 = np.mean(layer_coords2, axis=0)\n            layer_properties.append({\n                'centroid1': layer_centroid1,\n                'centroid2': layer_centroid2,\n                'size': len(layer),\n                'coords1': layer_coords1,\n                'coords2': layer_coords2\n            })\n\n        # Calculate layer compatibility\n        num_layers = len(layers)\n        compatibility_matrix = np.zeros((num_layers, num_layers))\n        for i in range(num_layers):\n            for j in range(num_layers):\n                if i != j:\n                    dist1 = np.linalg.norm(layer_properties[i]['centroid1'] - layer_properties[j]['centroid1'])\n                    dist2 = np.linalg.norm(layer_properties[i]['centroid2'] - layer_properties[j]['centroid2'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist1 + dist2)\n\n        # Reorder layers based on compatibility\n        current_order = list(range(num_layers))\n        for _ in range(2):\n            i = random.randint(0, num_layers-1)\n            j = np.argmax(compatibility_matrix[i])\n            if i != j:\n                current_order[i], current_order[j] = current_order[j], current_order[i]\n\n        new_solution = np.concatenate([layers[i] for i in current_order])\n    else:\n        new_solution = base_solution.copy()\n\n    # Step 4: Layer fusion\n    if len(layers) > 1 and random.random() < 0.4:\n        # Find most compatible adjacent layers\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n\n        for i in range(len(layers)-1):\n            dist1 = np.linalg.norm(layer_properties[i]['centroid1'] - layer_properties[i+1]['centroid1'])\n            dist2 = np.linalg.norm(layer_properties[i]['centroid2'] - layer_properties[i+1]['centroid2'])\n            compatibility = 1.0 / (1.0 + dist1 + dist2)\n            if compatibility > best_compatibility:\n                best_compatibility = compatibility\n                best_i, best_j = i, i+1\n\n        if best_i != -1 and best_j != -1:\n            # Merge compatible layers\n            merged_layer = np.concatenate([layers[best_i], layers[best_j]])\n            new_layers = layers[:best_i] + [merged_layer] + layers[best_j+1:]\n            new_solution = np.concatenate(new_layers)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != len(base_solution):\n        # Multi-objective centroid-based repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate combined geometric properties\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        combined_metric = dist1 * dist2\n\n        # Sort nodes by combined metric\n        sorted_indices = np.argsort(combined_metric)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n",
          "score": [
               -0.9642584663819409,
               0.2063705325126648
          ]
     },
     {
          "algorithm": null,
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution (weighted by inverse objective sum)\n    objectives = np.array([obj for _, obj in archive])\n    norm_obj = objectives / (np.max(objectives, axis=0) + 1e-8)\n    combined_scores = 1.0 / (1.0 + np.sum(norm_obj, axis=1))\n    probabilities = combined_scores / np.sum(combined_scores)\n    selected_idx = np.random.choice(len(archive), p=probabilities)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Dynamic geometric clustering\n    def geometric_clustering(solution):\n        coords1 = instance[solution, :2]\n        coords2 = instance[solution, 2:]\n\n        # Calculate geometric properties\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n\n        # Calculate geometric harmony metric\n        harmony = np.exp(-(dist1 + dist2) / (np.mean(dist1) + np.mean(dist2) + 1e-8))\n\n        # Cluster nodes based on harmony\n        sorted_indices = np.argsort(harmony)\n        num_clusters = max(2, min(5, len(solution) // 3))\n        cluster_sizes = np.full(num_clusters, len(solution) // num_clusters)\n        cluster_sizes[:len(solution) % num_clusters] += 1\n\n        clusters = []\n        start = 0\n        for size in cluster_sizes:\n            clusters.append(solution[sorted_indices[start:start+size]])\n            start += size\n\n        return clusters\n\n    clusters = geometric_clustering(base_solution)\n\n    # Step 3: Cluster reordering based on geometric compatibility\n    if len(clusters) > 1:\n        # Calculate cluster properties\n        cluster_properties = []\n        for cluster in clusters:\n            cluster_coords1 = instance[cluster, :2]\n            cluster_coords2 = instance[cluster, 2:]\n            cluster_properties.append({\n                'centroid1': np.mean(cluster_coords1, axis=0),\n                'centroid2': np.mean(cluster_coords2, axis=0),\n                'size': len(cluster)\n            })\n\n        # Calculate compatibility matrix\n        num_clusters = len(clusters)\n        compatibility_matrix = np.zeros((num_clusters, num_clusters))\n        for i in range(num_clusters):\n            for j in range(num_clusters):\n                if i != j:\n                    dist1 = np.linalg.norm(cluster_properties[i]['centroid1'] - cluster_properties[j]['centroid1'])\n                    dist2 = np.linalg.norm(cluster_properties[i]['centroid2'] - cluster_properties[j]['centroid2'])\n                    compatibility_matrix[i,j] = np.exp(-(dist1 + dist2) / (np.mean(dist1) + np.mean(dist2) + 1e-8))\n\n        # Reorder clusters based on compatibility\n        current_order = list(range(num_clusters))\n        for _ in range(3):\n            i = np.random.randint(0, num_clusters)\n            j = np.argmax(compatibility_matrix[i])\n            if i != j:\n                current_order[i], current_order[j] = current_order[j], current_order[i]\n\n        new_solution = np.concatenate([clusters[i] for i in current_order])\n    else:\n        new_solution = base_solution.copy()\n\n    # Step 4: Cluster fusion with probability\n    if len(clusters) > 1 and np.random.random() < 0.5:\n        # Find most compatible adjacent clusters\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n\n        for i in range(len(clusters)-1):\n            dist1 = np.linalg.norm(cluster_properties[i]['centroid1'] - cluster_properties[i+1]['centroid1'])\n            dist2 = np.linalg.norm(cluster_properties[i]['centroid2'] - cluster_properties[i+1]['centroid2'])\n            compatibility = np.exp(-(dist1 + dist2) / (np.mean(dist1) + np.mean(dist2) + 1e-8))\n            if compatibility > best_compatibility:\n                best_compatibility = compatibility\n                best_i, best_j = i, i+1\n\n        if best_i != -1 and best_j != -1:\n            # Merge compatible clusters\n            merged_cluster = np.concatenate([clusters[best_i], clusters[best_j]])\n            new_clusters = clusters[:best_i] + [merged_cluster] + clusters[best_j+1:]\n            new_solution = np.concatenate(new_clusters)\n\n    # Step 5: Probabilistic cluster splitting\n    if len(clusters) < 5 and np.random.random() < 0.3:\n        # Select a cluster to split\n        split_cluster_idx = np.random.randint(0, len(clusters))\n        split_cluster = clusters[split_cluster_idx]\n\n        if len(split_cluster) > 3:\n            # Split the cluster\n            coords1 = instance[split_cluster, :2]\n            coords2 = instance[split_cluster, 2:]\n            centroid1 = np.mean(coords1, axis=0)\n            centroid2 = np.mean(coords2, axis=0)\n\n            # Calculate splitting metric\n            dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n            dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n            split_metric = dist1 * dist2\n\n            # Split based on metric\n            sorted_indices = np.argsort(split_metric)\n            split_point = len(split_cluster) // 2\n            left = split_cluster[sorted_indices[:split_point]]\n            right = split_cluster[sorted_indices[split_point:]]\n\n            # Update clusters\n            new_clusters = clusters[:split_cluster_idx] + [left, right] + clusters[split_cluster_idx+1:]\n            new_solution = np.concatenate(new_clusters)\n\n    # Step 6: Feasibility check and repair\n    if len(np.unique(new_solution)) != len(base_solution):\n        # Multi-objective centroid-based repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate combined metric\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        combined_metric = dist1 * dist2\n\n        # Sort nodes by combined metric\n        sorted_indices = np.argsort(combined_metric)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n",
          "score": [
               -0.994485723087885,
               1.1914765238761902
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid multi-objective selection and geometric clustering strategy that first identifies the most geometrically diverse solutions in the archive by combining Pareto dominance with a novel spatial clustering metric, then probabilistically selects a base solution based on both objective performance and spatial distribution of its nodes. It will generate a neighbor solution by applying a geometric clustering operator that groups nodes into spatially coherent clusters in one objective space while preserving relative distances in the other space, using a novel \"objective-space partitioning\" technique that dynamically adjusts the clustering based on the relative improvement potential in each space, with a feasibility-preserving mechanism that ensures the clustered path remains a valid tour through a combination of spatial validation and path reconstruction.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Multi-objective and geometric selection\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-8)\n\n    # Calculate spatial diversity\n    spatial_diversity = []\n    for sol, _ in archive:\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        centroid1 = np.mean(path1, axis=0)\n        centroid2 = np.mean(path2, axis=0)\n        diversity = np.linalg.norm(centroid1 - centroid2)\n        spatial_diversity.append(diversity)\n\n    # Combine metrics for selection\n    combined_metric = normalized_obj[:, 0] + normalized_obj[:, 1] + np.array(spatial_diversity)\n    selected_idx = np.argmax(combined_metric)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space partitioning\n    if n > 3:\n        # Select a segment to cluster\n        segment_length = max(3, n // 4)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate clustering parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n\n        # Apply clustering transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Cluster in first objective space\n            vec1 = instance[node, :2] - centroid1\n            instance[node, :2] = centroid1 + 0.5 * vec1\n            # Preserve relative distances in second space\n            vec2 = instance[node, 2:] - centroid2\n            instance[node, 2:] = centroid2 + 1.5 * vec2\n\n        # Rebuild solution to maintain feasibility\n        temp_solution = []\n        remaining_nodes = set(range(n)) - set(segment)\n        temp_solution.extend(segment)\n        temp_solution.extend(list(remaining_nodes))\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair by randomizing the order of remaining nodes\n        remaining = list(set(range(n)) - set(new_solution[:n]))\n        random.shuffle(remaining)\n        new_solution[n-len(remaining):] = remaining\n\n    return new_solution\n\n",
          "score": [
               -1.2100000000000002,
               9.658039271831512
          ]
     },
     {
          "algorithm": "{The new algorithm builds upon the geometric clustering approach from the provided solutions but replaces the static centroid-based clustering with a dynamic, adaptive partitioning strategy that uses multi-objective k-means clustering to identify natural groupings in the solution space. It then applies a novel \"geometric inversion\" operator that flips the order of nodes within each cluster while maintaining compatibility between adjacent clusters, followed by a probabilistic \"objective-aware\" edge swap that considers both distance matrices to further improve the solution's multi-objective quality. The method ensures feasibility by always maintaining a valid tour and includes a multi-objective repair mechanism that prioritizes nodes with the worst combined geometric and distance-based deviations.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution (weighted by inverse objective sum)\n    objectives = np.array([obj for _, obj in archive])\n    norm_obj = objectives / (np.max(objectives, axis=0) + 1e-8)\n    combined_scores = 1.0 / (1.0 + np.sum(norm_obj, axis=1))\n    probabilities = combined_scores / np.sum(combined_scores)\n    selected_idx = np.random.choice(len(archive), p=probabilities)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Multi-objective k-means clustering\n    def multi_objective_kmeans(solution, k=3):\n        coords1 = instance[solution, :2]\n        coords2 = instance[solution, 2:]\n        n = len(solution)\n\n        # Initialize centroids randomly\n        centroids1 = coords1[np.random.choice(n, k, replace=False)]\n        centroids2 = coords2[np.random.choice(n, k, replace=False)]\n\n        for _ in range(10):  # Iterate\n            # Assign clusters\n            dist1 = np.linalg.norm(coords1[:, np.newaxis] - centroids1, axis=2)\n            dist2 = np.linalg.norm(coords2[:, np.newaxis] - centroids2, axis=2)\n            clusters = np.argmin(dist1 + dist2, axis=1)\n\n            # Update centroids\n            for i in range(k):\n                if np.sum(clusters == i) > 0:\n                    centroids1[i] = np.mean(coords1[clusters == i], axis=0)\n                    centroids2[i] = np.mean(coords2[clusters == i], axis=0)\n\n        # Create clusters\n        cluster_list = []\n        for i in range(k):\n            cluster_list.append(solution[clusters == i])\n        return cluster_list\n\n    clusters = multi_objective_kmeans(base_solution, k=min(4, len(base_solution)//3))\n\n    # Step 3: Geometric inversion operator\n    new_solution = []\n    for cluster in clusters:\n        if len(cluster) > 1 and np.random.random() < 0.7:\n            # Flip the cluster with probability\n            cluster = cluster[::-1]\n        new_solution.extend(cluster)\n    new_solution = np.array(new_solution)\n\n    # Step 4: Objective-aware edge swap\n    if len(new_solution) > 2:\n        n = len(new_solution)\n        i = np.random.randint(0, n)\n        j = np.random.randint(0, n)\n\n        # Calculate current and potential costs\n        current_cost1 = distance_matrix_1[new_solution[i-1], new_solution[i]] + distance_matrix_1[new_solution[j-1], new_solution[j]]\n        current_cost2 = distance_matrix_2[new_solution[i-1], new_solution[i]] + distance_matrix_2[new_solution[j-1], new_solution[j]]\n\n        swapped_cost1 = distance_matrix_1[new_solution[i-1], new_solution[j]] + distance_matrix_1[new_solution[j-1], new_solution[i]]\n        swapped_cost2 = distance_matrix_2[new_solution[i-1], new_solution[j]] + distance_matrix_2[new_solution[j-1], new_solution[i]]\n\n        # Accept if improvement in at least one objective\n        if (swapped_cost1 < current_cost1) or (swapped_cost2 < current_cost2):\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != len(base_solution):\n        # Multi-objective repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate combined metric\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        combined_metric = dist1 * dist2\n\n        # Sort nodes by combined metric\n        sorted_indices = np.argsort(combined_metric)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n",
          "score": [
               -1.0233195821505083,
               1.9273075461387634
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a multi-phase geometric fragmentation and reassembly strategy that first decomposes the solution tour into geometrically distinct segments based on the relative positions of nodes in both objective spaces, then probabilistically selects and reorders these segments using a novel geometric harmony metric that balances the spatial distribution and objective contributions of each segment, followed by a constrained segment fusion process that dynamically merges adjacent segments based on their geometric compatibility and potential for Pareto improvement, with feasibility maintained through a segment boundary validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a geometric centroid-based ordering approach if invalidity is detected.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution based on combined objective quality\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Geometric fragmentation\n    if n > 4:\n        # Calculate geometric properties for each node in both spaces\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n\n        # Calculate centroids and angles\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate angles relative to centroids\n        angles1 = np.arctan2(coords1[:,1] - centroid1[1], coords1[:,0] - centroid1[0])\n        angles2 = np.arctan2(coords2[:,1] - centroid2[1], coords2[:,0] - centroid2[0])\n\n        # Calculate geometric harmony metric\n        angle_diff = np.abs(angles1 - angles2)\n        harmony = 1.0 / (1.0 + angle_diff)\n\n        # Sort nodes by harmony\n        sorted_indices = np.argsort(harmony)[::-1]\n\n        # Split into segments based on harmony\n        num_segments = max(2, min(4, n // 3))\n        segment_size = n // num_segments\n        segments = [new_solution[sorted_indices[i*segment_size:(i+1)*segment_size]] for i in range(num_segments)]\n\n        # Step 3: Segment reordering\n        # Calculate segment properties\n        segment_properties = []\n        for seg in segments:\n            seg_coords1 = instance[seg, :2]\n            seg_coords2 = instance[seg, 2:]\n            seg_centroid1 = np.mean(seg_coords1, axis=0)\n            seg_centroid2 = np.mean(seg_coords2, axis=0)\n            seg_properties = {\n                'centroid1': seg_centroid1,\n                'centroid2': seg_centroid2,\n                'size': len(seg),\n                'coords1': seg_coords1,\n                'coords2': seg_coords2\n            }\n            segment_properties.append(seg_properties)\n\n        # Calculate segment compatibility\n        compatibility_matrix = np.zeros((num_segments, num_segments))\n        for i in range(num_segments):\n            for j in range(num_segments):\n                if i != j:\n                    dist1 = np.linalg.norm(segment_properties[i]['centroid1'] - segment_properties[j]['centroid1'])\n                    dist2 = np.linalg.norm(segment_properties[i]['centroid2'] - segment_properties[j]['centroid2'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist1 + dist2)\n\n        # Reorder segments based on compatibility\n        current_order = list(range(num_segments))\n        for _ in range(2):\n            i = random.randint(0, num_segments-1)\n            j = np.argmax(compatibility_matrix[i])\n            if i != j:\n                current_order[i], current_order[j] = current_order[j], current_order[i]\n\n        # Reconstruct solution from reordered segments\n        new_solution = np.concatenate([segments[i] for i in current_order])\n\n    # Step 4: Segment fusion\n    if n > 5 and random.random() < 0.3:\n        # Find the most compatible adjacent segments\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n\n        for i in range(len(segments)-1):\n            dist1 = np.linalg.norm(segment_properties[i]['centroid1'] - segment_properties[i+1]['centroid1'])\n            dist2 = np.linalg.norm(segment_properties[i]['centroid2'] - segment_properties[i+1]['centroid2'])\n            compatibility = 1.0 / (1.0 + dist1 + dist2)\n            if compatibility > best_compatibility:\n                best_compatibility = compatibility\n                best_i, best_j = i, i+1\n\n        if best_i != -1 and best_j != -1:\n            # Merge the compatible segments\n            merged_segment = np.concatenate([segments[best_i], segments[best_j]])\n            new_segments = segments[:best_i] + [merged_segment] + segments[best_j+1:]\n            new_solution = np.concatenate(new_segments)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != n:\n        # Geometric centroid-based repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate angles relative to centroids\n        angles1 = np.arctan2(coords1[:,1] - centroid1[1], coords1[:,0] - centroid1[0])\n        angles2 = np.arctan2(coords2[:,1] - centroid2[1], coords2[:,0] - centroid2[0])\n\n        # Sort nodes by combined angle\n        combined_angles = angles1 + angles2\n        sorted_indices = np.argsort(combined_angles)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n",
          "score": [
               -0.934290307549595,
               0.31503432989120483
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid multi-objective selection and geometric transformation strategy that first identifies the most balanced solutions in the archive by combining Pareto dominance with a novel geometric diversity metric, then probabilistically selects a base solution based on both objective performance and geometric alignment of its path segments. It will generate a neighbor solution by applying a geometric transformation operator that warps the tour in one objective space while preserving geometric consistency in the other space, using a novel \"objective-space morphing\" technique that dynamically adjusts the transformation based on the relative improvement potential in each space, with a feasibility-preserving mechanism that ensures the transformed path remains a valid tour through a combination of geometric validation and path reconstruction.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Multi-objective balanced selection\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-8)\n\n    # Calculate geometric diversity\n    geometric_diversity = []\n    for sol, _ in archive:\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        centroid1 = np.mean(path1, axis=0)\n        centroid2 = np.mean(path2, axis=0)\n        diversity = np.linalg.norm(centroid1 - centroid2)\n        geometric_diversity.append(diversity)\n\n    # Combine metrics for selection\n    combined_metric = normalized_obj[:, 0] + normalized_obj[:, 1] + np.array(geometric_diversity)\n    selected_idx = np.argmax(combined_metric)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space morphing transformation\n    if n > 3:\n        # Select a segment to transform\n        segment_length = max(3, n // 4)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate transformation parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n        scale_factor = 0.8 + 0.4 * random.random()\n\n        # Apply geometric transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Transform in first objective space\n            vec1 = instance[node, :2] - centroid1\n            instance[node, :2] = centroid1 + scale_factor * vec1\n            # Transform in second objective space\n            vec2 = instance[node, 2:] - centroid2\n            instance[node, 2:] = centroid2 + (1.0 / scale_factor) * vec2\n\n        # Rebuild solution to maintain feasibility\n        temp_solution = []\n        remaining_nodes = set(range(n)) - set(segment)\n        temp_solution.extend(segment)\n        temp_solution.extend(list(remaining_nodes))\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair by randomizing the order of remaining nodes\n        remaining = list(set(range(n)) - set(new_solution[:n]))\n        random.shuffle(remaining)\n        new_solution[n-len(remaining):] = remaining\n\n    return new_solution\n\n",
          "score": [
               -1.0180486271477518,
               3.075184404850006
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid objective-space partitioning and adaptive tour reconstruction strategy that first identifies promising regions of the objective space through a novel multi-objective grid partitioning approach, then selects a base solution from the most under-explored partition to balance exploration and exploitation. It dynamically decomposes the tour into segments based on alternating objective-space improvement patterns, applies a geometric path smoothing operator to optimize local segments while preserving global structure, and reconstructs the tour with an adaptive segment insertion mechanism that prioritizes edges showing complementary improvements in both objectives. The method ensures feasibility through a geometric validation step and a partition-aware repair mechanism, while incorporating a dynamic objective weighting scheme that adapts to the current archive's Pareto front characteristics to guide the local search toward more balanced solutions in the objective space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Objective-space partitioning\n    objectives = np.array([obj for _, obj in archive])\n    grid_size = max(3, int(np.sqrt(len(archive)) / 2))\n    grid_x = np.linspace(objectives[:, 0].min(), objectives[:, 0].max(), grid_size + 1)\n    grid_y = np.linspace(objectives[:, 1].min(), objectives[:, 1].max(), grid_size + 1)\n\n    # Assign partitions\n    partitions = {}\n    for i, obj in enumerate(objectives):\n        x_idx = np.digitize(obj[0], grid_x) - 1\n        y_idx = np.digitize(obj[1], grid_y) - 1\n        partitions.setdefault((x_idx, y_idx), []).append(i)\n\n    # Select from least explored partition\n    partition_counts = {k: len(v) for k, v in partitions.items()}\n    selected_partition = min(partition_counts, key=partition_counts.get)\n    partition_indices = partitions[selected_partition]\n\n    if not partition_indices:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = np.random.choice(partition_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Dynamic tour decomposition\n    decomposition_points = []\n    for i in range(n-1):\n        u, v = new_solution[i], new_solution[i+1]\n        obj_u = archive[selected_idx][1]\n        obj_v = (distance_matrix_1[u,v], distance_matrix_2[u,v])\n        if (obj_v[0] < obj_u[0] and obj_v[1] <= obj_u[1]) or (obj_v[0] <= obj_u[0] and obj_v[1] < obj_u[1]):\n            decomposition_points.append(i+1)\n\n    # Step 3: Geometric path smoothing\n    if decomposition_points:\n        segments = []\n        start = 0\n        for point in sorted(decomposition_points):\n            segments.append(new_solution[start:point+1])\n            start = point+1\n        if start < n:\n            segments.append(new_solution[start:])\n\n        # Apply smoothing to each segment\n        for i in range(len(segments)):\n            segment = segments[i]\n            if len(segment) > 3:\n                # Sort by alternating objective coordinates\n                if i % 2 == 0:\n                    segment = sorted(segment, key=lambda x: instance[x, 0] + instance[x, 2])\n                else:\n                    segment = sorted(segment, key=lambda x: instance[x, 1] + instance[x, 3])\n                segments[i] = segment\n\n        new_solution = np.concatenate(segments)\n\n    # Step 4: Adaptive segment insertion\n    if len(decomposition_points) > 1 and np.random.random() < 0.3:\n        insert_pos = np.random.choice(decomposition_points)\n        new_segment = np.random.permutation(new_solution[insert_pos-2:insert_pos+2])\n        new_solution = np.concatenate([new_solution[:insert_pos], new_segment, new_solution[insert_pos+2:]])\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -0.9555656211302912,
               1.22073096036911
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid Pareto-geometric selection and path reconstruction strategy that combines a probabilistic dominance-based solution selection with a novel objective-space alignment operator. First, it will use non-dominated sorting to identify promising solutions in the archive, then probabilistically select a base solution based on its Pareto rank. Next, it will analyze the geometric alignment of nodes in both objective spaces using a custom alignment metric, and reconstruct the tour by sequentially connecting nodes that show the highest alignment potential in both spaces, with a dynamic segment reordering mechanism that alternates between objective spaces to balance improvement in both dimensions. The method will ensure feasibility through a permutation validation step and a geometric repair mechanism if needed, while prioritizing edges that show the most promising alignment improvement in both spaces.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Pareto-based probabilistic selection\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and all(objectives[j] <= objectives[i]) and any(objectives[j] < objectives[i]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if pareto_front:\n        selected_idx = np.random.choice(pareto_front)\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space alignment analysis\n    alignment_scores = []\n    for i in range(n):\n        u = new_solution[i]\n        v = new_solution[(i + 1) % n]\n\n        # Calculate alignment metric: cosine similarity of direction vectors\n        vec1 = instance[v, :2] - instance[u, :2]\n        vec2 = instance[v, 2:] - instance[u, 2:]\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n\n        if norm1 > 0 and norm2 > 0:\n            cos_sim = np.dot(vec1, vec2) / (norm1 * norm2)\n            alignment_scores.append((i, cos_sim))\n        else:\n            alignment_scores.append((i, 0.0))\n\n    # Step 3: Dynamic segment reordering\n    if n > 3:\n        # Sort segments by alignment score (highest first)\n        alignment_scores.sort(key=lambda x: -x[1])\n        selected_segments = [x[0] for x in alignment_scores[:max(2, n//5)]]\n\n        # Reorder segments in alternating objective spaces\n        toggle = True\n        new_order = []\n        used_nodes = set()\n\n        for seg in selected_segments:\n            if toggle:\n                # Connect in first objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([u, v])\n            else:\n                # Connect in second objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([v, u])\n            toggle = not toggle\n\n        # Add remaining nodes\n        remaining_nodes = [node for node in base_solution if node not in new_order]\n        new_order.extend(remaining_nodes)\n\n        new_solution = np.array(new_order)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -1.0035997006156805,
               2.016536831855774
          ]
     }
]