[
     {
          "prompt": "You are solving a Bi-objective Travelling Salesman Problem (bi-TSP), where each node has two different 2D coordinates: (x1, y1) and (x2, y2), representing its position in two objective spaces. The goal is to find a tour visiting each node exactly once and returning to the starting node, while minimizing two objectives simultaneously: the total tour length in each coordinate space. Given an archive of solutions, where each solution is a numpy array representing a TSP tour, and its corresponding objective is a tuple of two values (cost in each space), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it.  Must always ensure that the generated neighbor solution remains feasible, i.e., the solution must represent a valid TSP tour: it visits each node exactly once, ensuring no node is skipped or revisited.Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm will employ a hybrid objective-space partitioning and adaptive tour reconstruction strategy that first identifies promising regions of the objective space through a novel multi-objective grid partitioning approach, then selects a base solution from the most under-explored partition to balance exploration and exploitation. It dynamically decomposes the tour into segments based on alternating objective-space improvement patterns, applies a geometric path smoothing operator to optimize local segments while preserving global structure, and reconstructs the tour with an adaptive segment insertion mechanism that prioritizes edges showing complementary improvements in both objectives. The method ensures feasibility through a geometric validation step and a partition-aware repair mechanism, while incorporating a dynamic objective weighting scheme that adapts to the current archive's Pareto front characteristics to guide the local search toward more balanced solutions in the objective space.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Objective-space partitioning\n    objectives = np.array([obj for _, obj in archive])\n    grid_size = max(3, int(np.sqrt(len(archive)) / 2))\n    grid_x = np.linspace(objectives[:, 0].min(), objectives[:, 0].max(), grid_size + 1)\n    grid_y = np.linspace(objectives[:, 1].min(), objectives[:, 1].max(), grid_size + 1)\n\n    # Assign partitions\n    partitions = {}\n    for i, obj in enumerate(objectives):\n        x_idx = np.digitize(obj[0], grid_x) - 1\n        y_idx = np.digitize(obj[1], grid_y) - 1\n        partitions.setdefault((x_idx, y_idx), []).append(i)\n\n    # Select from least explored partition\n    partition_counts = {k: len(v) for k, v in partitions.items()}\n    selected_partition = min(partition_counts, key=partition_counts.get)\n    partition_indices = partitions[selected_partition]\n\n    if not partition_indices:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = np.random.choice(partition_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Dynamic tour decomposition\n    decomposition_points = []\n    for i in range(n-1):\n        u, v = new_solution[i], new_solution[i+1]\n        obj_u = archive[selected_idx][1]\n        obj_v = (distance_matrix_1[u,v], distance_matrix_2[u,v])\n        if (obj_v[0] < obj_u[0] and obj_v[1] <= obj_u[1]) or (obj_v[0] <= obj_u[0] and obj_v[1] < obj_u[1]):\n            decomposition_points.append(i+1)\n\n    # Step 3: Geometric path smoothing\n    if decomposition_points:\n        segments = []\n        start = 0\n        for point in sorted(decomposition_points):\n            segments.append(new_solution[start:point+1])\n            start = point+1\n        if start < n:\n            segments.append(new_solution[start:])\n\n        # Apply smoothing to each segment\n        for i in range(len(segments)):\n            segment = segments[i]\n            if len(segment) > 3:\n                # Sort by alternating objective coordinates\n                if i % 2 == 0:\n                    segment = sorted(segment, key=lambda x: instance[x, 0] + instance[x, 2])\n                else:\n                    segment = sorted(segment, key=lambda x: instance[x, 1] + instance[x, 3])\n                segments[i] = segment\n\n        new_solution = np.concatenate(segments)\n\n    # Step 4: Adaptive segment insertion\n    if len(decomposition_points) > 1 and np.random.random() < 0.3:\n        insert_pos = np.random.choice(decomposition_points)\n        new_segment = np.random.permutation(new_solution[insert_pos-2:insert_pos+2])\n        new_solution = np.concatenate([new_solution[:insert_pos], new_segment, new_solution[insert_pos+2:]])\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The new algorithm will employ a hybrid Pareto-geometric selection and path reconstruction strategy that combines a probabilistic dominance-based solution selection with a novel objective-space alignment operator. First, it will use non-dominated sorting to identify promising solutions in the archive, then probabilistically select a base solution based on its Pareto rank. Next, it will analyze the geometric alignment of nodes in both objective spaces using a custom alignment metric, and reconstruct the tour by sequentially connecting nodes that show the highest alignment potential in both spaces, with a dynamic segment reordering mechanism that alternates between objective spaces to balance improvement in both dimensions. The method will ensure feasibility through a permutation validation step and a geometric repair mechanism if needed, while prioritizing edges that show the most promising alignment improvement in both spaces.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Pareto-based probabilistic selection\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and all(objectives[j] <= objectives[i]) and any(objectives[j] < objectives[i]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if pareto_front:\n        selected_idx = np.random.choice(pareto_front)\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space alignment analysis\n    alignment_scores = []\n    for i in range(n):\n        u = new_solution[i]\n        v = new_solution[(i + 1) % n]\n\n        # Calculate alignment metric: cosine similarity of direction vectors\n        vec1 = instance[v, :2] - instance[u, :2]\n        vec2 = instance[v, 2:] - instance[u, 2:]\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n\n        if norm1 > 0 and norm2 > 0:\n            cos_sim = np.dot(vec1, vec2) / (norm1 * norm2)\n            alignment_scores.append((i, cos_sim))\n        else:\n            alignment_scores.append((i, 0.0))\n\n    # Step 3: Dynamic segment reordering\n    if n > 3:\n        # Sort segments by alignment score (highest first)\n        alignment_scores.sort(key=lambda x: -x[1])\n        selected_segments = [x[0] for x in alignment_scores[:max(2, n//5)]]\n\n        # Reorder segments in alternating objective spaces\n        toggle = True\n        new_order = []\n        used_nodes = set()\n\n        for seg in selected_segments:\n            if toggle:\n                # Connect in first objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([u, v])\n            else:\n                # Connect in second objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([v, u])\n            toggle = not toggle\n\n        # Add remaining nodes\n        remaining_nodes = [node for node in base_solution if node not in new_order]\n        new_order.extend(remaining_nodes)\n\n        new_solution = np.array(new_order)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 301,
          "algorithm": "{The new algorithm will employ a hybrid objective-space clustering and geometric path optimization strategy that first groups solutions in the archive by their objective-space characteristics using a density-based clustering approach, then probabilistically selects a base solution from the least dense cluster to focus on under-explored regions. It then identifies critical path segments by analyzing the geometric relationships between nodes in both objective spaces using a novel \"bi-objective convex hull\" metric, and reconstructs the tour by iteratively optimizing these segments with a geometric path smoothing operator that alternates between objective spaces to balance improvements in both dimensions. The method ensures feasibility through a topological validation step and a cluster-aware repair mechanism, while dynamically adjusting the clustering parameters based on the current archive's Pareto front characteristics to guide the search toward more balanced solutions in the objective space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Objective-space clustering\n    objectives = np.array([obj for _, obj in archive])\n    from sklearn.cluster import DBSCAN\n\n    clustering = DBSCAN(eps=np.std(objectives) / 2, min_samples=2).fit(objectives)\n    labels = clustering.labels_\n\n    # Select from least dense cluster\n    unique_labels = set(labels)\n    cluster_counts = {label: np.sum(labels == label) for label in unique_labels}\n    selected_label = min(cluster_counts, key=cluster_counts.get)\n    cluster_indices = [i for i, label in enumerate(labels) if label == selected_label]\n\n    if not cluster_indices:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = np.random.choice(cluster_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Bi-objective convex hull analysis\n    from scipy.spatial import ConvexHull\n    critical_segments = []\n\n    for i in range(n-1):\n        segment = new_solution[i:i+2]\n        coords = instance[segment, :4]\n        try:\n            hull = ConvexHull(coords)\n            if hull.volume > 0.1 * np.max(hull.volume):  # Threshold for critical segments\n                critical_segments.append(i)\n        except:\n            continue\n\n    # Step 3: Geometric path optimization\n    if critical_segments:\n        toggle = True\n        for seg in sorted(critical_segments, reverse=True):\n            segment = new_solution[seg:seg+2]\n            if toggle:\n                # Optimize in first objective space\n                u, v = segment\n                neighbors = [i for i in range(n) if i != u and i != v]\n                for i in neighbors:\n                    new_cost1 = distance_matrix_1[u,i] + distance_matrix_1[i,v] - distance_matrix_1[u,v]\n                    new_cost2 = distance_matrix_2[u,i] + distance_matrix_2[i,v] - distance_matrix_2[u,v]\n                    if new_cost1 < 0 and new_cost2 < 0:\n                        new_solution = np.concatenate([new_solution[:seg], [u, i, v], new_solution[seg+2:]])\n                        break\n            else:\n                # Optimize in second objective space\n                u, v = segment\n                neighbors = [i for i in range(n) if i != u and i != v]\n                for i in neighbors:\n                    new_cost1 = distance_matrix_1[u,i] + distance_matrix_1[i,v] - distance_matrix_1[u,v]\n                    new_cost2 = distance_matrix_2[u,i] + distance_matrix_2[i,v] - distance_matrix_2[u,v]\n                    if new_cost2 < 0 and new_cost1 < 0:\n                        new_solution = np.concatenate([new_solution[:seg], [u, i, v], new_solution[seg+2:]])\n                        break\n            toggle = not toggle\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -0.9141514580406418,
               3.40961617231369
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Objective-space clustering\n    objectives = np.array([obj for _, obj in archive])\n    from sklearn.cluster import DBSCAN\n\n    clustering = DBSCAN(eps=np.std(objectives) / 2, min_samples=2).fit(objectives)\n    labels = clustering.labels_\n\n    # Select from least dense cluster\n    unique_labels = set(labels)\n    cluster_counts = {label: np.sum(labels == label) for label in unique_labels}\n    selected_label = min(cluster_counts, key=cluster_counts.get)\n    cluster_indices = [i for i, label in enumerate(labels) if label == selected_label]\n\n    if not cluster_indices:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = np.random.choice(cluster_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Bi-objective convex hull analysis\n    from scipy.spatial import ConvexHull\n    critical_segments = []\n\n    for i in range(n-1):\n        segment = new_solution[i:i+2]\n        coords = instance[segment, :4]\n        try:\n            hull = ConvexHull(coords)\n            if hull.volume > 0.1 * np.max(hull.volume):  # Threshold for critical segments\n                critical_segments.append(i)\n        except:\n            continue\n\n    # Step 3: Geometric path optimization\n    if critical_segments:\n        toggle = True\n        for seg in sorted(critical_segments, reverse=True):\n            segment = new_solution[seg:seg+2]\n            if toggle:\n                # Optimize in first objective space\n                u, v = segment\n                neighbors = [i for i in range(n) if i != u and i != v]\n                for i in neighbors:\n                    new_cost1 = distance_matrix_1[u,i] + distance_matrix_1[i,v] - distance_matrix_1[u,v]\n                    new_cost2 = distance_matrix_2[u,i] + distance_matrix_2[i,v] - distance_matrix_2[u,v]\n                    if new_cost1 < 0 and new_cost2 < 0:\n                        new_solution = np.concatenate([new_solution[:seg], [u, i, v], new_solution[seg+2:]])\n                        break\n            else:\n                # Optimize in second objective space\n                u, v = segment\n                neighbors = [i for i in range(n) if i != u and i != v]\n                for i in neighbors:\n                    new_cost1 = distance_matrix_1[u,i] + distance_matrix_1[i,v] - distance_matrix_1[u,v]\n                    new_cost2 = distance_matrix_2[u,i] + distance_matrix_2[i,v] - distance_matrix_2[u,v]\n                    if new_cost2 < 0 and new_cost1 < 0:\n                        new_solution = np.concatenate([new_solution[:seg], [u, i, v], new_solution[seg+2:]])\n                        break\n            toggle = not toggle\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Travelling Salesman Problem (bi-TSP), where each node has two different 2D coordinates: (x1, y1) and (x2, y2), representing its position in two objective spaces. The goal is to find a tour visiting each node exactly once and returning to the starting node, while minimizing two objectives simultaneously: the total tour length in each coordinate space. Given an archive of solutions, where each solution is a numpy array representing a TSP tour, and its corresponding objective is a tuple of two values (cost in each space), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it.  Must always ensure that the generated neighbor solution remains feasible, i.e., the solution must represent a valid TSP tour: it visits each node exactly once, ensuring no node is skipped or revisited.Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm will employ a hybrid multi-objective selection and geometric clustering strategy that first identifies the most geometrically diverse solutions in the archive by combining Pareto dominance with a novel spatial clustering metric, then probabilistically selects a base solution based on both objective performance and spatial distribution of its nodes. It will generate a neighbor solution by applying a geometric clustering operator that groups nodes into spatially coherent clusters in one objective space while preserving relative distances in the other space, using a novel \"objective-space partitioning\" technique that dynamically adjusts the clustering based on the relative improvement potential in each space, with a feasibility-preserving mechanism that ensures the clustered path remains a valid tour through a combination of spatial validation and path reconstruction.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Multi-objective and geometric selection\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-8)\n\n    # Calculate spatial diversity\n    spatial_diversity = []\n    for sol, _ in archive:\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        centroid1 = np.mean(path1, axis=0)\n        centroid2 = np.mean(path2, axis=0)\n        diversity = np.linalg.norm(centroid1 - centroid2)\n        spatial_diversity.append(diversity)\n\n    # Combine metrics for selection\n    combined_metric = normalized_obj[:, 0] + normalized_obj[:, 1] + np.array(spatial_diversity)\n    selected_idx = np.argmax(combined_metric)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space partitioning\n    if n > 3:\n        # Select a segment to cluster\n        segment_length = max(3, n // 4)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate clustering parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n\n        # Apply clustering transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Cluster in first objective space\n            vec1 = instance[node, :2] - centroid1\n            instance[node, :2] = centroid1 + 0.5 * vec1\n            # Preserve relative distances in second space\n            vec2 = instance[node, 2:] - centroid2\n            instance[node, 2:] = centroid2 + 1.5 * vec2\n\n        # Rebuild solution to maintain feasibility\n        temp_solution = []\n        remaining_nodes = set(range(n)) - set(segment)\n        temp_solution.extend(segment)\n        temp_solution.extend(list(remaining_nodes))\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair by randomizing the order of remaining nodes\n        remaining = list(set(range(n)) - set(new_solution[:n]))\n        random.shuffle(remaining)\n        new_solution[n-len(remaining):] = remaining\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The new algorithm will employ a hierarchical multi-objective selection strategy that first identifies the most Pareto-efficient solutions in the archive, then uses a novel \"objective-space decomposition\" technique to probabilistically select a base solution based on both objective performance and the geometric properties of its node distribution across both spaces. It will generate a neighbor solution by applying a hybrid local search operator that combines a geometric clustering approach with a dynamic objective-space partitioning strategy, where nodes are grouped based on their relative positions in one space while maintaining the relative ordering of their distances in the other space. The algorithm uses a feasibility-preserving mechanism that ensures the solution remains a valid tour through a combination of spatial validation, path reconstruction, and a novel \"objective-space warping\" technique that dynamically adjusts the clustering based on the relative improvement potential in each space, with a probabilistic repair mechanism to handle any infeasible solutions that may arise during the transformation.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hierarchical multi-objective selection\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i, obj in enumerate(objectives):\n        dominated = False\n        for j, other_obj in enumerate(objectives):\n            if i != j and (other_obj[0] <= obj[0] and other_obj[1] <= obj[1]) and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    # Calculate geometric properties\n    geometric_metrics = []\n    for idx in pareto_front:\n        sol = archive[idx][0]\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        centroid1 = np.mean(path1, axis=0)\n        centroid2 = np.mean(path2, axis=0)\n        spread1 = np.max(np.linalg.norm(path1 - centroid1, axis=1))\n        spread2 = np.max(np.linalg.norm(path2 - centroid2, axis=1))\n        geometric_metrics.append(spread1 + spread2)\n\n    selected_idx = pareto_front[np.argmax(geometric_metrics)]\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space decomposition and transformation\n    if n > 3:\n        # Select a segment to transform\n        segment_length = max(3, n // 5)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate transformation parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n\n        # Apply geometric transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Warp in first objective space\n            vec1 = instance[node, :2] - centroid1\n            angle = np.arctan2(vec1[1], vec1[0])\n            radius = np.linalg.norm(vec1)\n            new_angle = angle + 0.5 * np.pi * np.random.randn()\n            new_radius = radius * (1 + 0.3 * np.random.randn())\n            instance[node, :2] = centroid1 + np.array([new_radius * np.cos(new_angle), new_radius * np.sin(new_angle)])\n\n            # Preserve relative distances in second space\n            vec2 = instance[node, 2:] - centroid2\n            instance[node, 2:] = centroid2 + 0.8 * vec2\n\n        # Rebuild solution to maintain feasibility\n        temp_solution = []\n        remaining_nodes = set(range(n)) - set(segment)\n        temp_solution.extend(segment)\n        temp_solution.extend(list(remaining_nodes))\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair by randomizing the order of remaining nodes\n        remaining = list(set(range(n)) - set(new_solution[:n]))\n        random.shuffle(remaining)\n        new_solution[n-len(remaining):] = remaining\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 302,
          "algorithm": "{The new algorithm will employ a novel adaptive multi-objective selection strategy that first identifies the most geometrically diverse solutions in the archive by combining Pareto efficiency with a dynamic spatial entropy metric, then probabilistically selects a base solution based on both objective performance and the local curvature of its path in both spaces. It will generate a neighbor solution by applying a hybrid local search operator that combines a geometric fractal transformation with a dynamic objective-space repulsion strategy, where nodes are repositioned based on their relative positions in one space while maintaining the relative ordering of their distances in the other space, with a feasibility-preserving mechanism that ensures the solution remains a valid tour through a combination of spatial validation, path reconstruction, and a novel \"objective-space fractal warping\" technique that dynamically adjusts the transformation based on the relative improvement potential in each space, incorporating a probabilistic repair mechanism to handle any infeasible solutions that may arise during the transformation.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive multi-objective selection with spatial entropy\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-8)\n\n    spatial_entropy = []\n    for sol, _ in archive:\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        centroid1 = np.mean(path1, axis=0)\n        centroid2 = np.mean(path2, axis=0)\n\n        # Calculate spatial entropy in both spaces\n        distances1 = np.linalg.norm(path1 - centroid1, axis=1)\n        distances2 = np.linalg.norm(path2 - centroid2, axis=1)\n        entropy1 = -np.sum((distances1 / np.sum(distances1)) * np.log(distances1 / np.sum(distances1) + 1e-8))\n        entropy2 = -np.sum((distances2 / np.sum(distances2)) * np.log(distances2 / np.sum(distances2) + 1e-8))\n        spatial_entropy.append(entropy1 + entropy2)\n\n    # Combine metrics for selection\n    combined_metric = normalized_obj[:, 0] + normalized_obj[:, 1] + np.array(spatial_entropy)\n    selected_idx = np.argmax(combined_metric)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Geometric fractal transformation with dynamic repulsion\n    if n > 3:\n        # Select a segment to transform\n        segment_length = max(3, n // 6)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate transformation parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n\n        # Apply fractal transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Fractal transformation in first space\n            vec1 = instance[node, :2] - centroid1\n            angle = np.arctan2(vec1[1], vec1[0])\n            radius = np.linalg.norm(vec1)\n            new_angle = angle + 0.3 * np.pi * np.random.randn()\n            new_radius = radius * (1 + 0.2 * np.random.randn())\n            instance[node, :2] = centroid1 + np.array([new_radius * np.cos(new_angle), new_radius * np.sin(new_angle)])\n\n            # Dynamic repulsion in second space\n            vec2 = instance[node, 2:] - centroid2\n            repulsion_factor = 1 + 0.4 * np.random.randn()\n            instance[node, 2:] = centroid2 + repulsion_factor * vec2\n\n        # Rebuild solution to maintain feasibility\n        temp_solution = []\n        remaining_nodes = set(range(n)) - set(segment)\n        temp_solution.extend(segment)\n        temp_solution.extend(list(remaining_nodes))\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair by randomizing the order of remaining nodes\n        remaining = list(set(range(n)) - set(new_solution[:n]))\n        random.shuffle(remaining)\n        new_solution[n-len(remaining):] = remaining\n\n    return new_solution\n\n",
          "score": [
               -0.9602617270262866,
               5.987972915172577
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive multi-objective selection with spatial entropy\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-8)\n\n    spatial_entropy = []\n    for sol, _ in archive:\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        centroid1 = np.mean(path1, axis=0)\n        centroid2 = np.mean(path2, axis=0)\n\n        # Calculate spatial entropy in both spaces\n        distances1 = np.linalg.norm(path1 - centroid1, axis=1)\n        distances2 = np.linalg.norm(path2 - centroid2, axis=1)\n        entropy1 = -np.sum((distances1 / np.sum(distances1)) * np.log(distances1 / np.sum(distances1) + 1e-8))\n        entropy2 = -np.sum((distances2 / np.sum(distances2)) * np.log(distances2 / np.sum(distances2) + 1e-8))\n        spatial_entropy.append(entropy1 + entropy2)\n\n    # Combine metrics for selection\n    combined_metric = normalized_obj[:, 0] + normalized_obj[:, 1] + np.array(spatial_entropy)\n    selected_idx = np.argmax(combined_metric)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Geometric fractal transformation with dynamic repulsion\n    if n > 3:\n        # Select a segment to transform\n        segment_length = max(3, n // 6)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate transformation parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n\n        # Apply fractal transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Fractal transformation in first space\n            vec1 = instance[node, :2] - centroid1\n            angle = np.arctan2(vec1[1], vec1[0])\n            radius = np.linalg.norm(vec1)\n            new_angle = angle + 0.3 * np.pi * np.random.randn()\n            new_radius = radius * (1 + 0.2 * np.random.randn())\n            instance[node, :2] = centroid1 + np.array([new_radius * np.cos(new_angle), new_radius * np.sin(new_angle)])\n\n            # Dynamic repulsion in second space\n            vec2 = instance[node, 2:] - centroid2\n            repulsion_factor = 1 + 0.4 * np.random.randn()\n            instance[node, 2:] = centroid2 + repulsion_factor * vec2\n\n        # Rebuild solution to maintain feasibility\n        temp_solution = []\n        remaining_nodes = set(range(n)) - set(segment)\n        temp_solution.extend(segment)\n        temp_solution.extend(list(remaining_nodes))\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair by randomizing the order of remaining nodes\n        remaining = list(set(range(n)) - set(new_solution[:n]))\n        random.shuffle(remaining)\n        new_solution[n-len(remaining):] = remaining\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Travelling Salesman Problem (bi-TSP), where each node has two different 2D coordinates: (x1, y1) and (x2, y2), representing its position in two objective spaces. The goal is to find a tour visiting each node exactly once and returning to the starting node, while minimizing two objectives simultaneously: the total tour length in each coordinate space. Given an archive of solutions, where each solution is a numpy array representing a TSP tour, and its corresponding objective is a tuple of two values (cost in each space), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it.  Must always ensure that the generated neighbor solution remains feasible, i.e., the solution must represent a valid TSP tour: it visits each node exactly once, ensuring no node is skipped or revisited.Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm will employ a hybrid multi-objective selection and geometric transformation strategy that first identifies the most balanced solutions in the archive by combining Pareto dominance with a novel geometric diversity metric, then probabilistically selects a base solution based on both objective performance and geometric alignment of its path segments. It will generate a neighbor solution by applying a geometric transformation operator that warps the tour in one objective space while preserving geometric consistency in the other space, using a novel \"objective-space morphing\" technique that dynamically adjusts the transformation based on the relative improvement potential in each space, with a feasibility-preserving mechanism that ensures the transformed path remains a valid tour through a combination of geometric validation and path reconstruction.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Multi-objective balanced selection\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-8)\n\n    # Calculate geometric diversity\n    geometric_diversity = []\n    for sol, _ in archive:\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        centroid1 = np.mean(path1, axis=0)\n        centroid2 = np.mean(path2, axis=0)\n        diversity = np.linalg.norm(centroid1 - centroid2)\n        geometric_diversity.append(diversity)\n\n    # Combine metrics for selection\n    combined_metric = normalized_obj[:, 0] + normalized_obj[:, 1] + np.array(geometric_diversity)\n    selected_idx = np.argmax(combined_metric)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space morphing transformation\n    if n > 3:\n        # Select a segment to transform\n        segment_length = max(3, n // 4)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate transformation parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n        scale_factor = 0.8 + 0.4 * random.random()\n\n        # Apply geometric transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Transform in first objective space\n            vec1 = instance[node, :2] - centroid1\n            instance[node, :2] = centroid1 + scale_factor * vec1\n            # Transform in second objective space\n            vec2 = instance[node, 2:] - centroid2\n            instance[node, 2:] = centroid2 + (1.0 / scale_factor) * vec2\n\n        # Rebuild solution to maintain feasibility\n        temp_solution = []\n        remaining_nodes = set(range(n)) - set(segment)\n        temp_solution.extend(segment)\n        temp_solution.extend(list(remaining_nodes))\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair by randomizing the order of remaining nodes\n        remaining = list(set(range(n)) - set(new_solution[:n]))\n        random.shuffle(remaining)\n        new_solution[n-len(remaining):] = remaining\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The new algorithm will employ a novel adaptive multi-objective selection strategy that first identifies solutions with high potential for improvement by combining Pareto efficiency with a dynamic objective-space correlation analysis, then probabilistically selects a base solution based on both objective performance and the geometric alignment of its nodes across the two spaces. It generates a neighbor solution by applying a hybrid local search operator that combines a geometric transformation approach with a dynamic objective-space partitioning strategy, where nodes are grouped based on their relative positions in one space while maintaining the relative ordering of their distances in the other space. The algorithm uses an adaptive feasibility-preserving mechanism that ensures the solution remains a valid tour through a combination of spatial validation, path reconstruction, and a novel \"objective-space correlation warping\" technique that dynamically adjusts the clustering based on the relative improvement potential in each space, with an adaptive probabilistic repair mechanism that handles infeasible solutions by intelligently reordering nodes while preserving geometric properties.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive multi-objective selection with correlation analysis\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i, obj in enumerate(objectives):\n        dominated = False\n        for j, other_obj in enumerate(objectives):\n            if i != j and (other_obj[0] <= obj[0] and other_obj[1] <= obj[1]) and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    # Calculate correlation metrics\n    correlation_metrics = []\n    for idx in pareto_front:\n        sol = archive[idx][0]\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        correlation = np.corrcoef(path1.flatten(), path2.flatten())[0, 1]\n        correlation_metrics.append(abs(correlation))\n\n    selected_idx = pareto_front[np.argmax(correlation_metrics)]\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Adaptive geometric transformation with objective-space partitioning\n    if n > 3:\n        # Select a segment to transform\n        segment_length = max(3, n // 6)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate transformation parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n\n        # Apply adaptive transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Transform in first objective space\n            vec1 = instance[node, :2] - centroid1\n            angle = np.arctan2(vec1[1], vec1[0])\n            radius = np.linalg.norm(vec1)\n            new_angle = angle + 0.3 * np.pi * np.random.randn()\n            new_radius = radius * (1 + 0.2 * np.random.randn())\n            instance[node, :2] = centroid1 + np.array([new_radius * np.cos(new_angle), new_radius * np.sin(new_angle)])\n\n            # Transform in second space with correlation preservation\n            vec2 = instance[node, 2:] - centroid2\n            instance[node, 2:] = centroid2 + 0.9 * vec2\n\n        # Rebuild solution with adaptive ordering\n        temp_solution = []\n        remaining_nodes = list(set(range(n)) - set(segment))\n        temp_solution.extend(segment)\n\n        # Adaptive insertion of remaining nodes\n        for node in remaining_nodes:\n            if len(temp_solution) < 2:\n                temp_solution.append(node)\n            else:\n                # Find best insertion point based on both distance matrices\n                min_cost = float('inf')\n                best_pos = 1\n                for pos in range(1, len(temp_solution)):\n                    cost1 = distance_matrix_1[temp_solution[pos-1], node] + distance_matrix_1[node, temp_solution[pos]]\n                    cost2 = distance_matrix_2[temp_solution[pos-1], node] + distance_matrix_2[node, temp_solution[pos]]\n                    total_cost = 0.7 * cost1 + 0.3 * cost2\n                    if total_cost < min_cost:\n                        min_cost = total_cost\n                        best_pos = pos\n                temp_solution.insert(best_pos, node)\n\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Adaptive repair mechanism\n        current_nodes = set(new_solution[:n])\n        missing_nodes = list(set(range(n)) - current_nodes)\n        random.shuffle(missing_nodes)\n\n        # Insert missing nodes at positions that minimize both objectives\n        for node in missing_nodes:\n            min_cost = float('inf')\n            best_pos = 1\n            for pos in range(1, len(new_solution)):\n                cost1 = distance_matrix_1[new_solution[pos-1], node] + distance_matrix_1[node, new_solution[pos]]\n                cost2 = distance_matrix_2[new_solution[pos-1], node] + distance_matrix_2[node, new_solution[pos]]\n                total_cost = 0.6 * cost1 + 0.4 * cost2\n                if total_cost < min_cost:\n                    min_cost = total_cost\n                    best_pos = pos\n            new_solution = np.insert(new_solution, best_pos, node)\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 303,
          "algorithm": "{The new algorithm will employ a novel adaptive multi-objective selection strategy that first identifies solutions with high potential for improvement by combining Pareto efficiency with a dynamic objective-space correlation analysis, then probabilistically selects a base solution based on both objective performance and the geometric alignment of its nodes across the two spaces. It generates a neighbor solution by applying a hybrid local search operator that combines a geometric transformation approach with a dynamic objective-space partitioning strategy, where nodes are grouped based on their relative positions in one space while maintaining the relative ordering of their distances in the other space. The algorithm uses an adaptive feasibility-preserving mechanism that ensures the solution remains a valid tour through a combination of spatial validation, path reconstruction, and a novel \"objective-space correlation warping\" technique that dynamically adjusts the clustering based on the relative improvement potential in each space, with an adaptive probabilistic repair mechanism that handles infeasible solutions by intelligently reordering nodes while preserving geometric properties.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive multi-objective selection with correlation analysis\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i, obj in enumerate(objectives):\n        dominated = False\n        for j, other_obj in enumerate(objectives):\n            if i != j and (other_obj[0] <= obj[0] and other_obj[1] <= obj[1]) and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    # Calculate correlation metrics\n    correlation_metrics = []\n    for idx in pareto_front:\n        sol = archive[idx][0]\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        correlation = np.corrcoef(path1.flatten(), path2.flatten())[0, 1]\n        correlation_metrics.append(abs(correlation))\n\n    selected_idx = pareto_front[np.argmax(correlation_metrics)]\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Adaptive geometric transformation with objective-space partitioning\n    if n > 3:\n        # Select a segment to transform\n        segment_length = max(3, n // 6)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate transformation parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n\n        # Apply adaptive transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Transform in first objective space\n            vec1 = instance[node, :2] - centroid1\n            angle = np.arctan2(vec1[1], vec1[0])\n            radius = np.linalg.norm(vec1)\n            new_angle = angle + 0.3 * np.pi * np.random.randn()\n            new_radius = radius * (1 + 0.2 * np.random.randn())\n            instance[node, :2] = centroid1 + np.array([new_radius * np.cos(new_angle), new_radius * np.sin(new_angle)])\n\n            # Transform in second space with correlation preservation\n            vec2 = instance[node, 2:] - centroid2\n            instance[node, 2:] = centroid2 + 0.9 * vec2\n\n        # Rebuild solution with adaptive ordering\n        temp_solution = []\n        remaining_nodes = list(set(range(n)) - set(segment))\n        temp_solution.extend(segment)\n\n        # Adaptive insertion of remaining nodes\n        for node in remaining_nodes:\n            if len(temp_solution) < 2:\n                temp_solution.append(node)\n            else:\n                # Find best insertion point based on both distance matrices\n                min_cost = float('inf')\n                best_pos = 1\n                for pos in range(1, len(temp_solution)):\n                    cost1 = distance_matrix_1[temp_solution[pos-1], node] + distance_matrix_1[node, temp_solution[pos]]\n                    cost2 = distance_matrix_2[temp_solution[pos-1], node] + distance_matrix_2[node, temp_solution[pos]]\n                    total_cost = 0.7 * cost1 + 0.3 * cost2\n                    if total_cost < min_cost:\n                        min_cost = total_cost\n                        best_pos = pos\n                temp_solution.insert(best_pos, node)\n\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Adaptive repair mechanism\n        current_nodes = set(new_solution[:n])\n        missing_nodes = list(set(range(n)) - current_nodes)\n        random.shuffle(missing_nodes)\n\n        # Insert missing nodes at positions that minimize both objectives\n        for node in missing_nodes:\n            min_cost = float('inf')\n            best_pos = 1\n            for pos in range(1, len(new_solution)):\n                cost1 = distance_matrix_1[new_solution[pos-1], node] + distance_matrix_1[node, new_solution[pos]]\n                cost2 = distance_matrix_2[new_solution[pos-1], node] + distance_matrix_2[node, new_solution[pos]]\n                total_cost = 0.6 * cost1 + 0.4 * cost2\n                if total_cost < min_cost:\n                    min_cost = total_cost\n                    best_pos = pos\n            new_solution = np.insert(new_solution, best_pos, node)\n\n    return new_solution\n\n",
          "score": [
               -1.2015464455337996,
               4.40812873840332
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive multi-objective selection with correlation analysis\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i, obj in enumerate(objectives):\n        dominated = False\n        for j, other_obj in enumerate(objectives):\n            if i != j and (other_obj[0] <= obj[0] and other_obj[1] <= obj[1]) and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    # Calculate correlation metrics\n    correlation_metrics = []\n    for idx in pareto_front:\n        sol = archive[idx][0]\n        path1 = instance[sol, :2]\n        path2 = instance[sol, 2:]\n        correlation = np.corrcoef(path1.flatten(), path2.flatten())[0, 1]\n        correlation_metrics.append(abs(correlation))\n\n    selected_idx = pareto_front[np.argmax(correlation_metrics)]\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Adaptive geometric transformation with objective-space partitioning\n    if n > 3:\n        # Select a segment to transform\n        segment_length = max(3, n // 6)\n        start = random.randint(0, n - segment_length)\n        segment = new_solution[start:start+segment_length]\n\n        # Calculate transformation parameters\n        centroid1 = np.mean(instance[segment, :2], axis=0)\n        centroid2 = np.mean(instance[segment, 2:], axis=0)\n\n        # Apply adaptive transformation\n        for i in range(start, start+segment_length):\n            node = new_solution[i]\n            # Transform in first objective space\n            vec1 = instance[node, :2] - centroid1\n            angle = np.arctan2(vec1[1], vec1[0])\n            radius = np.linalg.norm(vec1)\n            new_angle = angle + 0.3 * np.pi * np.random.randn()\n            new_radius = radius * (1 + 0.2 * np.random.randn())\n            instance[node, :2] = centroid1 + np.array([new_radius * np.cos(new_angle), new_radius * np.sin(new_angle)])\n\n            # Transform in second space with correlation preservation\n            vec2 = instance[node, 2:] - centroid2\n            instance[node, 2:] = centroid2 + 0.9 * vec2\n\n        # Rebuild solution with adaptive ordering\n        temp_solution = []\n        remaining_nodes = list(set(range(n)) - set(segment))\n        temp_solution.extend(segment)\n\n        # Adaptive insertion of remaining nodes\n        for node in remaining_nodes:\n            if len(temp_solution) < 2:\n                temp_solution.append(node)\n            else:\n                # Find best insertion point based on both distance matrices\n                min_cost = float('inf')\n                best_pos = 1\n                for pos in range(1, len(temp_solution)):\n                    cost1 = distance_matrix_1[temp_solution[pos-1], node] + distance_matrix_1[node, temp_solution[pos]]\n                    cost2 = distance_matrix_2[temp_solution[pos-1], node] + distance_matrix_2[node, temp_solution[pos]]\n                    total_cost = 0.7 * cost1 + 0.3 * cost2\n                    if total_cost < min_cost:\n                        min_cost = total_cost\n                        best_pos = pos\n                temp_solution.insert(best_pos, node)\n\n        new_solution = np.array(temp_solution)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Adaptive repair mechanism\n        current_nodes = set(new_solution[:n])\n        missing_nodes = list(set(range(n)) - current_nodes)\n        random.shuffle(missing_nodes)\n\n        # Insert missing nodes at positions that minimize both objectives\n        for node in missing_nodes:\n            min_cost = float('inf')\n            best_pos = 1\n            for pos in range(1, len(new_solution)):\n                cost1 = distance_matrix_1[new_solution[pos-1], node] + distance_matrix_1[node, new_solution[pos]]\n                cost2 = distance_matrix_2[new_solution[pos-1], node] + distance_matrix_2[node, new_solution[pos]]\n                total_cost = 0.6 * cost1 + 0.4 * cost2\n                if total_cost < min_cost:\n                    min_cost = total_cost\n                    best_pos = pos\n            new_solution = np.insert(new_solution, best_pos, node)\n\n    return new_solution\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Travelling Salesman Problem (bi-TSP), where each node has two different 2D coordinates: (x1, y1) and (x2, y2), representing its position in two objective spaces. The goal is to find a tour visiting each node exactly once and returning to the starting node, while minimizing two objectives simultaneously: the total tour length in each coordinate space. Given an archive of solutions, where each solution is a numpy array representing a TSP tour, and its corresponding objective is a tuple of two values (cost in each space), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it.  Must always ensure that the generated neighbor solution remains feasible, i.e., the solution must represent a valid TSP tour: it visits each node exactly once, ensuring no node is skipped or revisited.Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm will employ a hierarchical multi-objective path decomposition strategy that first hierarchically partitions the solution tour into geometrically distinct layers based on the relative dominance relationships between nodes in both objective spaces, then probabilistically selects and reorders these layers using a novel geometric entropy metric that balances the spatial distribution and objective contributions of each layer, followed by a constrained layer fusion process that dynamically merges adjacent layers based on their geometric compatibility and potential for Pareto improvement, with feasibility maintained through a hierarchical boundary validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a multi-objective centroid-based ordering approach if invalidity is detected.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hierarchical layer partitioning\n    def hierarchical_partition(solution, depth=3):\n        if depth == 0 or len(solution) <= 3:\n            return [solution]\n\n        # Calculate geometric properties\n        coords1 = instance[solution, :2]\n        coords2 = instance[solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate geometric entropy\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        entropy = np.sum(dist1 * dist2)\n\n        # Split based on entropy\n        sorted_indices = np.argsort(entropy)\n        split_point = len(solution) // 2\n        left = solution[sorted_indices[:split_point]]\n        right = solution[sorted_indices[split_point:]]\n\n        return hierarchical_partition(left, depth-1) + hierarchical_partition(right, depth-1)\n\n    # Step 2: Select base solution\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    layers = hierarchical_partition(base_solution)\n\n    # Step 3: Layer reordering\n    if len(layers) > 1:\n        # Calculate layer properties\n        layer_properties = []\n        for layer in layers:\n            layer_coords1 = instance[layer, :2]\n            layer_coords2 = instance[layer, 2:]\n            layer_centroid1 = np.mean(layer_coords1, axis=0)\n            layer_centroid2 = np.mean(layer_coords2, axis=0)\n            layer_properties.append({\n                'centroid1': layer_centroid1,\n                'centroid2': layer_centroid2,\n                'size': len(layer),\n                'coords1': layer_coords1,\n                'coords2': layer_coords2\n            })\n\n        # Calculate layer compatibility\n        num_layers = len(layers)\n        compatibility_matrix = np.zeros((num_layers, num_layers))\n        for i in range(num_layers):\n            for j in range(num_layers):\n                if i != j:\n                    dist1 = np.linalg.norm(layer_properties[i]['centroid1'] - layer_properties[j]['centroid1'])\n                    dist2 = np.linalg.norm(layer_properties[i]['centroid2'] - layer_properties[j]['centroid2'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist1 + dist2)\n\n        # Reorder layers based on compatibility\n        current_order = list(range(num_layers))\n        for _ in range(2):\n            i = random.randint(0, num_layers-1)\n            j = np.argmax(compatibility_matrix[i])\n            if i != j:\n                current_order[i], current_order[j] = current_order[j], current_order[i]\n\n        new_solution = np.concatenate([layers[i] for i in current_order])\n    else:\n        new_solution = base_solution.copy()\n\n    # Step 4: Layer fusion\n    if len(layers) > 1 and random.random() < 0.4:\n        # Find most compatible adjacent layers\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n\n        for i in range(len(layers)-1):\n            dist1 = np.linalg.norm(layer_properties[i]['centroid1'] - layer_properties[i+1]['centroid1'])\n            dist2 = np.linalg.norm(layer_properties[i]['centroid2'] - layer_properties[i+1]['centroid2'])\n            compatibility = 1.0 / (1.0 + dist1 + dist2)\n            if compatibility > best_compatibility:\n                best_compatibility = compatibility\n                best_i, best_j = i, i+1\n\n        if best_i != -1 and best_j != -1:\n            # Merge compatible layers\n            merged_layer = np.concatenate([layers[best_i], layers[best_j]])\n            new_layers = layers[:best_i] + [merged_layer] + layers[best_j+1:]\n            new_solution = np.concatenate(new_layers)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != len(base_solution):\n        # Multi-objective centroid-based repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate combined geometric properties\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        combined_metric = dist1 * dist2\n\n        # Sort nodes by combined metric\n        sorted_indices = np.argsort(combined_metric)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\n{The new algorithm will employ a hierarchical multi-objective path decomposition strategy that first hierarchically partitions the solution tour into geometrically distinct layers based on the relative dominance relationships between nodes in both objective spaces, then probabilistically selects and reorders these layers using a novel geometric entropy metric that balances the spatial distribution and objective contributions of each layer, followed by a constrained layer fusion process that dynamically merges adjacent layers based on their geometric compatibility and potential for Pareto improvement, with feasibility maintained through a hierarchical boundary validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a multi-objective centroid-based ordering approach if invalidity is detected.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hierarchical layer partitioning\n    def hierarchical_partition(solution, depth=3):\n        if depth == 0 or len(solution) <= 3:\n            return [solution]\n\n        # Calculate geometric properties\n        coords1 = instance[solution, :2]\n        coords2 = instance[solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate geometric entropy\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        entropy = np.sum(dist1 * dist2)\n\n        # Split based on entropy\n        sorted_indices = np.argsort(entropy)\n        split_point = len(solution) // 2\n        left = solution[sorted_indices[:split_point]]\n        right = solution[sorted_indices[split_point:]]\n\n        return hierarchical_partition(left, depth-1) + hierarchical_partition(right, depth-1)\n\n    # Step 2: Select base solution\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    layers = hierarchical_partition(base_solution)\n\n    # Step 3: Layer reordering\n    if len(layers) > 1:\n        # Calculate layer properties\n        layer_properties = []\n        for layer in layers:\n            layer_coords1 = instance[layer, :2]\n            layer_coords2 = instance[layer, 2:]\n            layer_centroid1 = np.mean(layer_coords1, axis=0)\n            layer_centroid2 = np.mean(layer_coords2, axis=0)\n            layer_properties.append({\n                'centroid1': layer_centroid1,\n                'centroid2': layer_centroid2,\n                'size': len(layer),\n                'coords1': layer_coords1,\n                'coords2': layer_coords2\n            })\n\n        # Calculate layer compatibility\n        num_layers = len(layers)\n        compatibility_matrix = np.zeros((num_layers, num_layers))\n        for i in range(num_layers):\n            for j in range(num_layers):\n                if i != j:\n                    dist1 = np.linalg.norm(layer_properties[i]['centroid1'] - layer_properties[j]['centroid1'])\n                    dist2 = np.linalg.norm(layer_properties[i]['centroid2'] - layer_properties[j]['centroid2'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist1 + dist2)\n\n        # Reorder layers based on compatibility\n        current_order = list(range(num_layers))\n        for _ in range(2):\n            i = random.randint(0, num_layers-1)\n            j = np.argmax(compatibility_matrix[i])\n            if i != j:\n                current_order[i], current_order[j] = current_order[j], current_order[i]\n\n        new_solution = np.concatenate([layers[i] for i in current_order])\n    else:\n        new_solution = base_solution.copy()\n\n    # Step 4: Layer fusion\n    if len(layers) > 1 and random.random() < 0.4:\n        # Find most compatible adjacent layers\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n\n        for i in range(len(layers)-1):\n            dist1 = np.linalg.norm(layer_properties[i]['centroid1'] - layer_properties[i+1]['centroid1'])\n            dist2 = np.linalg.norm(layer_properties[i]['centroid2'] - layer_properties[i+1]['centroid2'])\n            compatibility = 1.0 / (1.0 + dist1 + dist2)\n            if compatibility > best_compatibility:\n                best_compatibility = compatibility\n                best_i, best_j = i, i+1\n\n        if best_i != -1 and best_j != -1:\n            # Merge compatible layers\n            merged_layer = np.concatenate([layers[best_i], layers[best_j]])\n            new_layers = layers[:best_i] + [merged_layer] + layers[best_j+1:]\n            new_solution = np.concatenate(new_layers)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != len(base_solution):\n        # Multi-objective centroid-based repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate combined geometric properties\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        combined_metric = dist1 * dist2\n\n        # Sort nodes by combined metric\n        sorted_indices = np.argsort(combined_metric)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n\nPlease help me create a new algorithm that has a totally different form from the given ones but can be motivated from them.\n1. Firstly, identify the common backbone idea in the provided algorithms. \n2. Secondly, based on the backbone idea describe your new algorithm in one long, detail sentence. The description must be inside within boxed {}.\n3. Thirdly, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 304,
          "algorithm": "{The new algorithm will employ a multi-objective geometric clustering and adaptive path inversion strategy that first identifies geometrically distinct clusters of nodes in both objective spaces using a novel multi-dimensional k-means++ initialization approach, then adaptively inverts the order of nodes within each cluster based on their relative dominance relationships in the Pareto frontier, followed by a constrained cluster reallocation process that dynamically repositions clusters based on their geometric compatibility and potential for Pareto improvement, with feasibility maintained through a geometric boundary validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a multi-objective geometric median-based ordering approach if invalidity is detected.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Multi-objective geometric clustering\n    def geometric_clustering(solution, k=3):\n        coords = instance[solution]\n        centroids = np.zeros((k, 4))\n\n        # Initialize centroids using multi-dimensional k-means++\n        remaining = list(range(len(solution)))\n        centroids[0] = coords[random.choice(remaining)]\n        remaining.remove(np.argmin(np.linalg.norm(coords - centroids[0], axis=1)))\n\n        for i in range(1, k):\n            distances = np.min([np.linalg.norm(coords - centroids[j], axis=1) for j in range(i)], axis=0)\n            probabilities = distances ** 2 / np.sum(distances ** 2)\n            next_idx = np.random.choice(remaining, p=probabilities[remaining])\n            centroids[i] = coords[next_idx]\n            remaining.remove(next_idx)\n\n        # Assign nodes to clusters\n        clusters = [[] for _ in range(k)]\n        for idx in range(len(solution)):\n            dists = np.linalg.norm(coords[idx] - centroids, axis=1)\n            cluster = np.argmin(dists)\n            clusters[cluster].append(solution[idx])\n\n        return clusters\n\n    # Step 2: Select base solution\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    clusters = geometric_clustering(base_solution)\n\n    # Step 3: Adaptive path inversion\n    new_solution = []\n    for cluster in clusters:\n        if len(cluster) > 1:\n            # Calculate Pareto dominance relationships\n            cluster_coords = instance[cluster]\n            dominance = np.zeros(len(cluster))\n            for i in range(len(cluster)):\n                for j in range(len(cluster)):\n                    if i != j:\n                        if (cluster_coords[i, 0] <= cluster_coords[j, 0] and cluster_coords[i, 1] <= cluster_coords[j, 1] and\n                            (cluster_coords[i, 0] < cluster_coords[j, 0] or cluster_coords[i, 1] < cluster_coords[j, 1])):\n                            dominance[i] += 1\n\n            # Invert cluster based on dominance\n            if np.max(dominance) > 0:\n                sorted_indices = np.argsort(dominance)\n                if random.random() < 0.5:\n                    cluster = [cluster[i] for i in sorted_indices[::-1]]\n        new_solution.extend(cluster)\n\n    # Step 4: Cluster reallocation\n    if len(clusters) > 1 and random.random() < 0.5:\n        # Calculate cluster compatibility\n        cluster_properties = []\n        for cluster in clusters:\n            cluster_coords = instance[cluster]\n            centroid = np.mean(cluster_coords, axis=0)\n            cluster_properties.append({\n                'centroid': centroid,\n                'size': len(cluster),\n                'coords': cluster_coords\n            })\n\n        compatibility_matrix = np.zeros((len(clusters), len(clusters)))\n        for i in range(len(clusters)):\n            for j in range(len(clusters)):\n                if i != j:\n                    dist = np.linalg.norm(cluster_properties[i]['centroid'] - cluster_properties[j]['centroid'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist)\n\n        # Find most compatible clusters\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n        for i in range(len(clusters)):\n            for j in range(len(clusters)):\n                if i != j and compatibility_matrix[i,j] > best_compatibility:\n                    best_compatibility = compatibility_matrix[i,j]\n                    best_i, best_j = i, j\n\n        if best_i != -1 and best_j != -1:\n            # Reallocate nodes between compatible clusters\n            merged = clusters[best_i] + clusters[best_j]\n            new_clusters = clusters[:best_i] + [merged] + clusters[best_i+1:best_j] + clusters[best_j+1:]\n            new_solution = []\n            for cluster in new_clusters:\n                new_solution.extend(cluster)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != len(base_solution):\n        # Geometric median-based repair\n        coords = instance[new_solution]\n        geometric_median = np.median(coords, axis=0)\n        distances = np.linalg.norm(coords - geometric_median, axis=1)\n        sorted_indices = np.argsort(distances)\n        new_solution = [new_solution[i] for i in sorted_indices]\n\n    return np.array(new_solution)\n\n",
          "score": [
               -0.9689848612249506,
               1.6827639937400818
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Multi-objective geometric clustering\n    def geometric_clustering(solution, k=3):\n        coords = instance[solution]\n        centroids = np.zeros((k, 4))\n\n        # Initialize centroids using multi-dimensional k-means++\n        remaining = list(range(len(solution)))\n        centroids[0] = coords[random.choice(remaining)]\n        remaining.remove(np.argmin(np.linalg.norm(coords - centroids[0], axis=1)))\n\n        for i in range(1, k):\n            distances = np.min([np.linalg.norm(coords - centroids[j], axis=1) for j in range(i)], axis=0)\n            probabilities = distances ** 2 / np.sum(distances ** 2)\n            next_idx = np.random.choice(remaining, p=probabilities[remaining])\n            centroids[i] = coords[next_idx]\n            remaining.remove(next_idx)\n\n        # Assign nodes to clusters\n        clusters = [[] for _ in range(k)]\n        for idx in range(len(solution)):\n            dists = np.linalg.norm(coords[idx] - centroids, axis=1)\n            cluster = np.argmin(dists)\n            clusters[cluster].append(solution[idx])\n\n        return clusters\n\n    # Step 2: Select base solution\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    clusters = geometric_clustering(base_solution)\n\n    # Step 3: Adaptive path inversion\n    new_solution = []\n    for cluster in clusters:\n        if len(cluster) > 1:\n            # Calculate Pareto dominance relationships\n            cluster_coords = instance[cluster]\n            dominance = np.zeros(len(cluster))\n            for i in range(len(cluster)):\n                for j in range(len(cluster)):\n                    if i != j:\n                        if (cluster_coords[i, 0] <= cluster_coords[j, 0] and cluster_coords[i, 1] <= cluster_coords[j, 1] and\n                            (cluster_coords[i, 0] < cluster_coords[j, 0] or cluster_coords[i, 1] < cluster_coords[j, 1])):\n                            dominance[i] += 1\n\n            # Invert cluster based on dominance\n            if np.max(dominance) > 0:\n                sorted_indices = np.argsort(dominance)\n                if random.random() < 0.5:\n                    cluster = [cluster[i] for i in sorted_indices[::-1]]\n        new_solution.extend(cluster)\n\n    # Step 4: Cluster reallocation\n    if len(clusters) > 1 and random.random() < 0.5:\n        # Calculate cluster compatibility\n        cluster_properties = []\n        for cluster in clusters:\n            cluster_coords = instance[cluster]\n            centroid = np.mean(cluster_coords, axis=0)\n            cluster_properties.append({\n                'centroid': centroid,\n                'size': len(cluster),\n                'coords': cluster_coords\n            })\n\n        compatibility_matrix = np.zeros((len(clusters), len(clusters)))\n        for i in range(len(clusters)):\n            for j in range(len(clusters)):\n                if i != j:\n                    dist = np.linalg.norm(cluster_properties[i]['centroid'] - cluster_properties[j]['centroid'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist)\n\n        # Find most compatible clusters\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n        for i in range(len(clusters)):\n            for j in range(len(clusters)):\n                if i != j and compatibility_matrix[i,j] > best_compatibility:\n                    best_compatibility = compatibility_matrix[i,j]\n                    best_i, best_j = i, j\n\n        if best_i != -1 and best_j != -1:\n            # Reallocate nodes between compatible clusters\n            merged = clusters[best_i] + clusters[best_j]\n            new_clusters = clusters[:best_i] + [merged] + clusters[best_i+1:best_j] + clusters[best_j+1:]\n            new_solution = []\n            for cluster in new_clusters:\n                new_solution.extend(cluster)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != len(base_solution):\n        # Geometric median-based repair\n        coords = instance[new_solution]\n        geometric_median = np.median(coords, axis=0)\n        distances = np.linalg.norm(coords - geometric_median, axis=1)\n        sorted_indices = np.argsort(distances)\n        new_solution = [new_solution[i] for i in sorted_indices]\n\n    return np.array(new_solution)\n\n"
     },
     {
          "prompt": "You are solving a Bi-objective Travelling Salesman Problem (bi-TSP), where each node has two different 2D coordinates: (x1, y1) and (x2, y2), representing its position in two objective spaces. The goal is to find a tour visiting each node exactly once and returning to the starting node, while minimizing two objectives simultaneously: the total tour length in each coordinate space. Given an archive of solutions, where each solution is a numpy array representing a TSP tour, and its corresponding objective is a tuple of two values (cost in each space), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it.  Must always ensure that the generated neighbor solution remains feasible, i.e., the solution must represent a valid TSP tour: it visits each node exactly once, ensuring no node is skipped or revisited.Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 2 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\n{The new algorithm will employ a multi-phase geometric fragmentation and reassembly strategy that first decomposes the solution tour into geometrically distinct segments based on the relative positions of nodes in both objective spaces, then probabilistically selects and reorders these segments using a novel geometric harmony metric that balances the spatial distribution and objective contributions of each segment, followed by a constrained segment fusion process that dynamically merges adjacent segments based on their geometric compatibility and potential for Pareto improvement, with feasibility maintained through a segment boundary validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a geometric centroid-based ordering approach if invalidity is detected.}\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution based on combined objective quality\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Geometric fragmentation\n    if n > 4:\n        # Calculate geometric properties for each node in both spaces\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n\n        # Calculate centroids and angles\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate angles relative to centroids\n        angles1 = np.arctan2(coords1[:,1] - centroid1[1], coords1[:,0] - centroid1[0])\n        angles2 = np.arctan2(coords2[:,1] - centroid2[1], coords2[:,0] - centroid2[0])\n\n        # Calculate geometric harmony metric\n        angle_diff = np.abs(angles1 - angles2)\n        harmony = 1.0 / (1.0 + angle_diff)\n\n        # Sort nodes by harmony\n        sorted_indices = np.argsort(harmony)[::-1]\n\n        # Split into segments based on harmony\n        num_segments = max(2, min(4, n // 3))\n        segment_size = n // num_segments\n        segments = [new_solution[sorted_indices[i*segment_size:(i+1)*segment_size]] for i in range(num_segments)]\n\n        # Step 3: Segment reordering\n        # Calculate segment properties\n        segment_properties = []\n        for seg in segments:\n            seg_coords1 = instance[seg, :2]\n            seg_coords2 = instance[seg, 2:]\n            seg_centroid1 = np.mean(seg_coords1, axis=0)\n            seg_centroid2 = np.mean(seg_coords2, axis=0)\n            seg_properties = {\n                'centroid1': seg_centroid1,\n                'centroid2': seg_centroid2,\n                'size': len(seg),\n                'coords1': seg_coords1,\n                'coords2': seg_coords2\n            }\n            segment_properties.append(seg_properties)\n\n        # Calculate segment compatibility\n        compatibility_matrix = np.zeros((num_segments, num_segments))\n        for i in range(num_segments):\n            for j in range(num_segments):\n                if i != j:\n                    dist1 = np.linalg.norm(segment_properties[i]['centroid1'] - segment_properties[j]['centroid1'])\n                    dist2 = np.linalg.norm(segment_properties[i]['centroid2'] - segment_properties[j]['centroid2'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist1 + dist2)\n\n        # Reorder segments based on compatibility\n        current_order = list(range(num_segments))\n        for _ in range(2):\n            i = random.randint(0, num_segments-1)\n            j = np.argmax(compatibility_matrix[i])\n            if i != j:\n                current_order[i], current_order[j] = current_order[j], current_order[i]\n\n        # Reconstruct solution from reordered segments\n        new_solution = np.concatenate([segments[i] for i in current_order])\n\n    # Step 4: Segment fusion\n    if n > 5 and random.random() < 0.3:\n        # Find the most compatible adjacent segments\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n\n        for i in range(len(segments)-1):\n            dist1 = np.linalg.norm(segment_properties[i]['centroid1'] - segment_properties[i+1]['centroid1'])\n            dist2 = np.linalg.norm(segment_properties[i]['centroid2'] - segment_properties[i+1]['centroid2'])\n            compatibility = 1.0 / (1.0 + dist1 + dist2)\n            if compatibility > best_compatibility:\n                best_compatibility = compatibility\n                best_i, best_j = i, i+1\n\n        if best_i != -1 and best_j != -1:\n            # Merge the compatible segments\n            merged_segment = np.concatenate([segments[best_i], segments[best_j]])\n            new_segments = segments[:best_i] + [merged_segment] + segments[best_j+1:]\n            new_solution = np.concatenate(new_segments)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != n:\n        # Geometric centroid-based repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate angles relative to centroids\n        angles1 = np.arctan2(coords1[:,1] - centroid1[1], coords1[:,0] - centroid1[0])\n        angles2 = np.arctan2(coords2[:,1] - centroid2[1], coords2[:,0] - centroid2[0])\n\n        # Sort nodes by combined angle\n        combined_angles = angles1 + angles2\n        sorted_indices = np.argsort(combined_angles)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Evaluate solutions and select base solution\n    non_dominated = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] <= obj[0] and other_obj[1] <= obj[1] and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            non_dominated.append(i)\n\n    if not non_dominated:\n        combined_costs = [obj[0] + obj[1] for _, obj in archive]\n        selected_idx = np.argmin(combined_costs)\n    else:\n        qualities = [1.0 / (1.0 + archive[i][1][0] + archive[i][1][1]) for i in non_dominated]\n        selected_idx = random.choices(non_dominated, weights=qualities, k=1)[0]\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Hybrid local search with dynamic node swapping\n    n = len(base_solution)\n    if n < 3:\n        return new_solution\n\n    def calculate_total_cost(sol):\n        cost1 = 0.0\n        cost2 = 0.0\n        for i in range(n):\n            cost1 += distance_matrix_1[sol[i], sol[(i+1)%n]]\n            cost2 += distance_matrix_2[sol[i], sol[(i+1)%n]]\n        return cost1, cost2\n\n    original_cost1, original_cost2 = calculate_total_cost(new_solution)\n\n    # Select two nodes to swap based on their relative positions in both spaces\n    i, j = random.sample(range(n), 2)\n    node_i = new_solution[i]\n    node_j = new_solution[j]\n\n    # Calculate the contribution of each node to both objectives\n    def calculate_node_contribution(sol, idx):\n        node = sol[idx]\n        prev_node = sol[idx-1]\n        next_node = sol[(idx+1)%n]\n        cost1 = distance_matrix_1[prev_node, node] + distance_matrix_1[node, next_node]\n        cost2 = distance_matrix_2[prev_node, node] + distance_matrix_2[node, next_node]\n        return cost1, cost2\n\n    contrib_i1, contrib_i2 = calculate_node_contribution(new_solution, i)\n    contrib_j1, contrib_j2 = calculate_node_contribution(new_solution, j)\n\n    # Perform the swap\n    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Calculate new costs\n    new_cost1, new_cost2 = calculate_total_cost(new_solution)\n\n    # Acceptance criterion\n    if not (new_cost1 <= original_cost1 and new_cost2 <= original_cost2) and random.random() >= 0.2:\n        # If not accepted, try another swap\n        for _ in range(2):\n            i_new, j_new = random.sample(range(n), 2)\n            new_solution[i_new], new_solution[j_new] = new_solution[j_new], new_solution[i_new]\n            temp_cost1, temp_cost2 = calculate_total_cost(new_solution)\n            if (temp_cost1 <= original_cost1 and temp_cost2 <= original_cost2) or random.random() < 0.15:\n                break\n            new_solution[i_new], new_solution[j_new] = new_solution[j_new], new_solution[i_new]\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        new_solution = base_solution.copy()\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
          "sample_order": 305,
          "algorithm": "{The new algorithm will employ a dynamic multi-objective path evolution strategy that first identifies critical nodes by analyzing their contribution to both objectives using a novel Pareto-frontier alignment metric, then constructs a probabilistic path transformation network where each node's position is adjusted based on its relative importance in both spaces, followed by a constrained path smoothing process that optimizes the tour's geometric properties in both spaces using a dual-objective gradient descent approach, with feasibility maintained through a topological consistency validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a harmonic path interpolation method if invalidity is detected.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution based on Pareto-frontier alignment\n    def calculate_alignment(obj):\n        return 1.0 / (1.0 + obj[0] * obj[1])\n\n    alignments = [calculate_alignment(obj) for _, obj in archive]\n    selected_idx = np.argmax(alignments)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Identify critical nodes\n    def calculate_node_importance(sol, idx):\n        node = sol[idx]\n        prev_node = sol[idx-1]\n        next_node = sol[(idx+1)%n]\n        cost1 = distance_matrix_1[prev_node, node] + distance_matrix_1[node, next_node]\n        cost2 = distance_matrix_2[prev_node, node] + distance_matrix_2[node, next_node]\n        return cost1 * cost2  # Multiplicative importance for Pareto-alignment\n\n    importance = [calculate_node_importance(new_solution, i) for i in range(n)]\n    critical_nodes = np.argsort(importance)[-max(2, n//5):]  # Top 20% or 2 nodes\n\n    # Step 3: Construct path transformation network\n    coords1 = instance[new_solution, :2]\n    coords2 = instance[new_solution, 2:]\n    centroid1 = np.mean(coords1, axis=0)\n    centroid2 = np.mean(coords2, axis=0)\n\n    # Calculate relative positions and gradients\n    relative_pos1 = coords1 - centroid1\n    relative_pos2 = coords2 - centroid2\n    gradients = np.zeros((n, 2))\n\n    for i in range(n):\n        if i in critical_nodes:\n            gradients[i] = (relative_pos1[i] + relative_pos2[i]) * 0.5\n        else:\n            gradients[i] = (relative_pos1[i] * 0.3 + relative_pos2[i] * 0.7) * 0.8\n\n    # Step 4: Apply dual-objective gradient descent\n    for _ in range(2):  # Limited iterations for efficiency\n        for i in range(n):\n            if i in critical_nodes:\n                continue  # Skip critical nodes to preserve structure\n\n            # Calculate movement direction\n            direction = gradients[i] / (np.linalg.norm(gradients[i]) + 1e-6)\n\n            # Find nearest neighbor in both spaces\n            distances1 = np.linalg.norm(coords1 - (coords1[i] + direction), axis=1)\n            distances2 = np.linalg.norm(coords2 - (coords2[i] + direction), axis=1)\n            combined_distances = distances1 * 0.6 + distances2 * 0.4\n            nearest_idx = np.argmin(combined_distances)\n\n            # Swap positions if beneficial\n            if nearest_idx != i:\n                new_solution[i], new_solution[nearest_idx] = new_solution[nearest_idx], new_solution[i]\n                coords1 = instance[new_solution, :2]\n                coords2 = instance[new_solution, 2:]\n                centroid1 = np.mean(coords1, axis=0)\n                centroid2 = np.mean(coords2, axis=0)\n                relative_pos1 = coords1 - centroid1\n                relative_pos2 = coords2 - centroid2\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != n:\n        # Harmonic path interpolation repair\n        angles1 = np.arctan2(coords1[:,1] - centroid1[1], coords1[:,0] - centroid1[0])\n        angles2 = np.arctan2(coords2[:,1] - centroid2[1], coords2[:,0] - centroid2[0])\n        combined_angles = angles1 * 0.7 + angles2 * 0.3\n        sorted_indices = np.argsort(combined_angles)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n",
          "score": [
               -0.9505383907904648,
               5.466543972492218
          ],
          "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution based on Pareto-frontier alignment\n    def calculate_alignment(obj):\n        return 1.0 / (1.0 + obj[0] * obj[1])\n\n    alignments = [calculate_alignment(obj) for _, obj in archive]\n    selected_idx = np.argmax(alignments)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Identify critical nodes\n    def calculate_node_importance(sol, idx):\n        node = sol[idx]\n        prev_node = sol[idx-1]\n        next_node = sol[(idx+1)%n]\n        cost1 = distance_matrix_1[prev_node, node] + distance_matrix_1[node, next_node]\n        cost2 = distance_matrix_2[prev_node, node] + distance_matrix_2[node, next_node]\n        return cost1 * cost2  # Multiplicative importance for Pareto-alignment\n\n    importance = [calculate_node_importance(new_solution, i) for i in range(n)]\n    critical_nodes = np.argsort(importance)[-max(2, n//5):]  # Top 20% or 2 nodes\n\n    # Step 3: Construct path transformation network\n    coords1 = instance[new_solution, :2]\n    coords2 = instance[new_solution, 2:]\n    centroid1 = np.mean(coords1, axis=0)\n    centroid2 = np.mean(coords2, axis=0)\n\n    # Calculate relative positions and gradients\n    relative_pos1 = coords1 - centroid1\n    relative_pos2 = coords2 - centroid2\n    gradients = np.zeros((n, 2))\n\n    for i in range(n):\n        if i in critical_nodes:\n            gradients[i] = (relative_pos1[i] + relative_pos2[i]) * 0.5\n        else:\n            gradients[i] = (relative_pos1[i] * 0.3 + relative_pos2[i] * 0.7) * 0.8\n\n    # Step 4: Apply dual-objective gradient descent\n    for _ in range(2):  # Limited iterations for efficiency\n        for i in range(n):\n            if i in critical_nodes:\n                continue  # Skip critical nodes to preserve structure\n\n            # Calculate movement direction\n            direction = gradients[i] / (np.linalg.norm(gradients[i]) + 1e-6)\n\n            # Find nearest neighbor in both spaces\n            distances1 = np.linalg.norm(coords1 - (coords1[i] + direction), axis=1)\n            distances2 = np.linalg.norm(coords2 - (coords2[i] + direction), axis=1)\n            combined_distances = distances1 * 0.6 + distances2 * 0.4\n            nearest_idx = np.argmin(combined_distances)\n\n            # Swap positions if beneficial\n            if nearest_idx != i:\n                new_solution[i], new_solution[nearest_idx] = new_solution[nearest_idx], new_solution[i]\n                coords1 = instance[new_solution, :2]\n                coords2 = instance[new_solution, 2:]\n                centroid1 = np.mean(coords1, axis=0)\n                centroid2 = np.mean(coords2, axis=0)\n                relative_pos1 = coords1 - centroid1\n                relative_pos2 = coords2 - centroid2\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != n:\n        # Harmonic path interpolation repair\n        angles1 = np.arctan2(coords1[:,1] - centroid1[1], coords1[:,0] - centroid1[0])\n        angles2 = np.arctan2(coords2[:,1] - centroid2[1], coords2[:,0] - centroid2[0])\n        combined_angles = angles1 * 0.7 + angles2 * 0.3\n        sorted_indices = np.argsort(combined_angles)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n"
     }
]