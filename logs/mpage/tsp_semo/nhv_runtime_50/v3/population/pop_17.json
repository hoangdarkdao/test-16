[
     {
          "algorithm": null,
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Evaluate solutions and select base solution\n    non_dominated = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] <= obj[0] and other_obj[1] <= obj[1] and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            non_dominated.append(i)\n\n    if not non_dominated:\n        combined_costs = [obj[0] + obj[1] for _, obj in archive]\n        selected_idx = np.argmin(combined_costs)\n    else:\n        qualities = [1.0 / (1.0 + archive[i][1][0] + archive[i][1][1]) for i in non_dominated]\n        selected_idx = random.choices(non_dominated, weights=qualities, k=1)[0]\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Hybrid local search with dynamic node swapping\n    n = len(base_solution)\n    if n < 3:\n        return new_solution\n\n    def calculate_total_cost(sol):\n        cost1 = 0.0\n        cost2 = 0.0\n        for i in range(n):\n            cost1 += distance_matrix_1[sol[i], sol[(i+1)%n]]\n            cost2 += distance_matrix_2[sol[i], sol[(i+1)%n]]\n        return cost1, cost2\n\n    original_cost1, original_cost2 = calculate_total_cost(new_solution)\n\n    # Select two nodes to swap based on their relative positions in both spaces\n    i, j = random.sample(range(n), 2)\n    node_i = new_solution[i]\n    node_j = new_solution[j]\n\n    # Calculate the contribution of each node to both objectives\n    def calculate_node_contribution(sol, idx):\n        node = sol[idx]\n        prev_node = sol[idx-1]\n        next_node = sol[(idx+1)%n]\n        cost1 = distance_matrix_1[prev_node, node] + distance_matrix_1[node, next_node]\n        cost2 = distance_matrix_2[prev_node, node] + distance_matrix_2[node, next_node]\n        return cost1, cost2\n\n    contrib_i1, contrib_i2 = calculate_node_contribution(new_solution, i)\n    contrib_j1, contrib_j2 = calculate_node_contribution(new_solution, j)\n\n    # Perform the swap\n    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Calculate new costs\n    new_cost1, new_cost2 = calculate_total_cost(new_solution)\n\n    # Acceptance criterion\n    if not (new_cost1 <= original_cost1 and new_cost2 <= original_cost2) and random.random() >= 0.2:\n        # If not accepted, try another swap\n        for _ in range(2):\n            i_new, j_new = random.sample(range(n), 2)\n            new_solution[i_new], new_solution[j_new] = new_solution[j_new], new_solution[i_new]\n            temp_cost1, temp_cost2 = calculate_total_cost(new_solution)\n            if (temp_cost1 <= original_cost1 and temp_cost2 <= original_cost2) or random.random() < 0.15:\n                break\n            new_solution[i_new], new_solution[j_new] = new_solution[j_new], new_solution[i_new]\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        new_solution = base_solution.copy()\n\n    return new_solution\n\n",
          "score": [
               -0.9729095779833985,
               0.7925425171852112
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid Pareto-geometric selection and path reconstruction strategy that combines a probabilistic dominance-based solution selection with a novel objective-space alignment operator. First, it will use non-dominated sorting to identify promising solutions in the archive, then probabilistically select a base solution based on its Pareto rank. Next, it will analyze the geometric alignment of nodes in both objective spaces using a custom alignment metric, and reconstruct the tour by sequentially connecting nodes that show the highest alignment potential in both spaces, with a dynamic segment reordering mechanism that alternates between objective spaces to balance improvement in both dimensions. The method will ensure feasibility through a permutation validation step and a geometric repair mechanism if needed, while prioritizing edges that show the most promising alignment improvement in both spaces.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Pareto-based probabilistic selection\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and all(objectives[j] <= objectives[i]) and any(objectives[j] < objectives[i]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if pareto_front:\n        selected_idx = np.random.choice(pareto_front)\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Objective-space alignment analysis\n    alignment_scores = []\n    for i in range(n):\n        u = new_solution[i]\n        v = new_solution[(i + 1) % n]\n\n        # Calculate alignment metric: cosine similarity of direction vectors\n        vec1 = instance[v, :2] - instance[u, :2]\n        vec2 = instance[v, 2:] - instance[u, 2:]\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n\n        if norm1 > 0 and norm2 > 0:\n            cos_sim = np.dot(vec1, vec2) / (norm1 * norm2)\n            alignment_scores.append((i, cos_sim))\n        else:\n            alignment_scores.append((i, 0.0))\n\n    # Step 3: Dynamic segment reordering\n    if n > 3:\n        # Sort segments by alignment score (highest first)\n        alignment_scores.sort(key=lambda x: -x[1])\n        selected_segments = [x[0] for x in alignment_scores[:max(2, n//5)]]\n\n        # Reorder segments in alternating objective spaces\n        toggle = True\n        new_order = []\n        used_nodes = set()\n\n        for seg in selected_segments:\n            if toggle:\n                # Connect in first objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([u, v])\n            else:\n                # Connect in second objective space\n                u = new_solution[seg]\n                v = new_solution[(seg + 1) % n]\n                new_order.extend([v, u])\n            toggle = not toggle\n\n        # Add remaining nodes\n        remaining_nodes = [node for node in base_solution if node not in new_order]\n        new_order.extend(remaining_nodes)\n\n        new_solution = np.array(new_order)\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -1.027370962532713,
               2.509422779083252
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hierarchical multi-objective path decomposition strategy that first hierarchically partitions the solution tour into geometrically distinct layers based on the relative dominance relationships between nodes in both objective spaces, then probabilistically selects and reorders these layers using a novel geometric entropy metric that balances the spatial distribution and objective contributions of each layer, followed by a constrained layer fusion process that dynamically merges adjacent layers based on their geometric compatibility and potential for Pareto improvement, with feasibility maintained through a hierarchical boundary validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a multi-objective centroid-based ordering approach if invalidity is detected.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a numpy array of node IDs.\n             Each objective is a tuple of two float values (cost in each space).\n    instance: Numpy array of shape (N, 4). Each row corresponds to a node and contains its coordinates in two 2D spaces: (x1, y1, x2, y2).\n    distance_matrix_1: Distance matrix in the first objective space.\n    distance_matrix_2: Distance matrix in the second objective space.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hierarchical layer partitioning\n    def hierarchical_partition(solution, depth=3):\n        if depth == 0 or len(solution) <= 3:\n            return [solution]\n\n        # Calculate geometric properties\n        coords1 = instance[solution, :2]\n        coords2 = instance[solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate geometric entropy\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        entropy = np.sum(dist1 * dist2)\n\n        # Split based on entropy\n        sorted_indices = np.argsort(entropy)\n        split_point = len(solution) // 2\n        left = solution[sorted_indices[:split_point]]\n        right = solution[sorted_indices[split_point:]]\n\n        return hierarchical_partition(left, depth-1) + hierarchical_partition(right, depth-1)\n\n    # Step 2: Select base solution\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    layers = hierarchical_partition(base_solution)\n\n    # Step 3: Layer reordering\n    if len(layers) > 1:\n        # Calculate layer properties\n        layer_properties = []\n        for layer in layers:\n            layer_coords1 = instance[layer, :2]\n            layer_coords2 = instance[layer, 2:]\n            layer_centroid1 = np.mean(layer_coords1, axis=0)\n            layer_centroid2 = np.mean(layer_coords2, axis=0)\n            layer_properties.append({\n                'centroid1': layer_centroid1,\n                'centroid2': layer_centroid2,\n                'size': len(layer),\n                'coords1': layer_coords1,\n                'coords2': layer_coords2\n            })\n\n        # Calculate layer compatibility\n        num_layers = len(layers)\n        compatibility_matrix = np.zeros((num_layers, num_layers))\n        for i in range(num_layers):\n            for j in range(num_layers):\n                if i != j:\n                    dist1 = np.linalg.norm(layer_properties[i]['centroid1'] - layer_properties[j]['centroid1'])\n                    dist2 = np.linalg.norm(layer_properties[i]['centroid2'] - layer_properties[j]['centroid2'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist1 + dist2)\n\n        # Reorder layers based on compatibility\n        current_order = list(range(num_layers))\n        for _ in range(2):\n            i = random.randint(0, num_layers-1)\n            j = np.argmax(compatibility_matrix[i])\n            if i != j:\n                current_order[i], current_order[j] = current_order[j], current_order[i]\n\n        new_solution = np.concatenate([layers[i] for i in current_order])\n    else:\n        new_solution = base_solution.copy()\n\n    # Step 4: Layer fusion\n    if len(layers) > 1 and random.random() < 0.4:\n        # Find most compatible adjacent layers\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n\n        for i in range(len(layers)-1):\n            dist1 = np.linalg.norm(layer_properties[i]['centroid1'] - layer_properties[i+1]['centroid1'])\n            dist2 = np.linalg.norm(layer_properties[i]['centroid2'] - layer_properties[i+1]['centroid2'])\n            compatibility = 1.0 / (1.0 + dist1 + dist2)\n            if compatibility > best_compatibility:\n                best_compatibility = compatibility\n                best_i, best_j = i, i+1\n\n        if best_i != -1 and best_j != -1:\n            # Merge compatible layers\n            merged_layer = np.concatenate([layers[best_i], layers[best_j]])\n            new_layers = layers[:best_i] + [merged_layer] + layers[best_j+1:]\n            new_solution = np.concatenate(new_layers)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != len(base_solution):\n        # Multi-objective centroid-based repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate combined geometric properties\n        dist1 = np.linalg.norm(coords1 - centroid1, axis=1)\n        dist2 = np.linalg.norm(coords2 - centroid2, axis=1)\n        combined_metric = dist1 * dist2\n\n        # Sort nodes by combined metric\n        sorted_indices = np.argsort(combined_metric)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n",
          "score": [
               -0.9642584663819409,
               0.2063705325126648
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a two-phase neighborhood exploration strategy that combines a probabilistic Pareto-dominance-based selection with a novel geometric path reconstruction operator. First, it will probabilistically select a solution from the archive based on its Pareto dominance rank and non-dominated sorting, then analyze the instance's geometric properties to identify clusters of nodes in each objective space using DBSCAN clustering. The algorithm will then reconstruct the tour by sequentially connecting clusters in an alternating fashion between objective spaces, ensuring feasibility through a cluster-ordering validation step that guarantees no node is skipped or revisited. The geometric path reconstruction operator will dynamically adjust the sequence of cluster connections based on the relative improvement potential in each objective space, with the final solution validated through a permutation check to ensure it remains a valid TSP tour.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Pareto-based selection\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i in range(len(objectives)):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and all(objectives[j] <= objectives[i]) and any(objectives[j] < objectives[i]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if pareto_front:\n        selected_idx = np.random.choice(pareto_front)\n    else:\n        selected_idx = np.random.randint(len(archive))\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Geometric clustering and path reconstruction\n    n = len(base_solution)\n    if n < 3:\n        return base_solution\n\n    # Cluster nodes in both spaces using DBSCAN\n    from sklearn.cluster import DBSCAN\n    coords1 = instance[:, :2]\n    coords2 = instance[:, 2:]\n\n    clusterer1 = DBSCAN(eps=0.3, min_samples=2)\n    labels1 = clusterer1.fit_predict(coords1)\n\n    clusterer2 = DBSCAN(eps=0.3, min_samples=2)\n    labels2 = clusterer2.fit_predict(coords2)\n\n    # Create cluster mappings\n    clusters1 = {}\n    clusters2 = {}\n    for i, (l1, l2) in enumerate(zip(labels1, labels2)):\n        clusters1.setdefault(l1, []).append(i)\n        clusters2.setdefault(l2, []).append(i)\n\n    # Sort clusters by size (largest first)\n    sorted_clusters1 = sorted(clusters1.values(), key=lambda x: -len(x))\n    sorted_clusters2 = sorted(clusters2.values(), key=lambda x: -len(x))\n\n    # Alternate between clusters from both spaces\n    new_solution = []\n    used_nodes = set()\n    i = 0\n    j = 0\n    toggle = True  # Alternate between spaces\n\n    while len(new_solution) < n:\n        if toggle:\n            if i < len(sorted_clusters1):\n                cluster = sorted_clusters1[i]\n                i += 1\n            else:\n                cluster = sorted_clusters2[j]\n                j += 1\n        else:\n            if j < len(sorted_clusters2):\n                cluster = sorted_clusters2[j]\n                j += 1\n            else:\n                cluster = sorted_clusters1[i]\n                i += 1\n\n        toggle = not toggle\n\n        for node in cluster:\n            if node not in used_nodes:\n                new_solution.append(node)\n                used_nodes.add(node)\n\n    # Ensure all nodes are included\n    remaining_nodes = [node for node in base_solution if node not in new_solution]\n    new_solution.extend(remaining_nodes)\n\n    # Convert to numpy array and validate\n    new_solution = np.array(new_solution)\n    if len(np.unique(new_solution)) != n or len(new_solution) != n:\n        # Fallback to simple swap if invalid\n        new_solution = base_solution.copy()\n        new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n",
          "score": [
               -1.0065441045515167,
               4.810353696346283
          ]
     },
     {
          "algorithm": "{The proposed heuristic function 'select_neighbor' employs a multi-objective adaptive local search strategy that intelligently selects a promising solution from the archive based on a combined objective score (normalized sum of both objectives), then applies a novel hybrid local search operator that combines edge insertion with a biased random walk to explore high-potential regions of the search space. The method first identifies the Pareto-frontier solutions in the archive, then selects a base solution probabilistically weighted by its dominance and diversity, followed by a two-stage perturbation: 1) a constrained segment inversion that preserves node connectivity, and 2) a multi-objective-aware edge swap that prioritizes improvements in both objectives while ensuring feasibility through a validation check. The generated neighbor solution is guaranteed to be a valid TSP tour through the use of a permutation repair mechanism that corrects any invalid sequences.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a base solution (weighted random selection)\n    objectives = np.array([obj for _, obj in archive])\n    norm_obj = objectives / np.max(objectives, axis=0)\n    combined_scores = np.sum(norm_obj, axis=1)\n    probabilities = combined_scores / np.sum(combined_scores)\n    selected_idx = np.random.choice(len(archive), p=probabilities)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Hybrid local search operator\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Stage 1: Constrained segment inversion\n    if n > 3:\n        segment_length = np.random.randint(2, min(5, n//2))\n        start = np.random.randint(0, n - segment_length)\n        end = start + segment_length\n        segment = new_solution[start:end]\n        np.random.shuffle(segment)\n        new_solution[start:end] = segment\n\n    # Stage 2: Multi-objective-aware edge swap\n    for _ in range(2):\n        i, j = np.random.choice(n, 2, replace=False)\n        if i > j:\n            i, j = j, i\n\n        # Calculate cost before swap\n        prev_cost1 = (distance_matrix_1[new_solution[i-1], new_solution[i]] +\n                      distance_matrix_1[new_solution[j], new_solution[(j+1)%n]])\n        prev_cost2 = (distance_matrix_2[new_solution[i-1], new_solution[i]] +\n                      distance_matrix_2[new_solution[j], new_solution[(j+1)%n]])\n\n        # Calculate cost after swap\n        new_cost1 = (distance_matrix_1[new_solution[i-1], new_solution[j]] +\n                     distance_matrix_1[new_solution[i], new_solution[(j+1)%n]])\n        new_cost2 = (distance_matrix_2[new_solution[i-1], new_solution[j]] +\n                     distance_matrix_2[new_solution[i], new_solution[(j+1)%n]])\n\n        # Accept if both objectives improve\n        if (new_cost1 < prev_cost1) and (new_cost2 < prev_cost2):\n            new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n\n    # Ensure solution is valid\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism if invalid\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -0.9434502893378602,
               0.6291956901550293
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a multi-phase geometric fragmentation and reassembly strategy that first decomposes the solution tour into geometrically distinct segments based on the relative positions of nodes in both objective spaces, then probabilistically selects and reorders these segments using a novel geometric harmony metric that balances the spatial distribution and objective contributions of each segment, followed by a constrained segment fusion process that dynamically merges adjacent segments based on their geometric compatibility and potential for Pareto improvement, with feasibility maintained through a segment boundary validation mechanism that ensures the solution remains a valid tour by reconstructing the tour using a geometric centroid-based ordering approach if invalidity is detected.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select base solution based on combined objective quality\n    def calculate_quality(obj):\n        return 1.0 / (1.0 + obj[0] + obj[1])\n\n    qualities = [calculate_quality(obj) for _, obj in archive]\n    selected_idx = np.argmax(qualities)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Geometric fragmentation\n    if n > 4:\n        # Calculate geometric properties for each node in both spaces\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n\n        # Calculate centroids and angles\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate angles relative to centroids\n        angles1 = np.arctan2(coords1[:,1] - centroid1[1], coords1[:,0] - centroid1[0])\n        angles2 = np.arctan2(coords2[:,1] - centroid2[1], coords2[:,0] - centroid2[0])\n\n        # Calculate geometric harmony metric\n        angle_diff = np.abs(angles1 - angles2)\n        harmony = 1.0 / (1.0 + angle_diff)\n\n        # Sort nodes by harmony\n        sorted_indices = np.argsort(harmony)[::-1]\n\n        # Split into segments based on harmony\n        num_segments = max(2, min(4, n // 3))\n        segment_size = n // num_segments\n        segments = [new_solution[sorted_indices[i*segment_size:(i+1)*segment_size]] for i in range(num_segments)]\n\n        # Step 3: Segment reordering\n        # Calculate segment properties\n        segment_properties = []\n        for seg in segments:\n            seg_coords1 = instance[seg, :2]\n            seg_coords2 = instance[seg, 2:]\n            seg_centroid1 = np.mean(seg_coords1, axis=0)\n            seg_centroid2 = np.mean(seg_coords2, axis=0)\n            seg_properties = {\n                'centroid1': seg_centroid1,\n                'centroid2': seg_centroid2,\n                'size': len(seg),\n                'coords1': seg_coords1,\n                'coords2': seg_coords2\n            }\n            segment_properties.append(seg_properties)\n\n        # Calculate segment compatibility\n        compatibility_matrix = np.zeros((num_segments, num_segments))\n        for i in range(num_segments):\n            for j in range(num_segments):\n                if i != j:\n                    dist1 = np.linalg.norm(segment_properties[i]['centroid1'] - segment_properties[j]['centroid1'])\n                    dist2 = np.linalg.norm(segment_properties[i]['centroid2'] - segment_properties[j]['centroid2'])\n                    compatibility_matrix[i,j] = 1.0 / (1.0 + dist1 + dist2)\n\n        # Reorder segments based on compatibility\n        current_order = list(range(num_segments))\n        for _ in range(2):\n            i = random.randint(0, num_segments-1)\n            j = np.argmax(compatibility_matrix[i])\n            if i != j:\n                current_order[i], current_order[j] = current_order[j], current_order[i]\n\n        # Reconstruct solution from reordered segments\n        new_solution = np.concatenate([segments[i] for i in current_order])\n\n    # Step 4: Segment fusion\n    if n > 5 and random.random() < 0.3:\n        # Find the most compatible adjacent segments\n        best_i, best_j = -1, -1\n        best_compatibility = -np.inf\n\n        for i in range(len(segments)-1):\n            dist1 = np.linalg.norm(segment_properties[i]['centroid1'] - segment_properties[i+1]['centroid1'])\n            dist2 = np.linalg.norm(segment_properties[i]['centroid2'] - segment_properties[i+1]['centroid2'])\n            compatibility = 1.0 / (1.0 + dist1 + dist2)\n            if compatibility > best_compatibility:\n                best_compatibility = compatibility\n                best_i, best_j = i, i+1\n\n        if best_i != -1 and best_j != -1:\n            # Merge the compatible segments\n            merged_segment = np.concatenate([segments[best_i], segments[best_j]])\n            new_segments = segments[:best_i] + [merged_segment] + segments[best_j+1:]\n            new_solution = np.concatenate(new_segments)\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != n:\n        # Geometric centroid-based repair\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n\n        # Calculate angles relative to centroids\n        angles1 = np.arctan2(coords1[:,1] - centroid1[1], coords1[:,0] - centroid1[0])\n        angles2 = np.arctan2(coords2[:,1] - centroid2[1], coords2[:,0] - centroid2[0])\n\n        # Sort nodes by combined angle\n        combined_angles = angles1 + angles2\n        sorted_indices = np.argsort(combined_angles)\n        new_solution = new_solution[sorted_indices]\n\n    return new_solution\n\n",
          "score": [
               -0.934290307549595,
               0.31503432989120483
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a hybrid approach combining a multi-objective aware edge insertion heuristic with a constrained geometric perturbation, where it first identifies critical edges in the Pareto frontier solutions using a geometric analysis of the instance coordinates, then probabilistically selects a base solution based on its geometric diversity, followed by a two-phase perturbation process: 1) a directed edge insertion that prioritizes edges connecting geometrically distant nodes while maintaining Pareto optimality, and 2) a constrained geometric rotation that dynamically selects and rotates segments based on the relative positions of nodes in both objective spaces, with feasibility maintained through a geometric validation check and a permutation repair mechanism that ensures the solution remains a valid tour by reconstructing the tour using a nearest-neighbor approach from the closest valid node if invalidity is detected.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify critical edges in Pareto frontier solutions\n    pareto_front = []\n    for i, (sol_i, obj_i) in enumerate(archive):\n        dominated = False\n        for j, (sol_j, obj_j) in enumerate(archive):\n            if i != j and obj_j[0] <= obj_i[0] and obj_j[1] <= obj_i[1] and (obj_j[0] < obj_i[0] or obj_j[1] < obj_i[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(sol_i)\n\n    if not pareto_front:\n        pareto_front = [sol for sol, _ in archive]\n\n    # Step 2: Select base solution based on geometric diversity\n    def geometric_diversity(sol):\n        coords1 = instance[sol, :2]\n        coords2 = instance[sol, 2:]\n        centroid1 = np.mean(coords1, axis=0)\n        centroid2 = np.mean(coords2, axis=0)\n        diversity = np.sum(np.linalg.norm(coords1 - centroid1, axis=1)) + np.sum(np.linalg.norm(coords2 - centroid2, axis=1))\n        return diversity\n\n    base_solution = max(pareto_front, key=geometric_diversity).copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 3: Directed edge insertion\n    if n > 3:\n        # Find geometrically distant nodes\n        coords1 = instance[new_solution, :2]\n        coords2 = instance[new_solution, 2:]\n        distances1 = np.linalg.norm(coords1 - np.roll(coords1, 1, axis=0), axis=1)\n        distances2 = np.linalg.norm(coords2 - np.roll(coords2, 1, axis=0), axis=1)\n        total_distances = distances1 + distances2\n        critical_edges = np.argsort(total_distances)[-min(3, n//2):]\n\n        for edge in critical_edges:\n            # Try inserting a new edge\n            u, v = new_solution[edge], new_solution[(edge+1)%n]\n            # Find the best insertion point\n            best_insert = -1\n            best_gain = -np.inf\n            for i in range(n):\n                if i != edge and i != (edge+1)%n:\n                    # Calculate potential gain\n                    old_cost = distance_matrix_1[new_solution[i], new_solution[(i+1)%n]] + distance_matrix_2[new_solution[i], new_solution[(i+1)%n]]\n                    new_cost = distance_matrix_1[u, new_solution[(i+1)%n]] + distance_matrix_2[u, new_solution[(i+1)%n]] + \\\n                               distance_matrix_1[new_solution[i], v] + distance_matrix_2[new_solution[i], v]\n                    gain = old_cost - new_cost\n                    if gain > best_gain:\n                        best_gain = gain\n                        best_insert = i\n\n            if best_insert != -1 and best_gain > 0:\n                # Perform insertion\n                new_solution = np.concatenate([new_solution[:best_insert+1], [v], new_solution[best_insert+1:]])\n                new_solution = np.concatenate([new_solution[:edge+1], [u], new_solution[edge+1:]])\n\n    # Step 4: Constrained geometric rotation\n    if n > 4:\n        segment_length = np.random.randint(2, min(5, n//2))\n        start = np.random.randint(0, n - segment_length)\n        end = start + segment_length\n\n        # Calculate geometric properties\n        segment_coords1 = instance[new_solution[start:end], :2]\n        segment_coords2 = instance[new_solution[start:end], 2:]\n        centroid1 = np.mean(segment_coords1, axis=0)\n        centroid2 = np.mean(segment_coords2, axis=0)\n\n        # Rotate segment based on geometric properties\n        angle1 = np.arctan2(segment_coords1[:,1] - centroid1[1], segment_coords1[:,0] - centroid1[0])\n        angle2 = np.arctan2(segment_coords2[:,1] - centroid2[1], segment_coords2[:,0] - centroid2[0])\n        avg_angle = np.mean(angle1 + angle2)\n\n        # Sort nodes by angle\n        angles = angle1 + angle2\n        sorted_indices = np.argsort(angles)\n        new_solution[start:end] = new_solution[start:end][sorted_indices]\n\n    # Step 5: Feasibility check and repair\n    if len(np.unique(new_solution)) != n:\n        # Nearest-neighbor repair\n        valid_tour = []\n        remaining_nodes = set(new_solution)\n        current_node = new_solution[0]\n        valid_tour.append(current_node)\n        remaining_nodes.remove(current_node)\n\n        while remaining_nodes:\n            min_dist = np.inf\n            next_node = None\n            for node in remaining_nodes:\n                dist = distance_matrix_1[current_node, node] + distance_matrix_2[current_node, node]\n                if dist < min_dist:\n                    min_dist = dist\n                    next_node = node\n            valid_tour.append(next_node)\n            remaining_nodes.remove(next_node)\n            current_node = next_node\n\n        new_solution = np.array(valid_tour)\n\n    return new_solution\n\n",
          "score": [
               -0.9502865232406194,
               0.8973692655563354
          ]
     },
     {
          "algorithm": "{The new algorithm will employ a multi-objective clustering-based solution selection and adaptive path decomposition strategy that first clusters solutions in the archive using a novel objective-space distance metric, then selects a base solution from the most diverse cluster to promote exploration. It then decomposes the tour into segments based on alternating objective-space dominance relationships, dynamically reorders these segments using a hybrid of geometric and objective-aware criteria, and reconstructs the tour with an adaptive segment merging mechanism that prioritizes edges showing balanced improvement in both objectives. The method ensures feasibility through a permutation validation step and a cluster-aware repair mechanism, while incorporating a dynamic objective weighting scheme that adjusts based on the current archive's Pareto front characteristics to guide the local search toward more promising regions of the objective space.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Multi-objective clustering\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-10)\n\n    # Calculate cluster centers using k-means\n    from sklearn.cluster import KMeans\n    n_clusters = min(3, len(archive))\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(normalized_obj)\n    cluster_labels = kmeans.labels_\n\n    # Select from most diverse cluster\n    cluster_sizes = np.bincount(cluster_labels)\n    selected_cluster = np.argmin(cluster_sizes)\n    cluster_indices = [i for i, label in enumerate(cluster_labels) if label == selected_cluster]\n\n    if not cluster_indices:\n        selected_idx = np.random.randint(len(archive))\n    else:\n        selected_idx = np.random.choice(cluster_indices)\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    n = len(new_solution)\n\n    # Step 2: Adaptive path decomposition\n    decomposition_points = []\n    for i in range(n-1):\n        u, v = new_solution[i], new_solution[i+1]\n        # Calculate dominance relationship\n        obj_u = archive[selected_idx][1]\n        obj_v = (distance_matrix_1[u,v], distance_matrix_2[u,v])\n        if (obj_v[0] <= obj_u[0] and obj_v[1] < obj_u[1]) or (obj_v[0] < obj_u[0] and obj_v[1] <= obj_u[1]):\n            decomposition_points.append(i+1)\n\n    # Step 3: Dynamic segment reordering\n    if decomposition_points:\n        segments = []\n        start = 0\n        for point in sorted(decomposition_points):\n            segments.append(new_solution[start:point+1])\n            start = point+1\n        if start < n:\n            segments.append(new_solution[start:])\n\n        # Reorder segments with adaptive criteria\n        reordered_segments = []\n        toggle = True\n        for i in range(len(segments)):\n            if toggle:\n                # Geometric criteria\n                segment = sorted(segments[i], key=lambda x: instance[x, 0] + instance[x, 2])\n            else:\n                # Objective criteria\n                segment = sorted(segments[i], key=lambda x: distance_matrix_1[segments[i][0], x] + distance_matrix_2[segments[i][0], x])\n            reordered_segments.append(segment)\n            toggle = not toggle\n\n        new_solution = np.concatenate(reordered_segments)\n\n    # Step 4: Adaptive segment merging\n    if len(decomposition_points) > 1 and np.random.random() < 0.5:\n        merge_pos = np.random.choice(decomposition_points)\n        new_solution = np.concatenate([new_solution[:merge_pos], new_solution[merge_pos+1:]])\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        # Repair mechanism\n        missing = set(range(n)) - set(new_solution)\n        duplicates = [x for x in range(n) if list(new_solution).count(x) > 1]\n        for dup in duplicates:\n            pos = np.where(new_solution == dup)[0][1]\n            new_solution[pos] = missing.pop()\n\n    return new_solution\n\n",
          "score": [
               -0.9591244042383689,
               2.6537925004959106
          ]
     },
     {
          "algorithm": null,
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Select solution with highest potential for improvement (highest sum of objectives)\n    selected_solution = max(archive, key=lambda x: x[1][0] + x[1][1])[0].copy()\n    n = len(selected_solution)\n\n    if n < 3:\n        return selected_solution\n\n    # Dynamic segment selection based on solution properties\n    segment_length = min(max(2, n // 3), n - 2)\n    start_idx = np.random.randint(0, n - segment_length)\n\n    # Adaptive segment shuffling\n    segment = selected_solution[start_idx:start_idx + segment_length]\n    np.random.shuffle(segment)\n\n    # Objective-aware reinsertion\n    remaining_nodes = [node for node in selected_solution if node not in segment]\n    new_segment = []\n\n    # Start with node that minimizes combined distance to neighbors\n    if remaining_nodes:\n        current_node = min(remaining_nodes, key=lambda x: distance_matrix_1[x, segment[0]] + distance_matrix_2[x, segment[0]])\n        new_segment.append(current_node)\n        remaining_nodes.remove(current_node)\n\n    for node in segment:\n        # Find best position to insert current node based on both objectives\n        best_pos = 0\n        best_score = float('inf')\n\n        for i in range(len(new_segment) + 1):\n            if i == 0:\n                prev_node = new_segment[-1] if new_segment else selected_solution[-1]\n                next_node = new_segment[0] if new_segment else selected_solution[start_idx + segment_length]\n            elif i == len(new_segment):\n                prev_node = new_segment[-1]\n                next_node = selected_solution[start_idx + segment_length]\n            else:\n                prev_node = new_segment[i-1]\n                next_node = new_segment[i]\n\n            dist1 = distance_matrix_1[prev_node, node] + distance_matrix_1[node, next_node]\n            dist2 = distance_matrix_2[prev_node, node] + distance_matrix_2[node, next_node]\n            score = 0.7 * dist1 + 0.3 * dist2  # Weighted by objective importance\n\n            if score < best_score:\n                best_score = score\n                best_pos = i\n\n        new_segment.insert(best_pos, node)\n\n    # Construct new solution\n    new_solution = np.concatenate([\n        selected_solution[:start_idx],\n        np.array(new_segment),\n        selected_solution[start_idx + segment_length:]\n    ])\n\n    # Validate solution\n    if len(np.unique(new_solution)) == n and len(new_solution) == n:\n        return new_solution\n\n    return selected_solution\n\n",
          "score": [
               -0.9368335472606997,
               0.8721699118614197
          ]
     },
     {
          "algorithm": "{The new algorithm builds upon the common backbone idea of intelligent solution selection and hybrid local search from the provided algorithms, but introduces a novel approach by combining multi-objective solution evaluation with a dynamic segment insertion mechanism. It first identifies the most promising solutions in the archive by analyzing their dominance relationships and combined objective values, then employs a weighted random selection to choose a base solution with higher probability for better-performing solutions. The hybrid local search operator dynamically selects and inserts a segment from one part of the tour into another position, evaluating the impact on both objectives and accepting the change if it improves at least one objective or with a small probability if it doesn't. The algorithm incorporates a multi-objective-aware acceptance criterion, a fallback mechanism to ensure solution feasibility, and a secondary segment insertion attempt if the initial insertion doesn't yield improvement, while going beyond standard segment-based operations by considering dynamic segment selection and insertion points based on both objective spaces.}",
          "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], instance: np.ndarray, distance_matrix_1: np.ndarray, distance_matrix_2: np.ndarray) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Evaluate solutions and select base solution\n    non_dominated = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] <= obj[0] and other_obj[1] <= obj[1] and (other_obj[0] < obj[0] or other_obj[1] < obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            non_dominated.append(i)\n\n    if not non_dominated:\n        combined_costs = [obj[0] + obj[1] for _, obj in archive]\n        selected_idx = np.argmin(combined_costs)\n    else:\n        qualities = [1.0 / (1.0 + archive[i][1][0] + archive[i][1][1]) for i in non_dominated]\n        selected_idx = random.choices(non_dominated, weights=qualities, k=1)[0]\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Hybrid local search with dynamic segment insertion\n    n = len(base_solution)\n    if n < 4:\n        return new_solution\n\n    # Select a random segment to insert elsewhere\n    segment_start = random.randint(0, n - 3)\n    segment_length = random.randint(2, min(4, n - segment_start - 1))\n    segment_end = segment_start + segment_length\n    segment = new_solution[segment_start:segment_end]\n\n    # Calculate segment cost\n    def calculate_segment_cost(sol, start, end):\n        cost1 = 0.0\n        cost2 = 0.0\n        for i in range(start, end):\n            cost1 += distance_matrix_1[sol[i], sol[(i+1)%n]]\n            cost2 += distance_matrix_2[sol[i], sol[(i+1)%n]]\n        return cost1, cost2\n\n    original_cost1, original_cost2 = calculate_segment_cost(new_solution, segment_start, segment_end)\n\n    # Remove the segment from its current position\n    new_solution = np.concatenate([new_solution[:segment_start], new_solution[segment_end:]])\n\n    # Find a new position to insert the segment\n    insert_pos = random.randint(0, n - segment_length - 1)\n    new_solution = np.concatenate([new_solution[:insert_pos], segment, new_solution[insert_pos:]])\n\n    # Calculate new segment cost\n    new_cost1, new_cost2 = calculate_segment_cost(new_solution, insert_pos, insert_pos + segment_length)\n\n    # Acceptance criterion\n    if not (new_cost1 <= original_cost1 and new_cost2 <= original_cost2) and random.random() >= 0.3:\n        # If not accepted, try another insertion position\n        for _ in range(2):\n            new_insert_pos = random.randint(0, n - segment_length - 1)\n            temp_solution = np.concatenate([new_solution[:new_insert_pos], segment, new_solution[new_insert_pos:]])\n            temp_cost1, temp_cost2 = calculate_segment_cost(temp_solution, new_insert_pos, new_insert_pos + segment_length)\n            if (temp_cost1 <= original_cost1 and temp_cost2 <= original_cost2) or random.random() < 0.2:\n                new_solution = temp_solution\n                break\n\n    # Ensure feasibility\n    if len(np.unique(new_solution)) != n:\n        new_solution = base_solution.copy()\n\n    return new_solution\n\n",
          "score": [
               -0.9076509273958202,
               0.41547876596450806
          ]
     }
]